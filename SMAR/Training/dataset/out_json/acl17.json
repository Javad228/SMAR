[
  {
    "conf": "acl17",
    "idd": 216,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "NMI is all the rage!",
          "Driving the current state-of-the-art (Sennrich et al., 2016)",
          "Widely adopted by the industry"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "seq2seq with Attention",
          "Bahdanau et al. (2015)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Syntax was all the rage!",
          "The “previous” state-of-the-art was syntax-based SMT",
          "i.e. systems that used linguistic information (usually represented as parse trees)",
          "“Beaten” by NMT in 2016",
          "Can we bring the benefits of syntax into the recent neural systems?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Syntax: Constituency Structure",
          "A Constituency (a.k.a Phrase-Structure) grammar defines a set of rewrite rules which describe the structure of the language.",
          "Groups words into larger units (constituents)",
          "Defines a hierarchy between constituents",
          "Draws relations between different constituents (words, phrases, clauses...)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Why Syntax Can Help MT",
          "Hints as to which word sequences belong together",
          "Helps in producing well structured sentences",
          "Allows informed reordering decisions according to the syntactic structure",
          "Encourages long-distance dependencies when selecting translations"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Our Approach: String-to-Tree NMT",
          "Main idea: translate a source sentence into a linearized tree of the target sentence",
          "Inspired by works on RNN-based syntactic parsing (Vinyals et. al,2015, Choe & Charniak, 2016)",
          "Allows using the seq2seq framework as-is"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Experimental Details",
          "We used the Nematus toolkit (Sennrich et al. 2017)",
          "Joint BPE segmentation (Sennrich et al. 2016)",
          "For training, we parse the target side using the BLLIP parser(McClosky, Charniak and Johnson, 2006)",
          "Requires some care about making BPE, Tokenization and Parser work together"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Experiments - Large Scale",
          "German to English, 4.5 million parallel training sentences from WMT16",
          "Train two NMT models using the same setup (Same settings as the SOTA neural system in WMT16)",
          "syntax-aware (bpe2tree)",
          "syntax-agnostic baseline (bpe2bpe)",
          "The syntax-aware model performs better in terms of BLEU"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Experiments - Low Resource",
          "German/Russian/Czech to English - 180k-140k parallel training sentences",
          "The syntax-aware model performs better in terms of BLEU in all cases",
          "Up to 2+ BLEU improvement"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Looking Beyond BLEU"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Accurate Trees",
          "99% of the predicted trees in the development set had valid bracketing",
          "Eye-balling the predicted trees found them well-formed and following the syntax of English."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Where Syntax Helps? Alignments",
          "The attention based model induces soft alignments between the source and the target",
          "The syntax-aware model produced more sensible alignments"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Attending to Source syntax",
          "We inspected the attention weights during the production of the tree's opening brackets",
          "The model consistently attends to main verb (\"hatte\") or to structural markers (question marks, hyphens...) in the source sentence",
          "Indicates the system implicitly learns source syntax to some extent (Shi, Padhi and Knight, 2016) and possibly plans the decoding accordingly"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Structure (1) - Reordering",
          "German to English translation requires a significant amount of reordering during translation",
          "Quantifying reordering shows that the syntax-aware system performs more reordering during the training process"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Structure (1) - Reordering",
          "We would like to interpret the increased reordering from a syntactic perspective",
          "We extract GHKM rules (Galley et al., 2004) from the dev set using the predicted trees and attention-induced alignments",
          "The most common rules reveal linguistically sensible transformations, like moving the verb from the end of a German constituent to the beginning of the matching English one"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Structure (II) - Relative Constructions",
          "A common linguistic structure is relative constructions, i.e. \"The XXX which YYY\", \"A XXX whose YYY\"...",
          "The words that connect the clauses in such constructions are called relative pronouns, i.e. \"who\", \"which\", \"whom\"...",
          "The syntax-aware system produced more relative pronouns due to the syntactic context"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Structure (II) - Relative Constructions",
          "Source:",
          "\"Guangzhou, das in Deutschland auch Kanton genannt wird...\"",
          "Reference:",
          "\"Guangzhou, which is also known as Canton in Germany...\"",
          "Syntax-Agnostic:",
          "\"Guangzhou, also known in Germany, is one of...\"",
          "Syntax-Based:",
          "\"Guangzhou, which is also known as the canton in Germany,...\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Structure (II) - Relative Constructions",
          "Source:\"Zugleich droht der stark von internationalen Firmen abhdngigenRegion ein Imageschaden...\"",
          "Reference:\"At the same time, the image of the region, which is heavily reliant on international companies...\"",
          "Syntax-Agnostic:\"At the same time, the region's heavily dependent region...\"",
          "Syntax-Based:\"At the same time, the region, which is heavily dependent on international firms...\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Human Evaluation",
          "We performed a small-scale human-evaluation using mechanical turk on the first 500 sentences in newstest 2015",
          "Two turkers per sentence",
          "The syntax-aware translations had an advantage over the baseline"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Conclusion",
          "Neural machine translation can clearly benefit from target-side syntax",
          "Other recent work include:",
          "Eriguchi et al., 2017, Wu et al., 2017 (Dependency)",
          "Nadejde et al., 2017 (CCG)",
          "A general approach - can be easily incorporated into other neural language generation tasks like summarization, image caption generation...",
          "Larger picture: don't throw away your linguistics! Neural systems can also leverage symbolic linguistic information"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 48,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "STARTING FROM THE END (spoiler ©) the Indo-European phylogenetic tree.\\n\\nThe \"ground truth\" monolingual English texts translated from 17 IE languages:\\n- English\\n- Italian\\n- Swedish\\n- French\\n- Danish\\n- Spanish\\n- German\\n- Dutch\\n- Romanian\\n- Portuguese\\n- Lithuanian\\n- Latvian\\n- Polish\\n- Slovak\\n- Bulgarian\\n- Slovenian"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "BACKGROUND\\n- THE FEATURES OF TRANSLATIONESE\\n- Translators (almost) always tried to remain invisible.\\n- Translations have unique characteristics that set them apart from originals.\\n- Universals (simplification, standardization, explicitation).\\n- Interference (the \"fingerprints\" of a source language on the translation product).\\n\\nLanguages closer to each other are likely to share more features in the target language of translation.\\n\\nHYPOTHESIS\\n\\nThe distance between languages is retained and can be recovered when assessed through these features in translated texts."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "- Europarl (the proceedings of the European Parliament)\\n- Members are allowed to speak in any of the EU languages\\n- All parliament speeches were translated from the original language into other EU languages using English as a pivot\\n- Direct translations into English, indirect translations into all other languages\\n- We explore indirect translations into French in this work\\n- We focus on 17 source languages, grouped into 3 language families:\\n- Germanic, Romance, and Balto-Slavic"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "RECONSTRUCTION OF LANGUAGE TREES\\n\\nFEATURES USED\\n- POS-trigrams, reflecting shallow syntactic structures (strongly associated with interference)\\n- Function words, reflecting grammar (associated with interference)\\n- Cohesive markers (associated with translation universals)\\n\\nAGGLOMERATIVE (HIERARCHICAL) CLUSTERING OF FEATURE VECTORS\\n- Using the variance minimization algorithm (Ward,\\n1963)\\n- with Euclidean distance"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Phylogenetic language trees generated with translated text (POS-trigrams)\\n\\nItalian\\nItalian\\nReps\\ntokaeenes\\nFrench\\nSpanish\\nas\\nFrench\\nPOHL\\nCU\\nGE\\nunr\\nGerman\\nGerman\\nDutch\\nporeree\\nere\\nrests\\nss\\nplaiaine\\nSwedish\\n1\\nEnglish\\nDutch\\n-\\nSwedish\\n|\\n---\\n_\\nDanish\\nDanish\\nEnglish\\nRomanian\\n-\\nSlovak\\nLithuanian\\n(....\\n---\\n=\\nLithuanian\\nBee\\neee\\nee\\nPortuguese\\nCleccceseneereeee\\n.\\nLatvian\\n1)\\ne+\\nCzech\\n-\\nBulgarian\\nPct\\nSlovak\\nReCEEECEUEECROHCECLCCoseeee\\nRomanian\\nBulgarian\\nSlovenian\\neee\\nLatvian\\nfo\\nreeee\\nCay\\nca\\nees\\n(\\n\"\"\\n--\\nPortuguese\\nPolish\\nPolish\\nSlovenian\\nCzechisi\\ntiesto\\nsienna\\ninns\\nENGLISH translations\\nFRENCH translations"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "EVALUATION METHODOLOGY\\nMEASURE SIMILARITY TO THE GOLD STANDARD\\nUNWEIGHTED EVALUATION\\nWEIGHTED EVALUATION\\n(CLADOGRAM) (PHYLOGRAM)\\nassessing only structural\\nassessing similarity based on both\\n(topological) similarity\\nstructure and branching length\\nCLADOGRAM\\nPHYLOGRAM"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "EVALUATION METHODOLOGY\\n- CONT.\\n\\nAdaptation of the L2-norm to leaf-pair distance\\n- Suitable for both weighted and unweighted evaluation.\\n\\nDist.g) = (D.Q.4)\\n- 0.04; i, j € [1..N]; i, j\\n\\nAg\\n- the gold tree\\n\\nBt\\n- a tree subject to evaluation\\n\\nD(i,\\nj)\\n- distance between two leaves in a tree"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "DISTANCE OF A RECONSTRUCTED TREE FROM THE GOLD STANDARD (using various feature sets)\\n\\nUNWEIGHTED EVALUATION\\n\\nWEIGHTED EVALUATION\\n\\nfeature                  AVG     STD       AVG     STD\\nPOS-trigrams + FW      362    0.07      367    0.06\\nPOS-trigrams + FW      278    0.03      348    0.02\\nPOS-trigrams           353    0.06      399    0.08\\nFunction words          429    0.07      450    0.08\\nFunction words          304    0.03      316    0.05\\nCohesive markers        626    0.16      678    0.14\\nCohesive markers        263    0.06      279    0.07\\n\\nThe quality of trees increases; the worst tree is systematically closer to the gold standard.\\n\\nFor feature sets associated with cohesive markers than trees built from translations into French (done via a third language)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "DISTANCE OF A RECONSTRUCTED TREE FROM THE GOLD STANDARD (using various feature sets)\\n\\nUNWEIGHTED EVALUATION\\n\\nFEATURE        AVG   STD\\nPOS-trigrams + FW   362   0.07\\nPOS-trigrams + FW   278   0.03\\nPOS-trigrams        353   0.06\\nPOS-trigrams        301   0.03\\nFunction words      429   0.07\\nFunction words      304   0.03\\nCohesive markers    626   0.16\\nCohesive markers    263   0.07\\n\\nWEIGHTED EVALUATION\\n\\nFEATURE        AVG   STD\\nPOS-trigrams + FW   367   0.06\\nPOS-trigrams + FW   348   0.02\\nPOS-trigrams        399   0.08\\nPOS-trigrams        351   0.03\\nFunction words      450   0.08\\nFunction words      376   0.05\\nCohesive markers    678   0.14\\nCohesive markers    263   0.06\\n\\nThe quality of trees increases. The worst tree is systematically closer to the gold standard.\\n\\nFeature sets associated with interference markers are generated using cohesive markers than trees built from translations into French (done via a third language)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "DISTANCE OF A RECONSTRUCTED TREE FROM THE GOLD STANDARD (using various feature sets)\\n\\nUNWEIGHTED EVALUATION\\n\\nWEIGHTED EVALUATION\\n\\nfeature           AVG     STD     AVG     STD\\nfeature           AVG     STD     AVG     STD\\n\\nPOS-trigrams + FW  362    0.07    0.367  0.06\\nPOS-trigrams + FW  278    0.03    0.348  0.02\\nPOS-trigrams       353    0.06    0.399  0.08\\nPOS-trigrams       501    0.00    0.500  0.07\\n\\nFunction words      429    0.07    0.450  0.08\\nFunction words      304    0.03    0.360  0.05\\n\\nCohesive markers     626    0.16    0.678  0.14\\nCohesive markers     98     0.02    0.050  0.07\\n\\nThe quality of trees increases; the worst tree is systematically closer to the gold standard for feature sets associated with coherence than trees built from translations into French (done via a third language)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "DISTANCE OF A RECONSTRUCTED TREE FROM THE GOLD STANDARD (using various feature sets)\\n\\nUNWEIGHTED EVALUATION\\n\\nWEIGHTED EVALUATION\\n\\nfeature           AVG       STD           AVG       STD\\n\\nPOS-trigrams + FW       362        0.07          367        0.06\\n\\nPOS-trigrams + FW       278        0.3           348        0.2\\n\\nPOS-trigrams              353        0.6           399        0.8\\n\\nPOS-trigrams SOI      POS 3515       0.3\\n\\nFunction words        429        0.7           450       0.8\\n\\nFunction words        304        0.3           376       0.5\\n\\nCohesive markers       626        1.6           678       1.4\\n\\nCohesive markers       598        2.6           736        0.7\\n\\nilt from English translation: the quality of trees increases; the worst tree is systematically closer to the gold standard; for feature sets associated: generated using cohesive than trees built from translations into French (done via a third language) -"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "ANALYSIS ARTICLES\\n\\nIndefinite (\"a\", \"an\") and definite (\"the\")\\n\\nPossessive constructions\\n- With clitic 's (\"the guest's room\")\\n- With a prepositional phrase containing \"of\" (\"the room of the guest\")\\n- With noun compounds (\"guest room\")\\n\\nVerb-particle constructions\\n- Verbs that combine with a particle to create a new meaning (MWEs), e.g., \"turn down\", \"get over\"\\n\\nTense and aspect\\n- With the auxiliary verbs \"have\" (present) or \"be\" (progressive), e.g., \"have done\", \"was going\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "ANALYSIS\\n- CONT. FREQUENCIES reflecting various linguistic phenomena in English translations\\n- Germanic\\n- Romance\\n- Balto-Slavic\\n\\n0.7 0.6 0.5 0.4 0.3 0.2 0.1\\n\\ndefine articles of constructions verb-particle perfect progressive (per 10 tokens) (per 25 tokens) (per 250 tokens) (per 100 tokens) (per 500 tokens)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Translation does not distort the original text randomly.\\nA phylogenetic language tree can be reconstructed from monolingual texts translated from various languages.\\nFeatures associated with interference (POS-ngrams, FWs) yield more accurate phylogenetic language trees.\\n\\nTranslations impact the evolution of languages\\n- it is estimated that for certain languages up to 30% of published texts are mediated through translations (Pym and Chrupata, 2005).\\n\\nAre translations likely to play a role in language change?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "STARTING FROM THE END (spoiler ©) phylogenetic tree reconstructed from monolingual English texts translated from monolingual French texts via English pivot.\\n- 17 IE languages\\n- Italian\\n- French\\n- Spanish\\n- German\\n- Dutch\\n- Swedish\\n- Danish\\n- English\\n- Romanian\\n- Slovak\\n- Lithuanian\\n- Portuguese\\n- Latvian\\n- Czech\\n- Bulgarian\\n- Slovenian\\n- Polish"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 65,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          ") foe Seem Re 8 [Reenerent Etecalk wave RtANNIntroductionEric Tucker, a 35-year-old co-founder of a marketing company in Austin,.\\nTex., had just about 40 Twitter followers. But his recent tweet about paiderictucker . . .\\n.exotucker protesters being bused to demonstrations against President-electDonald J.\\nTrump fueled a nationwide conspiracy theory\\n- one that Mr.Anti-Trump protestors n y are not as Trump joined in promoting.organic as they seem.\\nHere are the busses they Mr. Tucker's post was shared at least 16,000 times on Twitter and morecame In.\\n#fakeprotests #trump201 6 #austin than 350,000 times on Facebook. The problem is that Mr. Tucker got itwrong.\\nThere were no such buses packed with paid protesters.; cats But that didn't matter.i csi ' While some fake news is produced purposefully by teenagers in thea a j aire | Balkans or entrepreneurs in the United States seeking to make moneyPm\" tet oa + A .\\n\" es'al H Since = £ AY aj from advertising, false information can also arise from misinformedi.\\n4 he = BE nia Be mee aie social media posts by regular people that are seized on and spread7 Dee es a _: les = lal oak through a hyperpartisan blogosphere.3 z z Here, The New York Times deconstructs how Mr.\\nTucker's now-deletedi Fe 2g declaration on Twitter the night after the election turned into a fake----- a oS ne Palatina | news phenomenon.\\nIt is an example of how, in an ever-connected world< a.\\na a .where speed often takes precedence over truth, an observation by a4 P } private citizen can quickly become a talking point, even as it is beingproved false.RETWEETS: LIKES: ar a'ze16,931 14,521 a ae B 8 g 3&3 w@ eS @ Donald J.\\nTrump@realDonaldTrump8:43 PM\\n- 9 Nov 2016Just had a very open and successful presidential election.\\nNow oeprofessional protesters, incited by the media, are protesting --Very unfair! _--: 10:19 AM\\n- 11 Nov 2016 ;A story or statement whose truth value is .\\neuesus Women: 5° .unverified or deliberately false1; i} Y aSe tt-tSs TAZA"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Introduction\\n\\nThe fake news went viral.\\nEric Tucker, 40 followers, of Anti-Trump protestors in Austin today are not as organic as they seem.\\nHere, Gateway Pundit > 44k shares are the buses they came in. Figures. Anti-Trump #fakeprotests #trump2016 #austin rely.\\n\\nProtesters Were Bussed In to Austin\\n\\n#FakeProtests. 16,931 14,521 Reddit 363 comments. Donald J. Trump, 2.28M followers, booed.\\n\\nBREAKING: They found the buses!\\nWe see evidence of a very open and successful situation lined up just blocks away from the election.\\n\\nNow professional protesters, incited by the media, are protesting. Very unfair!\\n\\nMany asking for concrete evidence: They found the buses!\\nDozens have been lined up just blocks away from the Austin protests.\\n\\nEric Tucker, 40 followers, did not see loading or unloading. There were even more buses within pics.\\nQuite near protests at the right timing.\\n\\nTime indicates the level of influence.\\nStart from grassroots users, promoted by some influential accounts, widely spread."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Motivation: \"We generally are not good at distinguishing rumors.\\nIt is crucial to track and debunk rumors early to minimize their harmful effects.\\nOnline fact-checking services have limited topical coverage and long delay. Existing models use feature engineering\\n- over simplistic; or recently deep neural networks\\n- ignore propagation structures.\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Contributions:\\n- Represent information spread on Twitter with a propagation tree, formed by harvesting user's interactions, to capture high-order propagation patterns of rumors.\\n- Propose a kernel-based data-driven method to pore relevant features automatically for estimating the similarity between two propagation trees.\\n- Enhance the proposed model by considering propagation paths from the source tweet to subtrees to capture the context of transmission.\\n- Release two real-world Twitter datasets with finer-grained ground truth labels."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Related Work\\n- Systems based on common sense and investigative journalism, e.g., snopes.com, factcheck.org\\n- Learning-based models for rumor detection\\n- Information credibility: Castillo et al. (2011), Yang et al. (2012)\\n- Using handcrafted and temporal features: Liu et al. (2015), Ma et al. (2015), Kwon et al. (2013,\\n2017)\\n- Using cue terms: Zhao et al. (2015)\\n- Using recurrent neural networks: Ma et al. (2016)\\n- Kernel-based works\\n- Tree kernel: syntactic parsing (Collins and Duffy,\\n2001)\\n- Question-answering (Moschitti,\\n2006)\\n- Semantic analysis (Moschitti,\\n2004)\\n- Relation extraction (Zhang et al.,\\n2008)\\n- Machine translation (Sun et al., 2010)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Problem Statement = Given a set of microblog posts R = {r}, model each source tweet as a tree structure T(r) = < V, E >, where each node v = (uy, Cy, t,) provide the creator of the post, the text content and post time.\\nAnd E is directed edges corresponding to response relation.\\n- Task 1\\n- finer-grained classification for each source post: false rumor, true rumor, non-rumor, unverified rumor\\n- Task 2\\n- detect rumor as early as possible"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "pP Hoe mOLeMetelCO ee i ee ae ee ee eee eee1 & cai aoaatt Aen to es\\n\\nText Features\\n\\nAnnie Mae Gamiemae1000-19 0c 201\\n\\n\\mai jonates 10\\n\\nSupport!\\n\\nWarren Wilson\\n\\nOngoing racist police murders\\n\\n#Ferguson\\n\\n#BoycottWalmart\\n\\nS Se e7omenaP\\n\\nPropagation & Temporal\\n\\nI doubt they did but if it turns out to be true then I\\n\\nGood For Them!\\n\\nFeatures:\\n\\nMelanie B @meinooo Oct\\n2014.\\n\\n@anniemae1000 @70torinoman it'd be really inhumane if they did. If they did.\\n\\nThey support murder basically!\\n\\nSONY VS GO\\n\\nViva La Revolucion\\n\\nI @70torinoman\\n-\\n\\n' I alntaw'\\n\\nHeu davvrted ican / Revolutionary: The US capitalist\\n\\nAnnie Mae @anivemae1000\\n\\nThe system was founded on the genocide of a\\n\\nI think they support protecting their store from exploitation of workers.\\n\\nThis must be looters\\n\\nUser Features\\n\\nUm162K 9,954 12K\\n\\nMelanie B @meinooooo\\n- 19 Oct 2014\\n\\n@anniemae1000 @70torinoman\\n\\nDoubt it. They've already fixed the store & have it protected. Corporate had to have sent that.\\n\\nAnnie Mae @anniemae1000\\n- 19 Oct 2014\\n\\n@meinooooo @70torinoman\\n\\nWhatever: Walmart donates $10,000 to support Darren Wilson and the ongoing racist police murders\\n\\nNeed proof of it\\n\\nYou don't honestly believe that, do you?\\n\\nNot sure....sorry\\n\\nI honestly do see a meme trending, but no proof...perhaps\\n\\nSam Walton gave 300k to Obama's campaign?\\n\\nWhere is the credible link?\\n\\nI'm pretty good at research\\n- I think this is not true.\\n\\nSam Walton was dead before #Obama was born.\\n\\nReasons to boycott: Walmart.\\n\\nHe would have wired campaign donation from heavens."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Observation & Hypothesis Support\\n\\nDoubt\\n\\n(a) In rumors: Influence/Popularity\\n\\n(b) In non-tumors: Content-based signals (e.g., stance)\\n\\nNetwork-based signals (e.g., relative influence) and temporal traits (Kwon et al.,\\n2017)\\n\\nOur hypothesis: high-order patterns need to/could be captured using kernel methods"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Traditional Tree Kernel (TK) : NP | NP VP V NP D N D N pt ~ poi V NP D N ia tree | acut D N NP ' D N || | a N por, a tree D N a tree = TK compute the syntactic similarity between two sentences by counting the common subtrees = Kernel Function: ) iy, cv, div j E V > A(v % j, V ;) 7 A(v ;, v j ): common subtrees rooted at v; and 1;"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Propagation Tree Kernel (PTK)\\n\\nWhy PTK?\\n\\nExisting tree kernel cannot apply here, since in our case (1) node is a vector of continuous numerical values; (2) similarity needs to be softly defined between two trees instead of hardly counting on identical nodes.\\n\\nSimilarity Definition\\n\\nUser Similarity: E(uj, Uj) = |[U;\\n- u;||. co oy Waram(ci)nNgram(cj)\\n\\nContent Similarity: J(- j, cj) = INgram(c; UNgram(c))\\n\\nNode Similarity: f(vi, vj) = e(GEC;\\nw) + HI-DI(Cj, G)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Propagation Tree Kernel = Given two trees T₁ =< V₁, E₁ > and T₂ =< V₂, E₂ >, PTK computes similarity between them by enumerating all similar subtrees.\\n- Kernel Function: K(y₁, y₂, A₁, Y₁) + div E(V₁, A(v₁, v₂)) = A(v₁, v₂); and v₁ and v₂ are similar node pairs from V₁ and V₂ respectively.\\n\\nv₂ = arg max f(V₁, Y₁) v₁ ∈ V₂ = A(v, v'): similarity of two subtrees rooted at v and v' | y ∈ V.\\n\\nKernel algorithm:\\n1) if v or v' are leaf nodes, then A(v, v') = f(v, v');\\n2) else A(v, v') = f(w,\\nv) + A(c(h(v'), k));"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Context-Sensitive Extension of PTK\\n\\nConsider propagation paths from root node to the subtree.\\n\\nWhy cPTK? PTK ignores the clues outside the subtrees and the route embed how the propagation happens.\\nSimilar intuition to context-sensitive tree kernel (Zhou et al., 2007).\\n\\nKernel Function:\\n\\nYiy, ev, Yao Ax (Vi vj) + div jv, dixng Axl, 7%) = L7: the length of propagation path from root r to v.\\n\\nContexr path = v(x): the x-th ancestor of v. Le\\n7. Pree actos ecet i\\n- 1 = A,(v, v'): similarity of subtrees rooted at v[x] and v'[x].\\n\\nKernel Algorithm:\\n1) if v[x] and v'[x] are the x-th ancestor nodes of v and v', then A,(v, v') = f(v[x], v'[x])\\n2) else: A,(v, v') = A(v, v') (i.e., PTK)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Rumor Detection via Kernel Learning\\n- Incorporate the proposed tree kernel functions (i.e., PTK or cPTK) into a supervised learning framework, for which we utilize a kernel-based SVM classifier.\\n- Avoid feature engineering\\n- the kernel function can explore an implicit feature space when calculating the similarity between two objects.\\n- For multi-class task, perform One vs.\\nall, i.e., building K (# of classes) basic binary classifiers so as to separate one class from all the others."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Data Collection = Construct our propagation tree datasets based on two reference Twitter datasets:\\n\\n= Twitter15 (Liu et al,\\n2015)\\n\\n= Twitter16 (Ma et al,\\n2016)\\n\\nConvert event label: Extract popular source tweets propagation binary -> quarternary threads (Source tweet: highly retweeted or replied) (retweets: Twrench.com) (replies: Web crawler)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Statistics of Data Collection\\n\\nStatistic         Twitter15      Twitter16\\n# of users       276,663        173,487\\n# of source tweets 1,490        818\\n# of threads     331,612        204,820\\n# of non-rumors  374            205\\n# of false rumors 370            205\\n# of true rumors  312            205\\n# of unverified rumors 374       203\\nAvg. time length / tree 1,337 Hours  848 Hours\\nAvg. # of posts / tree  251\\nMax # of posts / tree  1,768        2,765\\nMin # of posts / tree  81\\n\\nURL of the datasets:\\nhttps://www.dropbox.com/s/Ojhsfwep3ywvpca/rumdetect2017.zip?dl=0"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Approaches to compare with:\\n- DTR: Decision tree-based ranking model using enquiry phrases to identify trending rumors (Zhao et al.,\\n2015)\\n- DTC and SVM-RBF: Twitter information credibility model using Decision Tree Classifier (Castillo et al., 2011); SVM-based model with RBF kernel (Yang et al.,\\n2012)\\n- RFC: Random Forest Classifier using three parameters to fit the temporal tweets volume curve (Kwon et al.,\\n2013)\\n- SVM-TS: Linear SVM classifier using time-series structures to model the variation of social context features (Ma et al.,\\n2015)\\n- GRU: The RNN-based rumor detection model (Ma et al.,\\n2016)\\n- BOW: Linear SVM classifier using bag-of-words\\n- Ours (PTK and cPTK): Our kernel based model\\n- PTK- and cPTK-: Our kernel based model with subset node features."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Results on Twitter\\n\\nNR: Non-Rumor;\\nFR: False Rumor;\\nTR: True Rumor;\\nUR: Unverified Rumor;\\n\\n0.409\\n0.501\\n0.311\\n0.364\\n0.473\\n\\n0.318\\n0.455\\n0.037\\n0.218\\n0.225\\n\\n0.454\\n0.733\\n0.355\\n0.317\\n0.415\\n\\n0.544\\n0.796\\n0.472\\n0.404\\n0.483\\n\\n0.565\\n0.810\\n0.422\\n0.401\\n0.543\\n\\n0.646\\n0.792\\n0.574\\n0.608\\n0.592\\n\\n0.548\\n0.564\\n0.524\\n0.582\\n0.512\\n\\n0.657\\n0.734\\n0.624\\n0.673\\n0.612\\n\\n0.697\\n0.760\\n0.645\\n0.696\\n0.689\\n\\n0.710\\n0.825\\n0.685\\n0.688\\n0.647\\n\\n0.750\\n0.804\\n0.698\\n0.765\\n0.733"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Results on Twitter\\n\\nNR: Non-Rumor;\\nFR: False Rumor;\\nTR: True Rumor;\\nUR: Unverified Rumor;\\n\\n0.414\\n0.394\\n0.273\\n0.630\\n0.344\\n0.321\\n0.423\\n0.085\\n0.419\\n0.037\\n0.465\\n0.643\\n0.393\\n0.419\\n0.403\\n0.574\\n0.755\\n0.420\\n0.571\\n0.526\\n0.585\\n0.752\\n0.415\\n0.547\\n0.563\\n0.633\\n0.772\\n0.489\\n0.686\\n0.593\\n0.585\\n0.553\\n0.556\\n0.655\\n0.578\\n0.653\\n0.673\\n0.640\\n0.722\\n0.567\\n0.702\\n0.711\\n0.664\\n0.816\\n0.608\\n0.722\\n0.784\\n0.690\\n0.786\\n0.644\\n0.732\\n0.740\\n0.709\\n0.836\\n0.686"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Results on Early Detection 85+\\n- BOW\\n- RFC 0.85\\n- BOW\\n- RFCos\\n- PTK\\n- DTR 08;\\n- PTK\\n- DIR cig, \"PT OR egy «te RT eeBa 07 a.\\n> a7 gpoeteess 0.65 * wane icicle gS 0.65 POS ost # eon S 05+ @ p ------ ~~~ -- ~~~ 83 : Pog e 3 0.55 é ~\\n- re, = -------- asst éf c <2... -- =2 === a oo4 ost $f i < 05\\n- 6 EA ~ 0.45 4 r 0.45 o4 o.44 i 0.35 0.350 12 24 36 48 0 12 24 36 43 Detection Deadline (hours)\\n\\nDetection Deadline (hours)\\n\\n(a) Twitter 15 DATASET\\n\\n(b) Twitter 16 DATASET\\n\\nIn the first few hours, the accuracy of the kernel-based methods climbs more rapidly and stabilize more quickly.\\nPTK can detect rumors with 72% accuracy for Twitter 15 and 69.0% for Twitter 16 within 12 hours, which is much earlier than the baselines and the mean official report times."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Early Detection Example\\n\\nExample subtree of a rumor captured by the algorithm at early stage of propagation:\\n- #Walmart donates $10,000 to #DarrenWilson fund to continue police racial profiling, brutality, murder of black ppl: Judging by the way #Walmart pays & treats its employees this is no surprise.\\n- That's Wal-Mart always doing good for the overlords of society.\\n- lol\\n- RT\\n- NEED SOURCE. have a feeling this is just hearsay or they donated to backstoppers or something tangential.\\n- Lagree. I have been hearing this all day but no source;\\n- Exactly, I don't think Wal-Mart would let everyone know this if they did!!\\n\\nTime delay after root (hours)\\nInfluential users boost its propagation, unpopular-to-popular information flow, Textual signals (underlined)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Conclusion and future work:\\n- Apply kernel learning method for rumor debunking by utilizing the propagation tree structures.\\n- Propagation tree encodes the spread of a source tweet with complex structured patterns and flat information regarding content, user, and time associated with the tree nodes.\\n- Our kernel are combined under a supervised framework for identifying rumors of finer-grained levels by directly measuring the similarity among propagation trees.\\n\\nFuture work:\\n- Explore network representation method to improve the rumor detection task.\\n- Develop unsupervised models due to massive unlabeled data from social media."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "7. a j gee, d bi tl : ( a r A£ $ . 4 fi ty e / > a ) ra [oe 5 wh ye a . le4 yay 78 7 le rig. PA /.\\n' es Js y, + \" of ; 4 Ter ed fe J . fa A os ore 4 ff j r 7A 4 z ) 4 v7 y SS MS Sf Ga , wy) tad 7 7 oy ee / l \" 7(7 \" L\\n- b\\n- L oe 2016/7/1"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          ") foe Seem Re 8 [Reenerent Etecalk wave RtANNIntroductionEric Tucker, a 35-year-old co-founder of a marketing company in Austin,.\\nTex., had just about 40 Twitter followers. But his recent tweet about paiderictucker . . .\\n.exotucker protesters being bused to demonstrations against President-electDonald J.\\nTrump fueled a nationwide conspiracy theory\\n- one that Mr.Anti-Trump protestors n y are not as Trump joined in promoting.organic as they seem.\\nHere are the busses they Mr. Tucker's post was shared at least 16,000 times on Twitter and morecame In.\\n#fakeprotests #trump201 6 #austin than 350,000 times on Facebook. The problem is that Mr. Tucker got itwrong.\\nThere were no such buses packed with paid protesters.; cats But that didn't matter.i csi ' While some fake news is produced purposefully by teenagers in thea a j aire | Balkans or entrepreneurs in the United States seeking to make moneyPm\" tet oa + A .\\n\" es'al H Since = £ AY aj from advertising, false information can also arise from misinformedi.\\n4 he = BE nia Be mee aie social media posts by regular people that are seized on and spread7 Dee es a _: les = lal oak through a hyperpartisan blogosphere.3 z z Here, The New York Times deconstructs how Mr.\\nTucker's now-deletedi Fe 2g declaration on Twitter the night after the election turned into a fake----- a oS ne Palatina | news phenomenon.\\nIt is an example of how, in an ever-connected world< a.\\na a .where speed often takes precedence over truth, an observation by a4 P } private citizen can quickly become a talking point, even as it is beingproved false.RETWEETS: LIKES: ar a'ze16,931 14,521 a ae B 8 g 3&3 w@ eS @ Donald J.\\nTrump@realDonaldTrump8:43 PM\\n- 9 Nov 2016Just had a very open and successful presidential election.\\nNow oeprofessional protesters, incited by the media, are protesting --Very unfair! _--: 10:19 AM\\n- 11 Nov 2016 ;A story or statement whose truth value is .\\neuesus Women: 5° .unverified or deliberately false1; i} Y aSe tt-tSs TAZA"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 30,
        "texts": [
          "Introduction\\n\\nThe fake news went viral.\\nEric Tucker, 40 followers of Anti-Trump protestors in Austin today are not as organic as they seem.\\nHere Gateway Pundit > 44k shares are the buses they came in. Figures.\\nAnti-Trump #fakeprotests #trump2016 #austin rely on protesters who were bussed in to Austin.\\n\\nBREAKING: They found the buses! Misael lined up just blocks away from the Austin protests.\\nProfessional protesters, incited by the media, are protesting. Very unfair!\\n\\nMany are asking for concrete evidence. They found the buses!\\nDozen buses have been lined up just blocks away from the Austin protests.\\n\\nI did not see loading or unloading. There were even more buses within pics. Quite near protests at right timing.\\n\\nIndicates the level of influence. Start from a grassroots users, promoted by some influential accounts, widely spread."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 31,
        "texts": [
          "Motivation: \"We generally are not good at distinguishing rumors.\\nIt is crucial to track and debunk rumors early to minimize their harmful effects.\\nOnline fact-checking services have limited topical coverage and long delay. Existing models use feature engineering\\n- over simplistic; or recently deep neural networks\\n- ignore propagation structures.\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 32,
        "texts": [
          "Contributions:\\n- Represent information spread on Twitter with propagation tree, formed by harvesting user's interactions, to capture high-order propagation patterns of rumors.\\n- Propose a kernel-based data-driven method to pore relevant features automatically for estimating the similarity between two propagation trees.\\n- Enhance the proposed model by considering propagation paths from source tweet to subtrees to capture the context of transmission.\\n- Release two real-world Twitter datasets with finer-grained ground truth labels."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 33,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 34,
        "texts": [
          "Related Work\\n- Systems based on common sense and investigative journalism, e.g., snopes.com, factcheck.org\\n- Learning-based models for rumor detection\\n- Information credibility: Castillo et al. (2011), Yang et al. (2012)\\n- Using handcrafted and temporal features: Liu et al. (2015), Ma et al. (2015), Kwon et al. (2013,\\n2017)\\n- Using cue terms: Zhao et al. (2015)\\n- Using recurrent neural networks: Ma et al. (2016)\\n- Kernel-based works\\n- Tree kernel: syntactic parsing (Collins and Duffy,\\n2001)\\n- Question-answering (Moschitti,\\n2006)\\n- Semantic analysis (Moschitti,\\n2004)\\n- Relation extraction (Zhang et al.,\\n2008)\\n- Machine translation (Sun et al., 2010)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 35,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 36,
        "texts": [
          "Problem Statement = Given a set of microblog posts R = {r}, model each source tweet as a tree structure T(r) = < V, E >, where each node v = (uy, Cy, t,) provides the creator of the post, the text content and post time.\\nAnd E F is directed edges corresponding to response relation.\\n- Task 1\\n- finer-grained classification for each source post: false rumor, true rumor, non-rumor, unverified rumor\\n- Task 2\\n- detect rumor as early as possible"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 37,
        "texts": [
          "pP Hoe mOLeMetelCO ee i ee ae ee ee eee eee1 & cai aoaatt Aen to es\\n\\nText Features Annie Mae Gamiemae1000-19 0c 201 \\mai jonates 10\\n\\nSupport! arren Wilson and ongoing racist police murders\\n\\n#Ferguson\\n#BoycottWalmart\\n\\nS Se e7omena P FOpagation & Temporal\\n\\n@meinooooo\\n@70torinoman\\n\\nI doubt they did but if it turns out to be true then I1 ee Me, BM\\n\\n'ek Seah aH a\\n\\nGood For Them!\\n\\nFeatures:\\n\\nNE SE ee Sey\\n\\nacorr me eR a a\\n\\nMelanie B @meinooo Oct\\n2014.\\n\\nBol @anniemae1000\\n@70torinoman\\n\\nIt'd be really inhumane if they did. If they did.\\n- Th th I'c ey support murder basically!\\n\\ncL SONY VS GO\\nViva La Revolucion\\n\\nI@70torinoman\\n\\n'I alntaw'\\n\\nHeu davvrted ican\\n\\nRevolutionary: The US capitalist\\n\\nAnnie Mae @anivemae1000\\n\\nThe system was founded on the genocide of\\na) a70t\\n\\nI think th port protecting th tore fiI\\n\\n+ bbu Ly Dawrert Wilear fol ance eH\\n\\nGees @meinooooo\\n@70torinoman\\n\\nink they support protecting their store from\\n\\n1i exploitation of workers.\\n\\nThis must be looters\\n\\nUser Features | Um162K\\n\\n9,954 12K\\n\\nMelanie B @meinooooo\\n- 19 Oct 2014\\n\\n@anniemae1000\\n@70torinoman\\n\\nDoubt it. They've already fixed the store &\\n\\nI' ae\\n\\nSP pee pe\\n\\nSe ee ss\\n\\nAnnie Mae @anniemae1000\\n- 19 Oct 2014\\n\\n& @meinooooo\\n@70torinoman\\n\\nWhatever\\n\\nWalmart donates $10,000 to support\\n\\nDarren Wilson and the ongoing racist police murders.\\n\\n@anniemae1000\\n@meinooooo\\n\\nThe privilege of \"whatever\"\\n\\nNeed proof of it?\\n\\nYou don't honestly believe that, do you?\\n\\nNot sure....sorry I a\\n\\nI honestly do see a meme trending\\n\\nbut no proof...perhaps\\n\\nSam Walton gave if we had real journalists?\\n\\n300k to Obama's campaign?\\n\\nWhere is the credible link?\\n\\nI'm pretty good at pretty gv research\\n- I think this is not\\n\\nSam Walton was dead before #Obama was born.\\n\\nReasons to boycott: WalMart.\\n\\nHe have wired campaign donation from heavens."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 38,
        "texts": [
          "Observation & Hypothesis\\n\\nSupport \"oo\" Lh a f ae 18h SyNeutral i £ \\e oe i i xh O:Sh é.\\n\\nDoubt\\n- In rumors: Influence/Popularity\\n- In non-tumors: Content-based signals (e.g., stance) (Zhao et al.,\\n2015)\\n- Network-based signals (e.g., relative influence) and temporal traits (Kwon et al.,\\n2017)\\n\\nOur hypothesis: high-order patterns need to/could be captured using kernel method"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 39,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 40,
        "texts": [
          "Traditional Tree Kernel (TK) : NP | NP VP V NP D N D N pt ~ poi V NP D N ia tree | acut D N NP ' D N || | a N por, a tree D N a tree = TK compute the syntactic similarity between two sentences by counting the common subtrees = Kernel Function: )iy, cv, div jE V> A(v%j, V;)7 A(v;, vj ): common subtrees rooted at v; and 1;"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 41,
        "texts": [
          "Propagation Tree Kernel (PTK)\\n\\nWhy PTK?\\n\\nExisting tree kernel cannot apply here, since in our case (1) node is a vector of continuous numerical values; (2) similarity needs to be softly defined between two trees instead of hardly counting on identical nodes.\\n\\nSimilarity Definition\\n\\nUser Similarity: E(uj, Uj) = |[U;\\n- u;||. co oy Waram(ci)n Ngram(cj)|\\n\\nContent Similarity: J(- j, cj) = INgram(c; UNgram(c))|\\n\\nNode Similarity: f(vi, vj) = e(GEC;\\nw) + HI-DI(Cj, G)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 42,
        "texts": [
          "Propagation Tree Kernel = Given two trees T1 =< V1, E1 > and T2 =< V2, E2 >, PTK computes similarity between them by enumerating all similar subtrees.\\n- Kernel Function: K(T1, T2) + div E(V1, A(v1, v2)) = v1 and v2 are similar node pairs from V1 and V2 respectively\\n\\nv2 = arg max f(Vj, Yj) vj ∈ V2 = A(v, v'): similarity of two subtrees rooted at v and v'\\n- Kernel algorithm\\n1) if v or v' are leaf nodes, then\\nA(v, v') = f(v, v');\\n2) else\\nA(v, v') = f(v, v')"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 43,
        "texts": [
          "Context-Sensitive Extension of PTK\\n\\nConsider propagation paths from root node to the subtree.\\n\\nWhy cPTK?\\n\\nPTK ignores the clues outside the subtrees and the route embed how the propagation happens.\\n\\nSimilar intuition to context-sensitive tree kernel (Zhou et al., 2007).\\n\\nKernel Function:\\n\\nYiy, ev, Yao Ax (Vi vj) + div jv, dixng Axl,\\n\\nL7: the length of propagation path from root r to v.\\n\\nContexrpath= v(x): the x-th ancestor of v.\\n\\nLe 7 .Pree actos ecet i\\n- 1/\\n\\nA,(v,v'): similarity of subtrees rooted at v[x] and v'[x].\\n\\nKernel Algorithm:\\n1) if v[x] and v'[x] are the x-th ancestor nodes of v and v', then A,(v,v') = f(v[x], v'[x])\\n2) else: A,(v, v') = A(v,v') (i.e., PTK)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 44,
        "texts": [
          "Rumor Detection via Kernel Learning\\n- Incorporate the proposed tree kernel functions (i.e., PTK or cPTK) into a supervised learning framework, for which we utilize a kernel-based SVM classifier.\\n- Avoid feature engineering\\n- the kernel function can explore an implicit feature space when calculating the similarity between two objects.\\n- For multi-class task, perform One vs.\\nall, i.e., building K (# of classes) basic binary classifiers so as to separate one class from all the others."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 45,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 46,
        "texts": [
          "Data Collection = Construct our propagation tree datasets based on two reference Twitter datasets:\\n\\n= Twitter15 (Liu et al,\\n2015)\\n\\n= Twitter16 (Ma et al,\\n2016)\\n\\nConvert event label: Extract popular source tweets propagation binary -> quaternary threads (Source tweet: highly retweeted or replied) (retweets: Twrench.com) (replies: Web crawler)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 47,
        "texts": [
          "Statistics of Data Collection\\n\\nStatistic                   Twitter15     Twitter16\\n\\n# of users                 276,663       173,487\\n# of source tweets         1,490         818\\n# of threads               331,612       204,820\\n# of non-rumors            374           205\\n# of false rumors          370           205\\n# of true rumors           312           205\\n# of unverified rumors     374           203\\nAvg. time length / tree    1,337 Hours   848 Hours\\nAvg. # of posts / tree     251\\nMax # of posts / tree      1,768         2,765\\nMin # of posts / tree      81\\n\\nURL of the datasets:\\nhttps://www.dropbox.com/s/Ojhsfwep3ywvpca/rumdetect2017.zip?dl=0"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 48,
        "texts": [
          "Approaches to compare with:\\n- DTR: Decision tree-based ranking model using enquiry phrases to identify trending rumors (Zhao et al.,\\n2015)\\n- DTC and SVM-RBF: Twitter information credibility model using Decision Tree Classifier (Castillo et al., 2011); SVM-based model with RBF kernel (Yang et al.,\\n2012)\\n- RFC: Random Forest Classifier using three parameters to fit the temporal tweets volume curve (Kwon et al.,\\n2013)\\n- SVM-TS: Linear SVM classifier using time-series structures to model the variation of social context features (Ma et al.,\\n2015)\\n- GRU: The RNN-based rumor detection model (Ma et al.,\\n2016)\\n- BOW: Linear SVM classifier using bag-of-words\\n- Ours (PTK and cPTK): Our kernel based model\\n- PTK- and cPTK-: Our kernel based model with subset node features."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 49,
        "texts": [
          "Results on Twitter\\n\\nNR: Non-Rumor;\\nFR: False Rumor;\\nTR: True Rumor;\\nUR: Unverified Rumor;\\n\\n0.409 0.501 0.311 0.364 0.473\\n0.318 0.455 0.037 0.218 0.225\\n0.454 0.733 0.355 0.317 0.415\\n0.544 0.796 0.472 0.404 0.483\\n0.565 0.810 0.422 0.401 0.543\\n0.646 0.792 0.574 0.608 0.592\\n0.548 0.564 0.524 0.582 0.512\\n0.657 0.734 0.624 0.673 0.612\\n0.697 0.760 0.645 0.696 0.689\\n0.710 0.825 0.685 0.688 0.647\\n0.750 0.804 0.698 0.765 0.733"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 50,
        "texts": [
          "Results on Twitter\\n\\nNR: Non-Rumor;\\nFR: False Rumor;\\nTR: True Rumor;\\nUR: Unverified Rumor;\\n\\n0.414\\n0.394\\n0.273\\n0.630\\n0.344\\n\\n0.321\\n0.423\\n0.085\\n0.419\\n0.037\\n\\n0.465\\n0.643\\n0.393\\n0.419\\n0.403\\n\\n0.574\\n0.755\\n0.420\\n0.571\\n0.526\\n\\n0.585\\n0.752\\n0.415\\n0.547\\n0.563\\n\\n0.633\\n0.772\\n0.489\\n0.686\\n0.593\\n\\n0.585\\n0.553\\n0.556\\n0.655\\n0.578\\n\\n0.653\\n0.673\\n0.640\\n0.722\\n0.567\\n\\n0.702\\n0.711\\n0.664\\n0.816\\n0.608\\n\\n0.722\\n0.784\\n0.690\\n0.786\\n0.644\\n\\n0.732\\n0.740\\n0.709\\n0.836\\n0.686"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 51,
        "texts": [
          "Results on Early Detection\\n\\n85+\\n- BOW\\n- RFC 0.85\\n- BOW\\n- RFCos\\n- PTK\\n- DTR 08\\n- PTK\\n- DIR\\n\\n\"PT OR egy\\n\\nDetection Deadline (hours)\\n\\n(a) Twitter15 DATASET\\n(b) Twitter16 DATASET\\n\\nIn the first few hours, the accuracy of the kernel-based methods climbs more rapidly and stabilizes more quickly.\\n\\nPTK can detect rumors with 72% accuracy for Twitter15 and 69.0% for Twitter16 within 12 hours, which is much earlier than the baselines and the mean official report times."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 52,
        "texts": [
          "Early Detection Example\\n\\nExample subtree of a rumor captured by the algorithm at early stage of propagation\\n- #Walmart donates $10,000 to #DarrenWilson fund to continue police racial profiling, brutality, murder of black ppl\\n- Judging by the way #Walmart pays & treats its employees this is no surprise.\\n- That's Wal-Mart always doing good for the overlords of society.\\n- lol\\n- RT\\n- NEED SOURCE. have a feeling this is just hearsay or they donated to backstoppers or something tangential.\\n- Lagree. I have been hearing this all day but no source\\n- Exactly, I don't think Wal-Mart would let everyone know this if they did!!\\n\\nTime delay after root (hours) Influential users boost its propagation, unpopular-to-popular information flow, Textual signals (underlined)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 53,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 54,
        "texts": [
          "Conclusion and future work:\\n- Apply kernel learning method for rumor debunking by utilizing the propagation tree structures.\\n- Propagation tree encodes the spread of a source tweet with complex structured patterns and flat information regarding content, user, and time associated with the tree nodes.\\n- Our kernel are combined under supervised framework for identifying rumors of finer-grained levels by directly measuring the similarity among propagation trees.\\n\\nFuture work:\\n- Explore network representation method to improve the rumor detection task.\\n- Develop unsupervised models due to massive unlabeled data from social media."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 55,
        "texts": [
          "7. a j gee,d bi tl : ( a r A£ $ . 4 fi ty e / > a )ra [oe 5 wh ye a . le4 yay 78 7 le rig. PA /.\\n' es Js y, + \"of ; 4 Ter ed fe J . fa A os ore 4 ff j r 7A 4 z ) 4 v7 y SS MS Sf Ga , wy)tad 7 7 oy ee / l \" 7(7 \" L\\n- b\\n- L oe2016/7/1"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 56,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 57,
        "texts": [
          "Introduction\\n\\nThe fake news went viral.\\nEric Tucker 40 followers of Anti-Trump protestors in Austin today are not as organic as they seem.\\nHere Gateway Pundit >44k shares are the buses they came in. Figures.\\n\\nAnti-Trump protestors were bussed in to Austin #FakeProtests.\\n\\n16,931 14,521 Reddit 363 comments. Donald J. Trump 2.28M followers booed.\\n\\nBREAKING: They found the buses! We see them lined up just blocks away from the Austin protests.\\n\\nProfessional protesters, incited by the media, are protesting. Very unfair!\\n\\nMany asking for concrete evidence.\\n\\nThey found the buses!\\n\\nDozens have been lined up just blocks away from the Austin protests.\\n\\nEric Tucker 40 followers did not see loading or unloading. There were even more buses within pics.\\nQuite near protests at right timing.\\n\\nIndicates the level of influence. Start from grass-roots users, promoted by some influential accounts, widely spread.\\n\\nJing Ma (CUHK) 2016/7/1"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 58,
        "texts": [
          "Motivation = \"We generally are not good at distinguishing rumors.\\nIt is crucial to track and debunk rumors early to minimize their harmful effects.\\nOnline fact-checking services have limited topical coverage and long delay. Existing models use feature engineering\\n- oversimplistic; or recently deep neural networks\\n- ignore propagation structures.\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 59,
        "texts": [
          "Contributions:\\n- Represent information spread on Twitter with propagation tree, formed by harvesting user's interactions, to capture high-order propagation patterns of rumors.\\n- Propose a kernel-based data-driven method to select relevant features automatically for estimating the similarity between two propagation trees.\\n- Enhance the proposed model by considering propagation paths from source tweet to subtrees to capture the context of transmission.\\n- Release two real-world Twitter datasets with finer-grained ground truth labels."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 60,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 61,
        "texts": [
          "Related Work\\n- Systems based on common sense and investigative journalism, e.g., snopes.com, factcheck.org\\n- Learning-based models for rumor detection\\n- Information credibility: Castillo et al. (2011), Yang et al. (2012)\\n- Using handcrafted and temporal features: Liu et al. (2015), Ma et al. (2015), Kwon et al. (2013,\\n2017)\\n- Using cue terms: Zhao et al. (2015)\\n- Using recurrent neural networks: Ma et al. (2016)\\n- Kernel-based works\\n- Tree kernel: syntactic parsing (Collins and Duffy,\\n2001)\\n- Question-answering (Moschitti,\\n2006)\\n- Semantic analysis (Moschitti,\\n2004)\\n- Relation extraction (Zhang et al.,\\n2008)\\n- Machine translation (Sun et al., 2010)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 62,
        "texts": [
          "Outline\\n= Introduction\\n= Related Work\\n= Tweets Propagation\\n= Kernel Modeling\\n= Evaluation\\n= Conclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 63,
        "texts": [
          "Problem Statement = Given a set of microblog posts R = {r}, model each source tweet as a tree structure T(r) = < V, E >, where each node v = (uy, Cy,\\nt) provides the creator of the post, the text content and post time.\\nAnd E is directed edges corresponding to response relation.\\n\\n= Task 1\\n- finer-grained classification for each source post: false rumor, true rumor, non-rumor, unverified rumor\\n\\n= Task 2\\n- detect rumor as early as possible"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 64,
        "texts": [
          "pP Hoe mOLeMetelCO ee i ee ae ee ee eee eee1 & cai aoaatt Aen to es\\n\\nText Features\\n\\nAnnie Mae Gamiemae1000-19 0c 201\\nmaijonates 10\\n\\nSupport!\\n\\nDarren Wilson and ongoing racist police murders\\n\\n#Ferguson #BoycottWalmart\\n\\nSe e7omenaP FOpagation & Temporal\\n\\ndoubt they did but if it turns out to be true then I\\n\\nGood For Them!\\n\\nFeatures:\\n\\nMelanie B @meinooo Oct 2014\\n\\n@anniemae1000 @7Otorinoman it'd be really inhumane if they did. If they did.\\n\\nI think they support murder basically!\\n\\nViva La Revolucion\\n\\n@70torinoman\\n- I alntaw'\\n\\nHeu davvrted ican / Revolutionary: The US capitalist system was founded on the genocide of a\\n\\nI think they support protecting their store from looters.\\n\\nUser Features | Um162K 9,954 12K\\n\\nMelanie B @meinooooo\\n- 19 Oct 2014\\n\\n@anniemae1000 @7Otorinoman doubt it. They've already fixed the store & have it protected.\\nCorporate had to have sent that.\\n\\nAnnie Mae @anniemae1000\\n- 19 Oct 2014\\n\\n@meinooooo @70torinoman whatever\\n\\nWalmart donates $10,000 to support Darren Wilson and the ongoing racist police murders\\n\\nthe privilege of \"whatever\"\\n\\nNeed proof of it\\n\\nYou don't honestly believe that, do you?\\n\\nnot sure....sorry\\n\\nI honestly do see a meme trending, but no proof...perhaps\\n\\nSam Walton gave...\\n\\nif we had real journalists?\\n\\nwhere is the credible link?\\n\\nI'm pretty good at pretty good research\\n- I think this is not\\n\\nSam Walton was dead before #Obama was born.\\n\\nreasons to boycott: WalMart."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 65,
        "texts": [
          "Observation & Hypothesis\\n\\nSupport\\n\\n(a) In rumors: Influence/Popularity\\n\\n(b) In non-tumors: Content-based signals (e.g., stance) (Zhao et al.,\\n2015)\\n\\nNetwork-based signals (e.g., relative influence) and temporal traits (Kwon et al.,\\n2017)\\n\\nOur hypothesis: high-order patterns need to/could be captured using kernel method"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 66,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 67,
        "texts": [
          "Traditional Tree Kernel (TK) : NP | NP VP V NP D N D N pt ~ poi V NP D N i.a tree | acut D N NP ' D N || | a N por, a tree D N a tree = TK compute the syntactic similarity between two sentences by counting the common subtrees = Kernel Function: ) iy, cv, div j E V > A(v % j, V;) 7 A(v;, vj ): common subtrees rooted at v; and 1;"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 68,
        "texts": [
          "Propagation Tree Kernel (PTK)\\n\\nWhy PTK?\\nExisting tree kernel cannot apply here, since in our case (1) node is a vector of continuous numerical values; (2) similarity needs to be softly defined between two trees instead of hardly counting on identical nodes.\\n\\nSimilarity Definition\\n\\nUser Similarity: E(uj, Uj) = |[U;\\n- u;||. co oy Waram(ci)nNgram(cj)|\\n\\nContent Similarity: J(- j, cj) = INgram(c;UNgram(c))|\\n\\nNode Similarity: f(vi, vj) = e (GEC;\\nw) + HI-DI(Cj, G)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 69,
        "texts": [
          "Propagation Tree Kernel = Given two trees T1 =< V1, E1 > and T2 =< V2, E2 >, PTK computes similarity between them by enumerating all similar subtrees.\\n- Kernel Function: A(y1, y2, A1, Y2) + div EV, A(v1, v2) = v1 and v2 are similar node pairs from V1 and V2 respectively.\\n\\nv2 = arg max f(Vj, Yj) where vj ∈ V2 = A(v, v'): similarity of two subtrees rooted at v and v'.\\n\\nKernel algorithm:\\n1) if v or v' are leaf nodes, then A(v, v') = f(v, v');\\n2) else A(v, v') = f(w, v)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 70,
        "texts": [
          "Context-Sensitive Extension of PTK\\n\\nConsider propagation paths from root node to the subtree.\\n\\nWhy cPTK? PTK ignores the clues outside the subtrees and the route embed how the propagation happens.\\nSimilar intuition to context-sensitive tree kernel (Zhou et al., 2007).\\n\\nKernel Function:\\n\\nYiy, ev, Yao Ax (Vi vj) + div jv, dixng Axl, L7,: the length of propagation path from root r to v.\\nContext path = v(x): the x-th ancestor of v.\\n\\nA,(v, v'): similarity of subtrees rooted at v[x] and v'[x].\\n\\nKernel Algorithm:\\n1) If v[x] and v'[x] are the x-th ancestor nodes of v and v', then\\n\\nA,(v, v') = f(v[x], v'[x])\\n2) Else: A,(v, v') = A(v, v') (i.e., PTK)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 71,
        "texts": [
          "Rumor Detection via Kernel Learning\\n- Incorporate the proposed tree kernel functions (i.e., PTK or cPTK) into a supervised learning framework, for which we utilize a kernel-based SVM classifier.\\n- Avoid feature engineering\\n- the kernel function can explore an implicit feature space when calculating the similarity between two objects.\\n- For multi-class task, perform One vs.\\nall, i.e., building K (# of classes) basic binary classifiers so as to separate one class from all the others."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 72,
        "texts": [
          "Outline\\n\\nIntroduction\\n\\nRelated Work\\n\\nTweets Propagation\\n\\nKernel Modeling\\n\\nEvaluation\\n\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 73,
        "texts": [
          "Data Collection = Construct our propagation tree datasets based on two reference Twitter datasets:\\n\\n= Twitter15 (Liu et al,\\n2015)\\n\\n= Twitter16 (Ma et al,\\n2016)\\n\\nConvert event label: Extract popular source tweets propagation binary -> quaternary threads (Source tweet: highly retweeted or replied) (retweets: Twrench.com) (replies: Web crawler)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 74,
        "texts": [
          "Statistics of Data Collection\\n\\nStatistic                   Twitter15    Twitter16\\n# of users                  276,663      173,487\\n# of source tweets          1,490        818\\n# of threads                331,612      204,820\\n# of non-rumors             374          205\\n# of false rumors           370          205\\n# of true rumors            312          205\\n# of unverified rumors      374          203\\nAvg. time length / tree     1,337 Hours   848 Hours\\nAvg. # of posts / tree      251\\nMax # of posts / tree       1,768        2,765\\nMin # of posts / tree       81\\n\\nURL of the datasets:\\nhttps://www.dropbox.com/s/Ojhsfwep3ywvpca/rumdetect2017.zip?dl=0"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 75,
        "texts": [
          "Approaches to compare with:\\n- DTR: Decision tree-based ranking model using enquiry phrases to identify trending rumors (Zhao et al.,\\n2015)\\n- DTC and SVM-RBF: Twitter information credibility model using Decision Tree Classifier (Castillo et al., 2011); SVM-based model with RBF kernel (Yang et al.,\\n2012)\\n- RFC: Random Forest Classifier using three parameters to fit the temporal tweets volume curve (Kwon et al.,\\n2013)\\n- SVM-TS: Linear SVM classifier using time-series structures to model the variation of social context features (Ma et al.,\\n2015)\\n- GRU: The RNN-based rumor detection model (Ma et al.,\\n2016)\\n- BOW: Linear SVM classifier using bag-of-words\\n- Ours (PTK and cPTK): Our kernel based model\\n- PTK- and cPTK-: Our kernel based model with subset node features"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 76,
        "texts": [
          "Results on Twitter\\n\\n15NR: Non-Rumor;\\nFR: False Rumor;\\nTR: True Rumor;\\nUR: Unverified Rumor;\\n\\n0.409\\n0.501\\n0.311\\n0.364\\n0.473\\n\\n0.318\\n0.455\\n0.037\\n0.218\\n0.225\\n\\n0.454\\n0.733\\n0.355\\n0.317\\n0.415\\n\\n0.544\\n0.796\\n0.472\\n0.404\\n0.483\\n\\n0.565\\n0.810\\n0.422\\n0.401\\n0.543\\n\\n0.646\\n0.792\\n0.574\\n0.608\\n0.592\\n\\n0.548\\n0.564\\n0.524\\n0.582\\n0.512\\n\\n0.657\\n0.734\\n0.624\\n0.673\\n0.612\\n\\n0.697\\n0.760\\n0.645\\n0.696\\n0.689\\n\\n0.710\\n0.825\\n0.685\\n0.688\\n0.647\\n\\n0.750\\n0.804\\n0.698\\n0.765\\n0.733"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 77,
        "texts": [
          "Results on Twitter\\n\\n16NR: Non-Rumor;\\nFR: False Rumor;\\nTR: True Rumor;\\nUR: Unverified Rumor;\\n\\n0.414 0.394 0.273 0.630 0.344\\n0.321 0.423 0.085 0.419 0.037\\n0.465 0.643 0.393 0.419 0.403\\n0.574 0.755 0.420 0.571 0.526\\n0.585 0.752 0.415 0.547 0.563\\n0.633 0.772 0.489 0.686 0.593\\n0.585 0.553 0.556 0.655 0.578\\n0.653 0.673 0.640 0.722 0.567\\n0.702 0.711 0.664 0.816 0.608\\n0.722 0.784 0.690 0.786 0.644\\n0.732 0.740 0.709 0.836 0.686"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 78,
        "texts": [
          "Results on Early Detection\\n\\n85+\\n- BOW\\n- RFC 0.85\\n- BOW\\n- RFCos\\n- PTK\\n- DTR 08\\n- PTK\\n- DIRcig, \"PT OR egy te RT eeBa 07 a.\\n\\nDetection Deadline (hours)\\nDetection Deadline (hours)\\n\\n(a) Twitter15 DATASET\\n(b) Twitter16 DATASET\\n\\nIn the first few hours, the accuracy of the kernel-based methods climbs more rapidly and stabilizes more quickly.\\nPTK can detect rumors with 72% accuracy for Twitter15 and 69.0% for Twitter16 within 12 hours, which is much earlier than the baselines and the mean official report times."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 79,
        "texts": [
          "Early Detection Example\\n\\nExample subtree of a rumor captured by the algorithm at early stage of propagation:\\n- #Walmart donates $10,000 to #DarrenWilson fund to continue police racial profiling, brutality, murder of black ppl\\n- Judging by the way #Walmart pays & treats its employees this is no surprise.\\n- That's Wal-Mart always doing good for the overlords of society.\\n- lol [8] RT\\n- RT\\n- NEED SOURCE. have a feeling this is just hearsay or they donated to backstoppers or something tangential.\\n- Lagree. I have been hearing this all day but no source\\n- Exactly, I don't think Wal-Mart would let everyone know this if they did!!\\n\\nTime delay after root (hours)\\n\\nInfluential users boost its propagation, unpopular-to-popular information flow, Textual signals (underlined)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 80,
        "texts": [
          "Outline\\nIntroduction\\nRelated Work\\nTweets Propagation\\nKernel Modeling\\nEvaluation\\nConclusion and Future Work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 81,
        "texts": [
          "Conclusion and future work:\\n- Apply kernel learning method for rumor debunking by utilizing the propagation tree structures.\\n- Propagation tree encodes the spread of a source tweet with complex structured patterns and flat information regarding content, user and time associated with the tree nodes.\\n- Our kernel are combined under supervised framework for identifying rumors of finer-grained levels by directly measuring the similarity among propagation trees.\\n\\nFuture work:\\n- Explore network representation method to improve the rumor detection task.\\n- Develop unsupervised models due to massive unlabeled data from social media."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 82,
        "texts": [
          "7. a j gee, d bi tl: (a r A£ $. 4 fi ty e/>\\na) ra [oe 5 wh ye a. le4 yay 78 7 le rig. PA /. ' es Js y, + \"of; 4 Ter ed fe J. fa A os ore 4 ff j r 7A 4\\nz) 4 v7 y SS MS Sf Ga, wy)tad 7 7 oy ee / l \" 7(7 \" L\\n- b\\n- L oe 2016/7/1"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 210,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          ""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "- Innovation = Novelty + Impact"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Challenges to Innovation\\n- Lack of value focus\\n- Lack of reproducibility\\n- Lack of (domain) data\\n- Overemphasis on test scores\\n- Difficulty of adoption\\n- Timelines"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "From Business Problem to NLP Problem\\n\\nThe \"cheat sheet\"\\n\\nNaess No No = Ys -- Ly\\n\\nRefine your problem\\n\\nZero Questions\\n\\nManual work\\n\\nRule-based\\n\\nManual work | Ruled | Yes\\n\\nNo Yes Yes Yes Yes\\n- NLP"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "SAP is hiring ML and NLP experts!\\n\\nCome and talk to us at the SAP booth!\\n\\nMachine Learning and the larger world of artificial intelligence (AI) are no longer the stuff of science fiction.\\nThey're here\\n- and many businesses are already taking advantage.\\n\\nAs a new breed of software that is able to learn without being explicitly programmed, machine learning (and deep learning) can access, analyze, and find patterns in Big Data in a way that is beyond human capabilities.\\n\\nThe business advantages are huge, and the market is expected to be worth $47 billion by\\n2020.\\n\\n© 2017 SAP SE or an SAP affiliate company. All rights reserved. PUBLIC"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "© 2017 SAP SE or an SAP affiliate company. All rights reserved.\\n\\nNo part of this publication may be reproduced or transmitted in any form or for any purpose without the express permission of SAP SE or an SAP affiliate company.\\n\\nThe information contained herein may be changed without prior notice.\\nSome software products marketed by SAP SE and its distributors contain proprietary software components of other software vendors.\\nNational product specifications may vary.\\n\\nThese materials are provided by SAP SE or an SAP affiliate company for informational purposes only, without representation or warranty of any kind, and SAP or its affiliated companies shall not be liable for errors or omissions with respect to the materials.\\nThe only warranties for SAP or SAP affiliate company products and services are those that are set forth in the express warranty statements accompanying such products and services, if any.\\nNothing herein should be construed as constituting an additional warranty.\\n\\nIn particular, SAP SE or its affiliated companies have no obligation to pursue any course of business outlined in this document or any related presentation, or to develop or release any functionality mentioned therein.\\nThis document, or any related presentation, and SAP SE's or its affiliated companies' strategy and possible future developments, products, and/or platform directions and functionality are all subject to change and may be changed by SAP SE or its affiliated companies at any time for any reason without notice.\\n\\nThe information in this document is not a commitment, promise, or legal obligation to deliver any material, code, or functionality.\\nAll forward-looking statements are subject to various risks and uncertainties that could cause actual results to differ materially from expectations.\\nReaders are cautioned not to place undue reliance on these forward-looking statements, and they should not be relied upon in making purchasing decisions.\\n\\nSAP and other SAP products and services mentioned herein, as well as their respective logos, are trademarks or registered trademarks of SAP SE (or an SAP affiliate company) in Germany and other countries.\\nAll other product and service names mentioned are the trademarks of their respective companies.\\n\\nSee http://global.sap.com/corporate-en/legal/copyright/index.epx for additional trademark information and notices."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Appendix"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "SAP's vision for enterprise machine learning\\n\\nSAP Leonardo\\n\\nMachine Learning\\n\\nCreate your own intelligent infrastructure\\n\\nAutomate Knowledge Work\\n\\nDo the Impossible\\n- Contextual Concur travel concierge\\n- Video-aware marketing\\n- Lights out finance operations\\n- Drone and satellite-based asset management\\n- Self-driving customer service\\n- Conversational sales bots\\n- Vision-enabled manufacturing\\n- Customer retention insights\\n- Contextual logistics\\n\\nSAP S/4HANA\\n\\nSAP Cloud Platform and SAP HANA © 2017 SAP SE or an SAP affiliate company. All rights reserved. PUBLIC"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Build your intelligent enterprise now\\n\\nSAP makes machine learning incredibly simple\\n\\nSAP Assets\\nSAP Leonardo Business Outcomes\\n\\nMachine Learning; Increase revenue with superior sales targeting and execution\\n\\nRe-imagine business processes\\n- with digital intelligence\\n\\nImproving quality time at work for employees\\n\\nIncreased customer satisfaction with superior service\\n\\nEnabling product, process and business model innovations\\n\\n© 2017 SAP SE or an SAP affiliate company. All rights reserved.\\nPUBLIC"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "- a odOE Y=3 10] Innovation = Novelty + Impact"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Challenges to Innovation\\n- Lack of value focus\\n- Lack of reproducibility\\n- Lack of (domain) data\\n- Overemphasis on test scores\\n- Difficulty of adoption\\n- Timelines"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "From Business Problem to NLP Problem\\n\\nThe \"cheat sheet\"\\n\\nNaess No No = Ys -- Ly\\n\\nRefine your problem\\n\\nZero Qves\\n\\nManual work\\n\\nRule-based\\n\\nManner work | Ruled | Yes No Yes Yes Yes Yes\\n- NLP"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "SAP is hiring ML and NLP experts! Come and talk to us at the SAP booth!\\n\\nMachine learning and the larger world of artificial intelligence (AI) are no longer the stuff of science fiction.\\nThey're here\\n- and many businesses are already taking advantage.\\nAs a new breed of software that is able to learn without being explicitly programmed, machine learning (and deep learning) can access, analyze, and find patterns in Big Data in a way that is beyond human capabilities.\\n\\nThe business advantages are huge, and the market is expected to be worth $47 billion by\\n2020.\\n\\n© 2017 SAP SE or an SAP affiliate company. All rights reserved."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "© 2017 SAP SE or an SAP affiliate company. All rights reserved.\\n\\nNo part of this publication may be reproduced or transmitted in any form or for any purpose without the express permission of SAP SE or an SAP affiliate company.\\nThe information contained herein may be changed without prior notice.\\nSome software products marketed by SAP SE and its distributors contain proprietary software components of other software vendors.\\nNational product specifications may vary.\\n\\nThese materials are provided by SAP SE or an SAP affiliate company for informational purposes only, without representation or warranty of any kind, and SAP or its affiliated companies shall not be liable for errors or omissions with respect to the materials.\\nThe only warranties for SAP or SAP affiliate company products and services are those that are set forth in the express warranty statements accompanying such products and services, if any.\\nNothing herein should be construed as constituting an additional warranty.\\n\\nIn particular, SAP SE or its affiliated companies have no obligation to pursue any course of business outlined in this document or any related presentation, or to develop or release any functionality mentioned therein.\\nThis document, or any related presentation, and SAP SE's or its affiliated companies' strategy and possible future developments, products, and/or platform directions and functionality are all subject to change and may be changed by SAP SE or its affiliated companies at any time for any reason without notice.\\n\\nThe information in this document is not a commitment, promise, or legal obligation to deliver any material, code, or functionality.\\nAll forward-looking statements are subject to various risks and uncertainties that could cause actual results to differ materially from expectations.\\nReaders are cautioned not to place undue reliance on these forward-looking statements, and they should not be relied upon in making purchasing decisions.\\n\\nSAP and other SAP products and services mentioned herein as well as their respective logos are trademarks or registered trademarks of SAP SE (or an SAP affiliate company) in Germany and other countries.\\nAll other product and service names mentioned are the trademarks of their respective companies.\\n\\nSee http://global.sap.com/corporate-en/legal/copyright/index.epx for additional trademark information and notices."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Appendix"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "SAP's vision for enterprise machine learning\\nSAP Leonardo\\nMachine Learning\\nCreate your own intelligent infrastructure\\nAutomate Knowledge Work\\nDo the Impossible\\n- Contextual Concur travel concierge\\n- Video-aware marketing\\n- Lights out finance operations\\n- Drone and satellite-based asset management\\n- Self-driving customer service\\n- Conversational sales bots\\n- Vision-enabled manufacturing\\n- Customer retention insights\\n- Contextual logistics\\n\\nSAP S/4HANA\\nSAP Cloud Platform and SAP HANA\\n© 2017 SAP SE or an SAP affiliate company. All rights reserved.\\nPUBLIC"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Build your intelligent enterprise now\\n\\nSAP makes machine learning incredibly simple.\\n\\nSAP Assets\\n\\nSAP Leonardo Business Outcomes\\n\\nMachine Learning; Increase revenue with superior sales targeting and execution.\\n\\nRe-imagine business processes with digital intelligence across 25 industries.\\n\\nImproving quality time at work for employees.\\n\\nIncreased customer satisfaction with superior service.\\n\\nThe world's largest enabling product, process, and business model innovations.\\n\\n© 2017 SAP SE or an SAP affiliate company. All rights reserved. PUBLIC"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 60,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Introduction\\n\\nEnd-to-end dialog models based on encoder-decoder models have shown great promise for modeling open-domain conversations, due to their flexibility and scalability.\\n\\nDialog History/Context\\n\\nSystem Response"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Introduction However, dull response problem! [Li et al 2015, Serban et al. 2016]. Current solutions include:\\n- Add more info to the dialog context [Xing et al 2016, Li et al 2016]\\n- Improve decoding algorithm, e.g. beam search [Wiseman and Rush 2016]\\n\\nUser: I am feeling quite happy today.... (previous utterances) I don't know"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Our Key Insights\\n\\nResponse generation in conversation is a ONE-TO-MANY mapping problem at the discourse level.\\nA similar dialog context can have many different yet valid responses.\\nLearn a probabilistic distribution over the valid responses instead of only keeping the most likely one."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Our Key Insights\\n\\nB: I don't like cats. (disagree)\\n\\nB: Do you like cats?\\n\\nA: Yes, I do.\\n\\nB: So do I. (statement)\\n\\nB: What do you like about cats? (wh-question)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Our Contributions\\n1. Present an E2E dialog model adapted from Conditional Variational Autoencoder (CVAE).\\n2. Enable integration of expert knowledge via knowledge-guided CVAE.\\n3. Improve the training method of optimizing CVAE/VAE for text generation."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Conditional Variational Auto Encoder (CVAE)\\nCis dialog context\\n(a) qe (zle,\\nx)\\nB: Do you like cats?\\nA: Yes\\nZis the latent variable (gaussian)\\nZe Xis the next response\\noPpo(xlc,\\nz) :o =B:Sodol."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Conditional Variational Auto Encoder (CVAE) is dialog context\\n\\n(a) B: Do you like cats?\\n\\nA: Yes\\n\\nZ is the latent variable (Gaussian)\\n\\nX is the next response\\n\\np(x|c,\\nz) = -KL(p(z|e,\\nc) || p(z|e))\\n- E_{y, (z|c)} [log p(x|z, c)]\\n\\nBayes (SGVB) [Kingma and Welling 2013] < log p(a|c)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Knowledge-Guided CVAE (kgCVAE) Y is linguistic features extracted from responses\\n\\n(b) qe (zlc, x,\\ny)\\n\\nDialog act: statement -> \"So do I.\" (OK)\\n\\nUse Y to guide the learning of latent Z\\n\\nO/L(0, -; =--KL P /(9, 9; 2,\\n- ,\\ny) (qe(z|z,\\n- , y)||Po(zlc))\\n\\nve(xle, 2,\\n9)\\n\\nOc) WT E, (z|c, 2,\\ny) [log pl a2, Cc, y]|7 E, (z|c, x,\\ny) [log ply|z, c)|"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "-- Training of (kg) CVAE Reconstruction loss: like cats\\n\\nRecognition = (2) PPro I like cats | Ok Network\\n\\nlike cats\\n\\nif KL(qllp) = Utterance Encoder meta network\\n\\nResponse Decoder Gro: Ee = a Xbow | Conversation Floor\\n\\nKL-divergence loss cats"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Testing of (kg) CVAEu | like cats\\n\\nPrior (2) P-__ PL PL PtNetwork ' | like cats> Gs me ts05 =| | Ue | Uk-4 © ee08 me \"MLPy} oy'"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Optimization Challenge\\n\\nTraining CVAE with RNN decoder is hard due to the vanishing latent variable problem [Bowman et al., 2015].\\nThe RNN decoder can cheat by using LM information and ignore.\\n\\nBowman et al. [2015] described two methods to alleviate the problem:\\n1. KL annealing (KLA): gradually increase the weight of KL term from 0 to 1 (need early stop).\\n2. Word drop decoding: setting a proportion of target words to 0 (need careful parameter picking)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "BOW Losse Predict the bag-of-words in the responses X at once (word counts in the response) Break the dependency between words and eliminate the chance of cheating based on LM.\\nL'(0,6;x,c) = L(O,b;z,c) + Eq, (z\\c,2,y) log p(toow|2,€) (6)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "BOW Loss\\n\\nPredict the bag-of-words in the responses X at once (word counts in the response).\\nBreak the dependency between words and eliminate the chance of cheating based on LM.\\n\\nL'(0, 6; x,\\nc) = L(0, b; z,\\nc) + Ey, (z | c,\\ny) [log P1 Rie 2, c]\\n\\nPSK © RNN Loss\\nja @ FF Bag-of-word Loss"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Dataset Data Name: Switchboard Release 2\\n\\nNumber of dialogs: 2,400 (2316/60/62\\n- train/valid/test)\\n\\nNumber of context-response pairs: 207,833/5,225/5,481\\n\\nVocabulary Size: Top 10K\\n\\nDialog Act Labels: 42 types, tagged by SVM and human\\n\\nNumber of Topics: 70 tagged by humans"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Quantitative Metrics"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Quantitative Metrics\\n- Reference response M, Hypothesis response NN. Initial MO2 JEF1, MJd(r;, hi) precision(c)\\n- 1 JE[L, Me] OG. Appropriateness NMeja1 Mazieti, njd(r;, hi)\\n- recall(e) = MAP NA D0. Med(r,\\nh) is a distance function [0, 1] to measure the similarity between a reference and a hypothesis."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Distance Functions used for Evaluation\\n1. Smoothed Sentence-level BLEU (1/2/3/4): lexical similarity\\n2. Cosine distance of Bag-of-word Embeddings: distributed semantic similarity.\\n(pre-trained Glove embedding on twitter)\\na. Average of embeddings (A-bow)\\nb. Extrema of embeddings (E-bow)\\n3. Dialog Act Match: illocutionary force-level similarity\\na. (Use pre-trained dialog act tagger for tagging)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Models (trained with BOW Loss) sampling"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Quantitative Analysis Results\\n\\nMetrics | Perplexity | BLEU-1 BLEU-2 BLEU-3 BLEU-4 | A-bow E-bow DAty(KL) | (p/r) (p/r) (p/r) (p/r) (p/r) (p/r) (p/r)\\n\\nBaseline | 35.4 0.405/0.3/0.272/0.226/0.387/0.701/0.736/(sample) | (n/a) 0.336 0.281 0.254 0.215 0.337 0.684 0.514\\n\\nCVAE | 20.2 0.372/0.295/0.265/0.223/0.389/0.705/0.704/(greedy) | (11.36) 0.381 0.322 0.292 0.248 0.361 0.709 0.604\\n\\nkgCVAE | 16.02 0.412/0.350/0.310/0.262/0.373/0.711/0.721/(greedy) | (13.08) 0.411 0.356 0.318 0.272 0.336 0.712 0.598\\n\\nNote: BLEU are normalized into [0, 1] to be valid precision and recall distance function"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Qualitative Analysis\\n\\nTopic: Recycling\\n\\nContext:\\n\\nA: Are they doing a lot of recycling out in Georgia?\\n\\nTarget (statement): Well, at my workplace we have places for aluminium cans.\\n\\nBaseline + Sampling kg CVAE + Greedy\\n1. Well, I'm a graduate student and have two kids.\\n2. Well, I was in last year and so we've had a lot of recycling. Curbside pickup here.\\n3. I'm not sure.\\n4. Well, I don't know; I just moved here in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "- statement-non-opinion.\\nI A : eOo0 backchannel\\nVisualization of the posterior Z on the test dataset;\\nturn_exit a dataset in 2D space using t-SNE.\\nAssign different colors to the top 8 frequent\\nappreciation\\nwh-question of a dialog acts.\\nThe size of circle represents the response length.\\nExhibit clear clusterings of responses w.r.t the dialog act"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "The Effect of BOW Loss\\n\\nSame setup on Penn Tree Bank for LM Model Perplexity\\n\\nKL Cost [Bowman 2015]. Compare 4 setups:\\n- Standard 122.0 0.051.\\n- Standard VAE\\n- KL Annealing (KLA) KLA 111.5 2.0\\n- BOW a BONS MLA BOW 97.72 7.1\\n\\nGoal: reconstruction loss + small BOW + KLA 73.04 45.94 but non-trivial KL cost"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "KL Cost during Training\\nThe standard model suffers from vanishing latent variable. The standard model requires early stopping.\\nBOW leads to stable convergence with/without KLA.\\nThe same trend is observed on CVAE.\\nBatch Index"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Conclusion and Future Work\\n- Identify the ONE-TO-MANY nature of open-domain dialog modeling.\\n- Propose two novel models based on latent variables for generating diverse yet appropriate responses.\\n- Explore further in the direction of leveraging both past linguistic findings and deep models for controllability and explainability.\\n- Utilize crowdsourcing to yield more robust evaluation.\\n\\nCode available here! https://github.com/snakeztc/NeuralDialog-CVAE"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Thank you! Questions?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "1. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016a.\\nA persona-based neural conversation model. arXiv preprint arXiv:1603.06155.\\n2. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan.\\n2015. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055.\\n3. Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.\\n2015. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349.\\n4. Diederik P Kingma and Max Welling.\\n2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\\n5. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016a.\\nA persona-based neural conversation model. arXiv preprint arXiv:1603.06155."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Training Details\\n- Word Embedding: 200 Glove pre-trained on Twitter\\n- Utterance Encoder Hidden Size: 300\\n- Context Encoder Hidden Size: 600\\n- Response Decoder Hidden Size: 400\\n- Latent Z Size: 200\\n- Context Window Size: 10 utterances\\n- Optimizer: Adam learning rate=0.001"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "Testset Creation\\nUse 10-nearest neighbour to collect similar context in the training data.\\nLabel a subset of the appropriateness of the 10 responses by 2 human annotators.\\nBootstrap via SVM on the whole test set (5481 context/response).\\nResulting 6.79 Avg references responses/context.\\nDistinct reference dialog acts 4.2"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 71,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "rivals pro-choice pro-life undocumented illegal alien immigrants small friends\\n- free market government word friends machine alignment\\n\\ntranslation Chong and Druckman, 2007; Dawkins, 1976; Entman, 1993; Gitlin, 1980; Lakoff, 2014; Milton, 1964"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "First quantitative framework to systematically describe relations between ideas.\\nDemonstrate effective explorations with this framework on a wide range of datasets.\\n\\n28\" om = is weary d d rivals illegal. Wa\" . t rivals je aS j ae : Se un ocumen e 1 esaSoe O10.\\nOar immigrants alienWs Pr yee55 5O%0 mECre ts, pS we .\\neat aie small friends freexe: ce eo) ae 4a kofra7njeres government market. WeaGo<SN7a e 3"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "\"USING TEXT TO TRACE IDEAS\\n\\nClassification\\n- Probabilistic Models\\n\\nOur focus is on relations between ideas.\\n\\nWe will use standard approaches\\n- Topics from latent Dirichlet allocation (Blei et al.\\n2003)\\n- Keywords (Monroe et al.\\n2008)\\n- Culturomics, Michel et al. 2011"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "QUANTITATIVELY DESCRIBE RELATIONS BETWEEN IDEAS\\n- Given a corpus of documents over time, each document consists of a set of ideas undocumented Heals immigrants <----> illegal alien Cooccurrence.\\nRarely cooccur Pointwise mutual information [Church and Hanks 1990]"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "QUANTITATIVELY DESCRIBE RELATIONS BETWEEN IDEAS\\n- Given a corpus of documents over time, each document consists of a set of ideas\\n- Co-occurrence does not capture which is winning or losing undocumented immigrants a5 Pearson correlation = 0 . .\\na illegal alien time °"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "QUANTITATIVELY DESCRIBE RELATIONS BETWEEN IDEAS\\n- Given a corpus of documents over time, each document consists of a set of ideas.\\n\\nCooccurrence & Prevalence correlation\\n\\nWithin\\n- Across-document document"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "HEAD-TO-HEAD (ANTI-CORRELATED, RARELY CO-OCCUR)\\n\\n8 immigrant, mit undocumented santa ft\\n\\n+++ illegal, aliens\\n\\n1980 1990 2000 2010"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "RELATIONS BETWEEN IDEAS\\n- Always cooccur\\n- Anti-correlated\\n- Correlated\\n- Head-to-head\\n- Arms-race\\n- Rarely cooccur"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "FRIENDSHIP (CORRELATED, LIKELY TO COOCCUR)\\nimmigrant, -undocumented\\nTM\\n-\\nobama, president: 1980 1990 2000 2010"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "RELATIONS BETWEEN IDEAS\\n\\nAlways cooccur\\n- Tryst\\n- Friendship\\n\\nAnti-correlated\\n- Correlated\\n\\nHead-to-head\\n- Arms-race\\n\\nRarely cooccur\\n12"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "RELATIONS BETWEEN IDEAS\\n\\nAlways cooccur\\n- Tryst\\n- Friendship\\n- Anti-correlated\\n- Correlated\\n- Immigration, deportation\\n- Republican, party\\n\\nRarely cooccur"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "ARMS-RACE (CORRELATED, RARELY COOCCUR) -- immigration, deportation.\\nRepublican party"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "RELATIONS BETWEEN IDEAS\\n\\nAlways cooccur\\n- Tryst\\n- Friendship\\n\\nAnti-correlated\\n- Correlated\\n\\nHead-to-head\\n- Arms-race\\n\\nRarely cooccur"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "RELATIONS BETWEEN IDEAS\\n\\nAlways cooccur\\nimmigration, deportation\\ndetainee, detention\\n\\nTrust Friendship\\n\\n1980 1990 2000 2010\\n\\nAnti-correlated\\nCorrelated\\nimmigration, deportation\\n\\nRepublican party\\n\\nHead-to-head\\nArms-race\\n\\n1980 1990 2000 2010\\n\\nRarely cooccur"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "TRYST (ANTI-CORRELATED, LIKELY TO CO-OCCUR)\\n- immigration\\n- deportation\\n- detainee\\n- detention\\n\\n1980 1990 2000 2010"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "RELATIONS BETWEEN IDEAS\\n\\nAlways cooccur\\n- immigration, deportation to immigrants, detainees, detention, undocumented.\\n\\nWe have shown a framework to quantitatively describe relations between ideas.\\n\\nCan we use them to effectively explore relations between ideas?\\n\\nRarely cooccur."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "A WIDE RANGE OF DATASETS\\n- Newspapers and research articles as datasets\\n- Immigration\\n- Terrorism\\n- Same-sex marriage\\n- Abortion\\n- Tobacco\\n- ACL\\n- NIPS"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "JOINT DISTRIBUTIONS) Correlated, but many pairs in all four quadrants!\\n\\n0.6 pearsonr = 0.55\\n- Oo 0.4\\n- ote\" CE-0.6-1.0\\n- 0.5 0.0 0.5 1.0 prevalence correlation »"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "THE STRENGTH OF RELATIONS\\n\\nStrength = |PMI| x |correlation|\\n\\n0.6 Pearson r = 0.6\\n\\n0.4\\n\\nExtreme pairs are 50.0\\n\\nCull the interesting ones!\\n\\n0.2\\n\\n-1.0 -0.5 0.0 0.5 1.0 prevalence correlation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "EFFECTIVELY EXPLORE RELATIONS BETWEEN IDEAS\\n- Terrorism\\n- Keywords\\n- Topics"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "#2 IN TRYSTS\\n\\narab aunun islamE\\n\\nMs. a s, oe,\\n- .Qi oe? * oe\\n\\n1980 1990 2000 2010"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "EFFECTIVELY EXPLORE RELATIONS BETWEEN IDEAS\\n- Terrorism\\n- Keywords\\n- Topics"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "TOP RELATIONS BETWEEN IDEAS\\n\\nIran, Libya, Afghanistan, Taliban\\n\\nIsrael, Palestinian, Pakistan, India\\n\\nThe relations between these topics are consistent with structural balance theory: the enemy of an enemy is a friend [Cartwright and Harary, 1956; Heider, 1946]"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "EFFECTIVE EXPLORATIONS\\n\\nRank among all relations\\n\\nPMI Correlation Joint Keywords\\narab islam 106 1,494\\nfederal, state afghanistan 43 99\\ntaliban Topics federal, state iran, libya 36 56\\n\\nThe \"interesting\" pair is ranked much higher according to our framework."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Always cooccur\\nmachine translation\\nrule, forest methods\\nword alignment\\n\\nTests Friendship  /2000 2010\\n1980. 1990 2000\\nAnti-correlated\\nCorrelated\\nmachine translation\\ndiscourse (coherence)\\nsentiment analysis\\n\\nHead-to-head\\nArms-race\\n1980 1990 2000 2010\\nRarely cooccur"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "PMI vs. Cooccurrence Legend: Time Series\\n\\n1.0e\\n- grammar\\n- parser\\n\\n0.8e TM\\n- Reset Graph\\n\\n0.6z\\n- geo\\n- Filtered by:\\n\\nSelected Relations:\\n\\n0.0\\n- 0.4\\n- grammar\\n- parser\\n- hosee\\n\\n0.0 -1.00\\n-0.75\\n-0.50\\n-0.25\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n\\n1980\\n1985\\n1990\\n1995\\n2000\\n2005\\n2010\\n2015\\n\\nPrevalence Correlation Year\\n\\nSelected Relation: grammar\\nSelected Relation: parser\\n\\n(0.769 | Head-To-Head\\n(documents | {0.493\\n- Friends\\n- 0.556 Head-To-Head\\n- yweb\\n- 0.487 Friends\\n- parse\\n- 0.513 Head-To-Head\\n- document\\n- (0.456 Friends\\n- parsing\\n- 0.447 Head-To-Head\\n- similarity\\n- (0.386\\n- head\\n- 0.398 Friends\\n- np\\n- 0.308 Friends\\n- verb\\n- 0.396 Friends\\n- rule\\n- 0.305 Friends\\n- grammar\\n- (0.363 Friends\\n- rules\\n- 0.283 Friends\\n- tree\\n- 0.305 Friends\\n- parser\\n- (0.261 Friends\\n- syntactic\\n- (0.249 Friends\\n- argument\\n- 0.238 Friends\\n- argument\\n- (0.234 Head-To-Head\\n- precision\\n- 0.225 Friends\\n- rule\\n\\nhttps://github.com/nwrush/Visualization"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Thank you!\\n\\nA quantitative way to describe cooccurrence relations between ideas: friendships, head-to-head, arms-race, tryst prevalence.\\nAn effective framework to explore correlation temporal text.\\n\\nchenhao@chenhaot.com, Twitter: @ChenhaoTan\\n\\nData & code: https://chenhaot.com/papers/idea-relations.html"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 56,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Automatic Speech Recognition\\nA major commercial success story in Language Technology -"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Very heavy-handed supervision. I can see you."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Grounded speech perceptions"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Data = Flickr8K Audio (Harwath & Glass\\n2015) = 8K images, five audio captions each\\n\\nMS COCO Synthetic Spoken Captions = 300K images, five synthetically spoken captions each"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Project speech and image to joint space. A bird walks on a beam. Bears play in water."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Image model BOAT | BIRD\\n- 97 BOAR"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Speech model = Input: MFCC ® = Subsampling CNN Fei Hse = Recurrent Highway Network (Zilly et al\\n2016) Ae Vind = Attention"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Model settings\\n\\nFlickr8K Speech\\n- Attention 128\\n- Attention 512\\n- RHN depth 2, 1024\\n- RHN depth 2, 512\\n- RHN depth 2, 1024\\n- RHN depth 2, 512\\n- RHN depth 2, 1024\\n- RHN depth 2, 512\\n- RHN depth 2, 1024\\n- RHN depth 2, 512\\n- Conv 6x64, stride 2\\n- RHN depth 2, 512\\n- Conv 6x64, stride 3\\n\\nFlickr8K Text\\n- COCO Text\\n- RHN depth 1, 1024\\n- RHN depth 1, 1024\\n- Embedding 300\\n- Embedding 300"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "eImage retrieval\\n\\nModel R@10 rFlickr8K\\nSpeech RHN: 0.253\\nHarwath & Glass 2015: 0.179\\n- 'Text RHN: 1 0.494 11'\\n\\nModel R@10 rMSCOCO\\nSpeech RHN52: 0.444\\n- 'Text RHN: 1 0.5695 8'\\n\\nNewer CNN architecture: Harwath et al 2016 (NIPS), Harwath and Glass 2017 (ACL)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Levels of representation\\n\"What aspects of sentences are encoded?\"\\nWhich layers encode Form, which encode meaning?\\nAuxiliary tasks (Adi et al 2017)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Form-related aspects\\n\\nUse activation vectors to decode\\n\\n= Utterance length in words\\n\\n= Presence of specific words"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Input:\\nActivations for utterance\\nModel: Linear regression\\nNetwork layers"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Word presence \"Input\" Activations for\\n- ... a utterance she= MFCC for word 5° r a. Model s a embeddings= MLP and \"Network layers)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Semantic aspects"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Representational Similarity Correlations between sets of pairwise similarities according to Layer 5\\n- Activations and Layer 3\\n- Edit ops on written sentences Layer 1\\n- Human judgments\\n\\nMECC: no 0.4 0.5 0.6 0.7 0.8 (SICK dataset) Be aD ilarity ratings"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Homonym disambiguation\\n\\n1.00\\n- ava\\n0.75: 2 f- itt. a ; y\\n0.50\\n- eS Z\\n0.25\\n- \"S80 J7\". , ff :\\n- -~ 7 \"\\n0.00: * hss ses\\n\\nlayer=\\npeaking/peeking\\nisle/aisle\\nTMware/wear\\nseen/scene\\nwaist/waste\\ngreat/grate\\nsight/site\\nsales/sails\\nplains/planes\\nhole/whole\\nmantle/mantel\\npic/pick\\nboarder/border\\nsee/sea\\nsuite/sweet\\nwords = peer/pier\\nsun/son apenas\\nmain/mane\\npairs/pears\\ntale/tail\\nwears/wares\\nTM lapse/laps\\nrains/reins\\ncole/coal\\nwit/whit\\nrose/rows\\ntea/tee\\nsale/sail\\nweight/wait\\ntied/tide\\nstares/stairs\\nstair/stare\\n\\nlog(mincount) = 4 = 5 = 6 = 7"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Follow-up work\\n\\nAfra Alishahi, Marie Barking and Grzegorz Chrupata. Encoding of phonology in a recurrent neural model of grounded speech\\n\\nFriday, session #4 at CoNLL"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "@Conclusion Encodings of form and meaning emerge and evolve in hidden layers of stacked\\n- RHN listening to grounded speech Code: github.com/gchrupala/visually-grounded-speech Data: doi.org/10.5281/zenodo.400926"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Baod-e{ f= [honBaci-e{ fee [hanInol-e{ fe [hae"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Error analysis = Text usually better = Speech better: a yellow and white\\n- Long bird is in flight descriptions, 4, Text Speech"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Text model = Convolution\\n- word embedding \"No attention"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 41,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Who cares?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Who cares? Word embeddings are useful!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "res?yor eeegent e Fat eoagar por? .eeagar caenTM 7\"@<aaes a. wordee yaar? wo _* : ae :aa* e .wie ° ap? |° |Ss areUSeful!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Who cares?\\nAre word embeddings useful?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n\\nThey are inherently crosslingual tasks."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n\\nThey are inherently crosslingual.\\n\\nCrosslingual transfer learning."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n- They are inherently cross-lingual tasks.\\n- Cross-lingual transfer learning can be beneficial.\\n\\nThis provides a bilingual signal for training."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Who cares? Perhaps a good word embeddings are useful!\\n- Inherently cross-lingual tasks\\n- Cross-lingual transfer learning\\n- A bilingual signal for training"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n\\nThey are inherently cross-lingual tasks.\\n\\nCross-lingual transfer learning were an avenue.\\n\\nPrevious work bilingual signal for training."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n- They are inherently cross-lingual tasks.\\n- Cross-lingual transfer learning.\\n\\nPrevious work on parallel corpora.\\n\\nBilingual signal for training."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n- Inherently cross-lingual tasks\\n- Cross-lingual transfer learning\\n\\nPrevious work:\\n- Parallel corpora\\n- Bilingual signal for training\\n- Comparable corpora\\n- (Big) dictionaries"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n\\nThey are inherently cross-lingual tasks.\\n\\nPrevious work on bilingual signal for training\\n-\\ncompag'sN corpus\\n- (4%) dictionary."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n- They are inherently cross-lingual tasks.\\n- Cross-lingual transfer learning.\\n\\nThis talk is about a bilingual signal for training.\\n- Comparison of bilingual corpora.\\n- Dictionaries."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n- Inherently crosslingual tasks\\n- Crosslingual transfer learning\\n- Previous work\\n\\nThis talk is about a 25 word dictionary bilingual signal for training."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Who cares?\\n\\nWord embeddings are useful!\\n\\nThis talk is about crosslingual transfer learning.\\n\\nPrevious work on bilingual signal for training\\n- numerals (1, 2, 3...)\\n- dictionaries."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Bilingual embedding mappings"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Bilingual embedding mappings\\nBasque English"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Bilingual embedding mappings over English-Seed dictionary means"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Bilingual embedding mappings\\nEnglish Seed dictionary"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Bilingual embedding mappings\\nx eZi ne yo\"e e° ag? ® x 62° @ap e °° oot és © a0\" caeos @ WwWsh of e ar? ye e ~ ve om Ps8 a © oat <x?\\nwoea e ° * ee eaB over\" English ae Seed dictionary nan Txakur Dog Sagar Apple Egutegi Calendar"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Bilingual embedding mappings\\n\\n@Zexwaa Ny seww\\n\\nx ooe 02\\n\\n@ oe2 ane 52 8 oer ogey ° ye gest\\n\\n® o.oo\" cae Seguecn)\\n\\nWwW\\ oe 02\\n\\n© @ panaae\\n\\n---> va Re ow 6,yw bs ° 7 oem a Pra\"° ro yar\\n\\no® oo<x?\\n\\noO3 ® e wer yn?3\\n\\ne aa* e e BPVM P_ neoBasque\\n\\nae i EnglishSeed\\n\\ndictionary\\n\\nTxakur Dog\\n\\nSagar Apple\\n\\nEgutegi Calendar"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Bilingual embedding mappings\\nEnglish Base due Seed dictionary: Txakur [X1, Z1], DogSagar [X2, Z2], Apple Egutegi [X1, Xn], Calendar"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Bilingual embedding mappings ex eZexw2ie wo Yeuee oo? e @ a6 ate e. y cert ® o pov?\\ncael eggos @ WwW i of 2@e@ atPe -- fae 6 oe oF usras getee eva° ro ya o® catof? oOS.\\n\\ni wer yn?3 9 aes eephieF ne. English Basile Seed dictionary mans Txakur [X1,« Z1,]\\n\\nDogSagar |X2. Iw) ~ Zz |\\n\\nApple Egutegi LXn.x Zn») Calendar"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Bilingual embedding mappings\\n- English WeEO(n)\\n- Txakur [X1, Z1] Dog\\n- Sagar [X2, Iw] Apple\\n- Egutegi [Lxn, Zn] Calendar"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Bilingual embedding mappings\\n\\n@Zexwaa) » sea . we or gx?o car? © x2 oP a aeOo e 'ap R oe et! : o' 90° cae\" oaos @ WwW\\ eo 02 © @ gad?ae\\n\\n--> a! oe owye e ° Re gett a . oe° ro 42 ya o® ca®ve oO® e ® i wer yn?<o aa* ®e P, 2 phieF neBasque arg min > lew\\n- Z;,|| English\\n\\nWeEO(n) ; Txakur [X1,« Z1,] Dog\\n\\nSagar |X2. Iw) ~ Zz | Apple\\n\\nEgutegi LXn.x Zn») Calendar"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Bilingual embedding mappings\\n@Zexwaa my ors? A ew e ecu? o car? © x2 oP a aeOo e 'ap R oe et!\\n: o', 90° cae\" oage e W aow = -__> ance @ gare my)ee we red\" Pese A vd » ® ost\" . yeeve oO® e ® i wer yn?\\n<o aa* ®e P, 2 phieF neBasque arg min > lew\\n- Z;,|| English WeEO(n) ; Txakur [X1,« Z1,] Dog Sagar | X2. Iw) ~ Zz | Apple Egutegi LXn.x Zn») Calendar"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Bilingual embedding mappings\\n\\nEnglish WeEO(n); Txakur [X1, «Z1,] Dog Sagar | X2. Iw) ~ Zz | Apple Egutegi LXn.x Zn) Calendar"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "Bilingual embedding mappings\\n- 3\\n- ca -"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          "Bilingual embedding mappings- o- ep"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 30,
        "texts": [
          "Bilingual embedding mappings\\n- proposed self-learning method"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 31,
        "texts": [
          "Bilingual embedding mappings\\n\\nMonolingual embeddings\\n- proposed self-learning method\\n- formalization and implementation details in the paper\\n- based on the mapping method of Artetxe et al. (2016)\\n\\nToo good to be true? a word? No!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 32,
        "texts": [
          "Experiments"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 33,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al. (2015)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 34,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al. (2015) extended to German and Finnish"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 35,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al. (2015) extended to German and Finnish = Monolingual embeddings (CBOW + negative sampling)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 36,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al. (2015) extended to German and Finnish.\\n\\nMonolingual embeddings (CBOW + negative sampling) => Seed dictionary: 5,000 word pairs / 25 word pairs / numerals."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 37,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al. (2015) extended to German and Finnish.\\n\\nMonolingual embeddings (CBOW + negative sampling) => Seed dictionary: 5,000 word pairs / 25 word pairs / numerals => Test dictionary: 1,500 word pairs.\\n\\nWord translation induction."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 38,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al. (2015) extended to German and Finnish\\n- Monolingual embeddings (CBOW + negative sampling)\\n- Seed dictionary: 5,000 word pairs / 25 word pairs / numerals\\n- Test dictionary: 1,500 word pairs\\n\\nMikolov et al. (2013a)\\n\\nXing et al. (2015)\\n\\nZhang et al. (2016)\\n\\nArtetxe et al. (2016)\\n\\nword translation induction"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 39,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al.\\n(2015) extended to German and Finnish = Monolingual embeddings (CBOW + negative sampling) => Seed dictionary: 5,000 word pairs / 25 word pairs / numerals => Test dictionary: 1,500 word pairs\\n\\nMikolov et al. (2013a) | 34.93% | 0.00% | 0.00% | 35.00% | 0.00% | 0.07% | 25.91% | 0.00% | 0.00%\\n\\nXing et al. (2015) | 36.87% | 0.00% | 0.13% | 41.27% | 0.07% | 0.53% | 28.23% | 0.07% | 0.56%\\n\\nZhang et al. (2016) | 36.73% | 0.07% | 0.27% | 40.80% | 0.13% | 0.87% | 28.16% | 0.14% | 0.42%\\n\\nArtetxe et al. (2016) | 39.27% | 0.07% | 0.40% | 41.87% | 0.13% | 0.73% | 30.62% | 0.21% | 0.77%\\n\\nword translation induction"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 40,
        "texts": [
          "- Dataset by Dinu et al. (2015) extended to German and Finnish:\\nMonolingual embeddings (CBOW + negative sampling)\\n=> Seed dictionary: 5,000 word pairs / 25 word pairs / numerals\\n=> Test dictionary: 1,500 word pairs\\n- English-Italian Mikolov et al. (2013a) | 34.93% | 0.00% | 0.00%\\n- Method Xing et al. (2015) | 36.87% | 0.00% | 0.13%\\n- Our method <== Artetxe et al. (2016) Zhang et al. (2016) | 36.73% | 0.07% | 0.27%\\n- Xing et al. (2015) Artetxe et al. (2016) | 39.27% | 0.07% | 0.40%\\n- Zhang et al. (2016) | 39.67% | 37.27% | 39.40%\\n- Seed dictionary size word translation induction"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 43,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al. (2015) extended to German and Finnish\\n- Monolingual embeddings (CBOW + negative sampling)\\n- Seed dictionary: 5,000 word pairs / 25 word pairs / numerals\\n- Mikolov et al. (2013a) 5k dict\\n- Xing et al. (2015) 5k dict\\n- Zhang et al. (2016) 5k dict\\n- Artetxe et al. (2016) 5k dict\\n- Cross-lingual word similarity"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 44,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al.\\n(2015) extended to German and Finnish = Monolingual embeddings (CBOW + negative sampling) => Seed dictionary: 5,000 word pairs / 25 word pairs / numerals\\n\\nPau [ence | Mikolov et al. (2013a) 5k dict\\n\\nXing et al. (2015) 5k dict\\n\\nZhang et al. (2016) 5k dict\\n\\nArtetxe et al. (2016) 5k dict\\n\\n5k dict\\n\\nOur method 25 dict\\n\\nnum. crosslingual word similarity"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 45,
        "texts": [
          "Experiments\\n- Dataset by Dinu et al. (2015) extended to German and Finnish\\n- Monolingual embeddings (CBOW + negative sampling)\\n- Seed dictionary: 5,000 word pairs / 25 word pairs / numerals\\n\\nPau [ence | Mikolov et al. (2013a) 5k dict 62.7% | 64.3% | 52.8%\\nXing et al. (2015) 5k dict 61.4% | 70.0% | 59.5%\\nZhang et al. (2016) 5k dict 61.6% | 70.4% | 59.6%\\nArtetxe et al. (2016) 5k dict 61.7% | 71.6% | 59.7%\\n5k dict 62.4% | 74.2% | 61.6%\\nOur method 25 dict 62.6% | 74.9% | 61.2%\\nnum. 62.8% | 73.9% | 60.4%\\n\\ncross-linguistic word similarity"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 46,
        "texts": [
          "Why does it work?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 47,
        "texts": [
          "Why does it work?\\n\\nMonolingual embeddings small large\\n\\nDictionary => Mapping => Dictionary no error errors ="
        ]
      },
      {
        "section_index": 0,
        "slide_index": 48,
        "texts": [
          "Why does it work?\\n\\nMonolingual embeddings\\n\\nsmall large\\n\\nDictionary => Mapping => Dictionary\\n\\nno error errors\\n\\nbetter?\\n\\nMapping PL Dictapping ictionary"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 49,
        "texts": [
          "Why does it work?\\n\\nMonolingual embeddings\\n\\nsmall large\\n\\nDictionary => Mapping => Dictionary\\n\\nno error errors\\n\\nbetter?\\n\\nMapping => Dictionary\\n\\nworse?\\n\\neven better?\\n\\nMapping => Dictionary"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 52,
        "texts": [
          "Why does it work? Implicit objective: W* = arg max > max(X;\\nW)\\n- Zj, st. WWTM = W'W = Iwo oa"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 53,
        "texts": [
          "Why does it work? Implicit objective: W* = arg max > max(X;\\nW)\\n- Zj, st. WWTM = W'W = IWw; J Independent from seed dictionary!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 54,
        "texts": [
          "Why does it work? Implicit objective: W* = arg max > max(X;\\nW) + Zj. st. WW? = WTW = IWw\\n- t Jia > «ee yous? a ew °e no @® ave a de @ap v' o* gost 'we ppv' cae®.\\n& : voye e ee ° ° gare co oee eee oa © cotoh woeoe e 5 * oa e© ye"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 55,
        "texts": [
          "Why does it work? Implicit objective: W* = arg max > max(XisW) » Zijs st. WW? = WTW = IWw"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 63,
        "texts": [
          "Why does it work?\\nImplicit objective: W* = arg max > max(X;\\nW)\\n- Zj, st. WWTM = W'W = Iw\\nIndependent from seed dictionary! So why do we need a seed dictionary?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 64,
        "texts": [
          "Why does it work? Implicit objective: W* = arg max > max(X;\\nW) + Zj, st. WW = W'W = I]Ww\\n- J Independent from seed dictionary! So why do we need a seed dictionary? Avoid poor local optima!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 65,
        "texts": [
          "Implicit objective: W* = arg max > max(X;,.W) + Zj. st. WW = W'W = I J Ye * 045\\n- Zz\\n\\nSeed dict. =\\n- 5,000\\n- 5,000\\n- 2,500\\n- 2,500\\n040 Pools oath secs a\\n- od\\n- er3 fe\\n- 500 e\\n- 5002 a\\n- 250 =\\n- 250g 0.35 : 5 0H B ao.\\n- 2 °°8 g\\n- 2\\n- 75 © 030\\n- }/: inn \"0 a.\\n- 25\\n\\n10\\n-\\n- 25:\\n- num.\\n- = num. 0.25 : » = ** none\\n- = \" * none:\\n- I t I it} a a i a ae a a | 10 20 30 40\\n10 20 30 40 Iteration\\nIteration"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 66,
        "texts": [
          "Conclusions"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 67,
        "texts": [
          "Conclusions\\n- Simple self-learning method to train bilingual embedding mappings"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 68,
        "texts": [
          "Conclusions\\n- Simple self-learning method to train bilingual embedding mappings.\\n- High quality results with almost no supervision (25 words, numerals)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 69,
        "texts": [
          "Conclusions\\n- Simple self-learning method to train bilingual embedding mappings\\n- High quality results with almost no supervision (25 words, numerals)\\n- Implicit optimization objective independent from seed dictionary"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 70,
        "texts": [
          "Conclusions\\n- Simple self-learning method to train bilingual embedding mappings.\\n- High quality results with almost no supervision (25 words, numerals).\\n- Implicit optimization objective independent from seed dictionary.\\n- Seed dictionary necessary to avoid poor local optima.\\n\\nFuture work: fully unsupervised training."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 71,
        "texts": [
          "One more thing..."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 72,
        "texts": [
          "One more thing...\\n\\ngit clone https://github.com/artetxem/vecmap.git"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 73,
        "texts": [
          "One more thing...\\n\\ngit clone https://github.com/artetxem/vecmap.git\\n\\npython3 vecmap/map_embeddings.py"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 74,
        "texts": [
          "One more thing...\\n\\ngit clone https://github.com/artetxem/vecmap.git\\n\\npython3 vecmap/map_embeddings.py --self_learning --numerals"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 75,
        "texts": [
          "One more thing...\\ngit clone https://github.com/artetxem/vecmap.git\\npython3 vecmap/map_embeddings.py --self_learning --numerals SRC_INPUT.EMB TRG_INPUT.EMB"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 76,
        "texts": [
          "One more thing...\\n\\ngit clone https://github.com/artetxem/vecmap.git\\n\\npython3 vecmap/map_embeddings.py --self_learning --numerals SRC_INPUT.EMB TRG_INPUT.EMB SRC_OUTPUT.EMB TRG_OUTPUT.EMB\\n\\nvecmap/reproduce_ac12017.sh"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 77,
        "texts": [
          "One more thing...\\n\\ngit clone https://github.com/artetxem/vecmap.\\n\\npython3 vecmap/map_embeddings.py --self_learning --numerals SRC_INPUT.EMB TRG_INPUT.EMB SRC_OUTPUT.EMB TRG_OUTPUT.EMB\\n\\nvecmap/reproduce_ac12017.sh\\n\\nENGLISH-ITALIAN\\n\\n5,000 WORD DICTIONARY\\n- Mikolov et al. (2013a) | Translation: 34.93% MWS353: 62.66%\\n- Xing et al. (2015) | Translation: 36.87% MWS353: 61.41%\\n- Zhang et al. (2016) | Translation: 36.73% MWS353: 61.62%\\n- Artetxe et al. (2016) | Translation: 39.27% MWS353: 61.74%\\n- Proposed method | Translation: 39.67% MWS353: 62.35%\\n\\n25 WORD DICTIONARY\\n- Mikolov et al. (2013a) | Translation: @.00% MWS353: -6.42%\\n- Xing et al. (2015) | Translation: @.00% MWS353: 19.49%\\n- Zhang et al. (2016) | Translation: @.07% MWS353: 15.52%\\n- Artetxe et al. (2016) | Translation: 0.07% MWS353: 17.45%\\n- Proposed method | Translation: 37.27% MWS353: 62.64%\\n\\nNUMERAL DICTIONARY\\n- Mikolov et al. (2013a) | Translation: @.00% MWS353: 28.75%\\n- Xing et al. (2015) | Translation: @.13% MWS353: 27.75%\\n- Zhang et al. (2016) | Translation: @.27% MWS353: 27.38%\\n- Artetxe et al. (2016) | Translation: @.40% MWS353: 24.85%\\n- Proposed method | Translation: 39.40% MWS353: 62.82%"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 78,
        "texts": [
          "Thank you! https://github.com/artetxem/vecmap"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 67,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Motivation\\n\\nUser attribute prediction from text is successful:\\n- Age (Rao et al. 2010 ACL)\\n- Gender (Burger et al. 2011 EMNLP)\\n- Location (Gisens tein et al. 2010 EMNLP)\\n- Personality (Schwartz et al. 2013 PLoS One)\\n- Impact (Ampos et al. 2014 EACL)\\n- Political Orientation (Votkova et al. 2014 ACL)\\n- Mental Illness (Coppersmith et al. 2014 ACL)\\n- Occupation (Preotiuc-Pietro et al. 2015 ACL)\\n- Income (Preotiuc-Pietro et al. 2015 PLoS One)... and useful in many applications."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Political Ideology & Text Hypothesis: Political ideology of a user is disclosed through language use.\\n\\nPartisan political mentions or issues:\\n- @realDonaldTrump your program last night was top notch! You Sir, are a class act!\\nGod bless our Vets #MakeAmericaGreatAgain #Trump2016\\n\\nCultural differences:\\n- Disappointed today. Either I trust God to \"have this\" or I don't. I truly do, but still disappointed."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Political Ideology & Text\\n\\nPrevious CS/NLP research used data sets with user labels identified through:\\n1. User descriptions\\n\\nUsers are far more likely to be politically engaged"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Political Ideology & Text\\n\\nPartisan Hashtags\\n- #senate\\n- #p2\\n- #thehill\\n\\nHatch: Trump's Sessions attacks bad for presidency.\\n- #Bernie\\n- #BernieSanders\\n- #p2\\n- #FeelTheBern\\n\\n#TheResistance\\n#Resist\\n#Resistance\\n#Progressives\\n#Millennials\\n#libcrib\\n#p2\\n#ImpeachTrump\\n#DemForce\\n#occupy\\n#BlackTwitter\\n\\n@RealDonaldTrump\\n\\nThe prediction problem was so far over-simplified."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "3. Lists of Conservative/Liberal users\\n\\nRight-Leaning Tweets\\n- List members\\n- Apne by Se\\n- Editor at large @WeeklyStandard\\n- Beast columnist. Author of 7 books.\\n- Tweets > Editor of the Washington Free Beacon, Contributing none >\\n- National Political Reporter @politic, Formerly @NRO\\n\\nMore lists by @Slate view\\n- Neutral users"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Political Ideology & Text\\n\\nFollowers of partisan accounts (@ Ba Ger Taw TT G):\\n\\nDifferences in language use exist between moderate and extreme users."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Data\\nPolitical ideology\\nspecific of country and culture\\nour use case is US politics (similar to all previous work)\\nthe major US ideology spectrum is Conservative\\n- Liberal\\n\\nseven point scale\\n- Very Conservative\\n- Moderate\\n- Very Liberal"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Data We collect a new data set: 3.938 users (4.8M tweets) public Twitter handle with >100 posts\\n\\nPolitical ideology is reported through an online survey only way to obtain unbiased ground truth labels (Flekova et al.\\n2016 ACL, Carpenter et al. 2016 SPPS) additionally reported age, gender and other demographics"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Data available at preotiuc.ro\\nFull data for research purposes\\nAggregate for replicability\\n\\nTwitter Developer Agreement & Policy VII.A4\\n\\n\"Twitter Content, and information derived from Twitter Content, may not be used by, or knowingly displayed, distributed, or otherwise made available to any entity to target, segment, or profile individuals based on [...] political affiliation or beliefs.\"\\n\\nStudy approved by the Internal Review Board (IRB) of the University of Pennsylvania"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Class Distribution 1000 750 696 692 594 453 501 500 aot 250 1950 SP BW SK BD O DO SS) x g wa a xeoe 3S oe S sya Y o e 3 YY& os aa Soe"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Data for comparison to previous work, we collect a data set: 13,651 users (25.5M tweets) follow liberal/conservative politicians on Twitter."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Hypotheses H1 Previous studies used users far more likely to be politically engaged"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Engagement\\n\\nPrevious studies used users far more likely to be politically engaged.\\n\\nManually coded:\\n- Political words (234)\\n- Political NEs: mentions of politician proper names (39)\\n- Media NEs: mentions of political media sources and pundits (20)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Engagement Data set obtained using previous methods\\n\\nPolitical word usage across user groups\\n\\nMedia/Pundit Names\\n\\nPolitician Names\\n\\nPolitical Words\\n\\nAverage percentage of political word usage"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Engagement\\n\\nOur data set\\n\\n400 Political word usage across user groups\\n\\n3.00 = Media/Pundit Names\\n@ Politician Names\\n2.50 Political Words\\n2.00\\n\\nAverage percentage of political word usage"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Engagement Takeaways:\\n- 3x more political terms for automatically identified users compared to the highest survey-based scores\\n- Almost perfectly symmetrical U-shape across all three types of political terms\\n- The difference between 1-2/6-7 is larger than 2-3/5-6"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "H1 Previous studies used users far more likely to be politically engaged\\n\\nH2 The prediction problem was so far over-simplified\\n\\nH3 Neutral users can be identified\\n\\nH4 Differences in language use exist between moderate and extreme users"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Over-simplification\\n\\nThe prediction problem was so far over-simplified.\\n\\n1.0\\n972\\n9769\\n8918765\\nCvLm\\n\\nTopics\\n- Political Terms\\n- Domain Adaptation\\n- ROC AUC\\n- Logistic Regression\\n- 10-fold cross-validation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Over-simplification\\nThe prediction problem was so far over-simplified.\\n\\nTopics = Political\\nTerms = Domain Adaptation\\nROC AUC, Logistic Regression, 10 fold-cross validation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Over-simplification\\n\\nPredicting continuous political leaning (1\\n-\\n7)\\n\\n40 36930 1294 agg 300 ae.2014510.00\\n\\nLeaning\\n\\nUnigrams @LIWC m@Topics mEmotions ® Political m All\\n\\nPearson R between predictions and true labels, Linear Regression,\\n\\n10-fold cross-validation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Over-simplification\\n\\nSeven-class classification\\n\\n30%\\n26.20%\\n27.60%\\n24.20%\\n22.20%\\n20%\\n19.60%\\n10%\\n0%\\n\\nAccuracy, 10-fold cross-validation\\n\\nGR\\n- Logistic regression with Group Lasso regularisation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Neutral Users\\n\\nNeutral users can be identified as having a score around 220M, with some characteristics showing a diversity in preferences and affiliations.\\n\\nWords associated with either neutral, extreme conservative, or liberal users have a correlation strength.\\nCorrelations are age and gender controlled. Extreme groups are combined using matched age and gender distributions."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Political Engagement\\n\\nThere is a separate dimension of political engagement.\\n\\nCombine the classes into a scale: 4\\n- 3 & 5\\n- 2 & 6\\n- 1 & 740 369.294\\n- 30030 286 see. 19628 145, 1650449 268 2168,\\n10. 079.00\\n\\nLeaning Engagement\\n\\nUnigrams = LIWC\\n\\nm @ Topics\\n\\nm Emotions\\n\\nPolitical\\n\\nm All\\n\\nPearson R between predictions and true labels, Linear Regression, 10-fold cross validation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Hypotheses\\n\\nH4 Differences in language use exist between moderate and extreme users"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Moderate Users\\n\\nDifferences between moderate and extreme users\\n\\nWords associated with moderate liberals (5 and 6).\\nWords associated with extreme liberals (7).\\n\\nCorrelations are age and gender controlled."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "Take Aways\\n- User-level trait acquisition methodologies can generate non-representative samples.\\n- Political ideology: Goes beyond binary classes.\\n- The problem was to date over-simplified.\\n- New data set available for research.\\n- New model to identify political leaning and engagement."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "Questions? www.preotiuc.rowwbp.org"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 13,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "AMR graph © Generate from AMR graph Encoder Decoder text, Attention -"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "AMR graph, Parse to AMR C )\\n- from AMR text eanrelel = t Dyererole at graph B graph eantelelet Decoder text"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "AMR graph © BATA TRAE Ratha Parse to AMR Generate from AMR CEPAUETTG\\n\\ntext Encoder Decoder graph B graph Encoder Decoder text. PNicialtelg\\n- text. Attention."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "AMR graph\\nBAHAI EAE\\n- O7. pareParse to AMR\\nGenerate from AMRCEPAUETTG\\ntext Encoder Decoder graph\\nB graph Encoder Decoder text.\\nPNicialtelg\\n- text.\\nAttention."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Abstract Meaning Representation (Banarescu et al.,\\n2013)\\n- Rooted Directed Acyclic Graph\\n- Nodes: concepts (nouns, verbs, named entities, etc)\\n- Edges: Semantic Role Labels\\n\\nhave known a planet that was inhabited by a lazy man. I ARGO mod"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "(Banarescu et al.,\\n2013)\\n\\nA pace PR >» Rooted Directed Acyclic Graph\\n\\nNodes: concepts (nouns, verbs, named entities, etc):\\n\\nEdges: Semantic Role Labels,\\n\\nInput: AMR Graph\\n\\nknew a planet that was inhabited by a lazy man.\\n\\nhave known a planet that was inhabited by a lazy man.\\n\\nknow a planet. It is inhabited by a lazy man."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Abstract Meaning Representation (Banarescu et al.,\\n2013)\\n- Rooted Directed Acyclic Graph\\n- Nodes: concepts (nouns, verbs, named entities, etc)\\n- Edges: Semantic Role Labels\\n- Input: Text\\n- Parse to AMR a lazy man."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Applications\\n- Text Summarization (Liu et al., 2015)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Applications I a oy -> DO «st[=] Parse Q 7L, Osentences: sentence = AMR graphs:» Text Summarization (Liu et al., 2015)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "BI moCQ © = mBI S SO\\n- OO ed\\n-\\nE) B QAR A &\\n- Parse\\na) summary (©) sentences: sentence AMR graph: AMR graphs: Text Summarization (Liu et al., 2015)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Text Summarization (Liu et al., 2015)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "aeEB RSD on- OO eo\\n- Parse wo summary Generate Saniancas: sentence AMR graph: summary: AMR graphs: Text Summarization (Liu et al.,\\n2015) FRESDM \"SL AgeoO -oO ; '5 mee icrer colcinaiene 2 sono uso-wa kodomo-tachi-ga tsul-taa\\n- that lie-TOP child-and others-NOM breathe out-PAST"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "CDG\\nE) Gos aes-» 0S eft -> S#« OO\\n- [=]EJ FE] QRcd ©' Parse eo summary ©) GenerateSaniancas: sentence AMR graph: summary: AMR graphs: >» Text Summarization (Liu et al.,\\n2015)\\n\\nED soARGO-ofParse AMR graph: ft© FOdtlA FREES D DU fe2;: op sono uso-wa kodomo-tachi-ga tsul-taS ilsexeniilelcclamolemiaciall = =a\\n- that lie-TOP child-and others-NOM breathe out-PAST >»\\n\\nMachine Translation (Jones et al.,\\n2012)\\n- a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "CD' iz et OQ --» 0S eft -> «#5 Oo -> [=]'\\n\\nParse eo summary ©)\\n\\nGenerateSaniancas:\\n\\nsentence AMR graph: summary: AMR graphs:\\n\\nText Summarization (Liu et al.,\\n2015)\\n\\nGraph-to-graph Coe) .transformation: Cvstt\\n\\nARGO-of ARGO-of graph:\\n\\nft© EDDC FHESD DU Fe2 ; : op sono uso-wa kodomo-tachi-ga tsul-ta3 The children told that lie =\"\\n- that lie-TOP child-and others-NOM breathe out-PAST\\n\\nMachine Translation (Jones et al.,\\n2012)\\n- a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "apcE\\nE) gt 9 QO a-» 0S eft -> «#5 Oo -> [=]E) B QAR A& JParse\\na) summary\\nC) Generate SPREE. sentence AMR graph: summary: AMR graphs:\\n\\nText Summarization (Liu et al.,\\n2015)\\nron Dw &e : Graph-to-graph Coe) transformation: Cn) ARGO-of ARGO-of Parse AMR Generate graph: ft translation: FOdtlA FREES D DU fe® -2 ; : op sono uso-wa kodomo-tachi-ga tsul-ta3 The children told that lie =n\"\\n- that lie-TOP child-and others-NOM breathe out-PAST\\n\\nMachine Translation (Jones et al.,\\n2012) -- a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Existing Approaches\\n\\nGenerate from AMR\\n- MT-based: Flanigan et al. 2016, Pourdamaghani and Knight 2016, Song et al. 2016\\n- Grammar-based: Lampouras and Vilachos 2017, Mille et al. 2017"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Existing Approaches\\n\\nGenerate from AMR\\n- MT-based: Flanigan et al. 2016, Pourdamaghani and Knight 2016, Song et al. 2016\\n- Grammar-based: Lampouras and Vilachos 2017, Mille et al. 2017\\n\\nParse to AMR\\n- Alignment-based: Flanigan et al. 2014, 2017 (JAMR)\\n- Grammar-based: Wang et al. 2016 (CAMR), Pust et al. 2015, Artzi et al. 2015, Damonte et al. 2017, Goodman et al.\\n2016, Puzikov et al. 2016, Brandt et al. 2017, Nguyen et al. 2017\\n- Neural-based: Barzdins and Gosko 2016, Peng et al. 2017, Noord and Bos 2017, Buys and Blunsom 2017"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Overview\\n- Sequence-to-sequence architecture\\n- End-to-end model w/o intermediate representations\\n- Linearisation of AMR graph to string\\n- Pre-process\\n- Paired Training\\n- Scalable data augmentation algorithm"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "is) (as) CG)\\na) a » Skeplanet was @ooq Caeg Ceeg Cee (ees (s) f t f fF fF w = argmax ] [e@ilwaish )3 8 m 3 ~ 0 i § a a input ialerelelels Decoder output i ,ity ple» i kaiGEL Eg Oy\" L9G!\\nee TIE deAPA <1 pip ea4 2p ef\" ,ns tifygds Ae a'of) COLEE he os- 3 o 'A #ae"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Sse a a Se xA planet\\n- was@cocog Caeg Coed Coed Qeee- fh hf 4 4 w = argmax ][e(ilwes, h?)= °o H ri me 'W .0 oO oO aZ a ainput Encoder Decoder output/ Bee ;¥ iy ilyVA aMEL, Lie 1 ptf/ OLE ole CIF, Cit YT rdf6 ie Meek \"a :AM!\\nLi ae Attention4 COL) We /(yg- £4 4 EY\\n- 4; 7é *- #OC 47 & know ARGO I ARG1 ( planet ARG1-of inhabit' 4, ve # <s>- lknowthe _planetof"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Graph\\n- Depth First Search (Human-authored annotation)\\n\\nUS officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Graph -> Depth First Search (Human-authored annotation)\\n\\nARGO ARG1 time location\\n- ARGO (person: ARGO-of (have-role(person) (cy) :ARG1 United States ARGO-of :ARG2 official) ARGO) year month name :ARG1 (meet: ARGO (person name) (02) (+) :ARG1-of expert ARG1-of ARG2-of) name :time (date-entity 2002\\n1)\\n\\nED group :location New York\\n\\nUnited States officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "tlGraph -> Depth First Search (Human-authored annotation)\\n\\nARGO ARG1 time location\\n- ARGO (person:ARGO-of (have-rolepena (cy):ARG1 United StatesARGO-of :ARG2 official)ARGO) year month namecles:ARG1 (meet:ARGO (personane \"pnce (02) (+):ARG1-of expertARG1-of ARG2-ofen) ee) name: time (date-entity 2002\\n1) ED group: location New York\\n\\nUS officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "tlGraph -> Depth First Search (Human-authored annotation)\\n\\nARGO\\n- ARG1 time location\\n- ARGO (person:ARGO-of (have-role pena (cy):ARG1 United States ARGO-of :ARG2 official) ARGO)\\n\\nyear month name\\n- (meet:ARGO (personane \"pnce (02) (+) :ARG1-of expert ARG1-of ARG2-of en) ee) name :time\\n- (date-entity 2002\\n1)\\n\\nED group :location\\n- New York\\n\\nUnited States officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "tlGraph -> Depth First Search (Human-authored annotation)\\n\\nARGO ARG1 time location\\n- ARGO (parson:ARGO-of (have-role pena:ARG1 United States ARGO-of :ARG2 official) ARGO) year month name have-role wahte:ARG1 (meet:ARGO (personane prc?\\n(02) (+) :ARG1-of expert country Official pRaro CIG20F:ARG2-of group)) name:time (date-entity 2002\\n1) group:location New York United States\\n\\nUS officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Linearization -> Anonymization\\n\\nARGO ARG1 time location\\n- ARGO (person:ARGO-of (have-role(rn) Con) acreARGO-of :ARG2 official)ARGO) year month namecles :ARG1 (meet:ARGO (personARG ARG2 é iARG1-of ARG2 (a)\\nC) Sarit bee pe-O -OSn) (on een) name :time (date-entity year _0 month_0) mie group :location loc 1 United US officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Linearization -> Anonymization\\n\\nARGO ARG1 time location\\n- ARGO (person: ARGO-of (have-role(rn) Cov) ere ARGO-of :ARG2 official) ARGO) year month name cb, :ARG1 (meet: ARGO (person ARG ARG2 é 'ARG1-of ARG2 (a) (1) Sarit bee pee T a (fc) ° ° :ARG2-of group)) name :time (date-entity year _0 month_0) ict group :location loc 1\\n\\nUS officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "Linearization -> Anonymization\\n- ARGO (person:ARGO-of (have-role(rn) Cor) erralARGO-of :ARG2 official)\\n- year month namecb, :ARG1 (meet:ARGO (personae phe sie :ARG1l-of expertG1-of G2-ofeT a (orcas re Vise :ARG2-of group)) name :time (date-entity year _0 month_0) ict group :location loc 1\\n\\nUnited\\n- US officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "Linearization -> Anonymization hold hold ARGO ARG1 time location\\n- ARGO (person: ARGO-of (have-role & ere ARGO-of :ARG2 official) ARGO) year month name abe: :ARG1 (meet: ARGO (person ARGI ARG2 ARG1-of ARG2 ©} iS eetS ae hered a (fc) ° ° :ARG2-of group)) name: time (date-entity year _0 month_0) ates group: location loc 1 United\\n- US officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          "Linearization -> Anonymization\\n- ARGO (person: ARGO-of (have-role & ere ARGO-of :ARG2 official) ARG0)\\n- year month name: :ARG1 (meet: ARGO (person ARG1 ARG2 ARG1-of ARG2)\\n- :ARG2-of group) name: time (date-entity year_0 month_0) group: location loc\\n\\nUS officials held an expert group meeting in January 2002 in New York.\\nOfficials held an expert group meeting in month_0 year_0 in loc_1."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 30,
        "texts": [
          "Experimental Setup\\n\\nAMR LDC2015E86 (SemEval-2016 Task 8): Hand annotated MR graphs: newswire, forums\\n- ~16k training / 1k development / 1k test pairs\\n\\nTrain mn: Optimize cross-entropy loss\\n\\nEvaluation areas include:\\n- BLEU n-gram precision (Generation) (Papineni et al.,\\n2002)\\n- SMATCH score (Parsing) (Cai and Knight, 2013)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 31,
        "texts": [
          "Experiments\\n- Vanilla experiment\\n- Limited Language Model Capacity\\n- Paired Training\\n- Data augmentation algorithm"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 32,
        "texts": [
          "1. First Attempt (Generation)\\nTreeToStr: Flanigan et al, NAACL 2016\\nTSP: Song et al, EMNLP 2016\\nPBMT: Pourdamaghani and Knight, INLG 2016"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 33,
        "texts": [
          "First Attempt (Generation)\\n\\nM@ TreeToStr\\nM@ TSP\\n® PBMT\\nMTM NeuralAMR\\n\\n> Lu-a0\\nTreeToStr: Flanigan et al, NAACL 2016\\nTSP: Song et al, EMNLP 2016\\nPBMT: Pourdamaghani and Knight, INLG 2016"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 34,
        "texts": [
          "First Attempt (Generation) @ TreeloStr @ TSP ® PBMT MTM Neural AMR\\n\\nAll systems use a \"S- ~ wailesails VEN tel V-Xe(-M (exe (:) SSwa g trained on a very large corpus.\\n\\nWe will emulate via data augmentation.\\n\\naITreeToStr: Flanigan et al, NAACL 2016\\nTSP: Song et al, EMNLP 2016\\nPBMT: Pourdamaghani and Knight, INLG 2016 (Sennrich et al., ACL 2016)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 35,
        "texts": [
          "US officials held an expert group meeting in January 2002 in New York."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 36,
        "texts": [
          "Data Augmentation\\n\\nOriginal Dataset: ~16k graph-sentence pairs"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 37,
        "texts": [
          "Data Augmentation\\n\\nPMMA HH Original Dataset: ~16k graph-sentence pairs\\n\\nHr F aa Fa Gigaword: ~183M sentences *only*"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 38,
        "texts": [
          "HAATAHPAFEFEERAHHT Original Dataset: ~16k graph-sentence pairs\\n\\nPererirrre F817 Gigaword: ~183M sentences *only* sample sentences with vocabulary overlap\\n\\nTHEE Ld @ Original Mi Giga-200kidgaadaa \" Giga-2M H Giga-20MBreraGQ) eee erence cere erencela 8 ee20 I erence a\"0OOV@1 OOV@5"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 39,
        "texts": [
          "Data Augmentation = graphOSO from AMRB graph Encoder Decoder text text willy,"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 40,
        "texts": [
          "Data Augmentation of graph/Parse to AMR\\n- from AMR text Encoder Decoder graph B graph Encoder Decoder text. Attention\\n- text. Attention -"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 41,
        "texts": [
          "Data Augmentation\\n- Graph Re-train\\n- Parse to AMR\\n- Generate from AMR\\n- Text Encoder\\n- Decoder graph\\n- B graph Encoder\\n- Decoder text.\\n- Attention\\n- text.\\n- Attention -"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 42,
        "texts": [
          "semi-supervised Learning\\nSelf-training\\n- McClosky et al. 2006\\nCo-training\\n- Yarowski 1995, Blum and Mitchell 1998, Sarkar 2001, Sogaard and Rishoe, 2010"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 43,
        "texts": [
          "Paired Training"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 44,
        "texts": [
          "Paired Training op AMIR (8.7)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 45,
        "texts": [
          "Paired Training Sop AMIR (8%) for i = 0...N"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 46,
        "texts": [
          "Paired Training SOP AIMIR (8.0%\\nS) for i = 0... N Boo"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 47,
        "texts": [
          "Paired Training Bob AMIR: (8.5) for i = 0... N"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 48,
        "texts": [
          "Paired Training Bop AMIR (8%) for i := O... No D\\n- aeal. E > z bop®~"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 49,
        "texts": [
          "Bo AMIR (8%) for i := O...N: alc E> i} cop bo GN (eB)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 50,
        "texts": [
          "rser Training AMR Pa"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 51,
        "texts": [
          "Training AMR Parser Bop"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 52,
        "texts": [
          "Training AMR Parsersample $1=200k Train P on"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 53,
        "texts": [
          "Training AMR Parser sample $1=200k Train P on\\n- 7Eo = (B.S) Train P on\\n- S := 200k PEE Babee boo Cee = || (se tape ee Gee |rs |"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 54,
        "texts": [
          "step and train on Original Dataset\\n\\nTrain P on sample $1 = 200k\\n\\nOriginal Dataset:\\n\\nneEo =.\\n\\nParse S1 with P\\n\\nOriginal Dataset:\\n\\nTrain P on q |S := 200k\\n\\nAMR \"200k\"\\n\\n200k\\n\\nAMIRBo} | epee >\"\\n\\nS| Bos || Bop"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 55,
        "texts": [
          "step and train on Original Dataset\\n\\nTraining AMR Parser\\n\\nsample S2 = 2M sentences from Gigaword\\n\\nEo = | Fine-tune P on .(B.S) | Train P on 4\\n\\nSo = 2M AMR 200k (200k AMR BOS)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 56,
        "texts": [
          "step and train on Original Dataset\\n\\nTraining AMR Parser\\n\\nsample S2=2M sentences from Gigaword\\n\\nFine-tune P on (B.S)\\n\\nTrain P on 4 | S2=2M aw (3M) Amir"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 57,
        "texts": [
          "step and train on Original Dataset sample $3 = 20M sentences from Gigaword E\\n- Fine-tune P on (Bee)t { USES oa\\n- $3 = 20M 1 aN) 20)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 58,
        "texts": [
          "step and train on Original Dataset\\n\\nTraining AMR Parser\\n\\nsample $3 = 20M sentences from Gigaword\\n\\nEo = Fine-tune P on\\n\\n(Bt) | Train P on 4\\n\\n(Tere)\\ni) see aan nineee ees"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 59,
        "texts": [
          "step and train on Original Dataset\\n\\nTraining AMR Parser sample $3=20M sentences from Gigaword\\n\\nFine-tune P on $3=20M"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 60,
        "texts": [
          "step and train on Original Dataset\\n\\nTraining AMR Generator\\n\\nsample S4=20M sentences from Gigaword\\n\\nFine-tune G on S4=20M AMR"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 61,
        "texts": [
          "step and train on Original Dataset\\n\\nTraining AMR Generator\\n\\nsample S4 = 20M sentences from Gigaword\\n\\nfine-tune (%\\n8) | Train Gon 4 | S = 20M\\n\\nAMIR a 20M / 20M | a MIR"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 62,
        "texts": [
          "step and train on Original Dataset\\n\\nTraining AMR Generator\\n\\nsample S4 = 20M sentences from Gigaword\\n\\nTrain Gon 4 | S = 20M\\n\\nMIR 20m\\n\\n20M A MIR"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 64,
        "texts": [
          "Final Results (Generation)\\n\\nM@ TreeloStr\\n\\nM@ TSP\\n\\nTM@ PBMT\\n\\nMTM NeuralAMR\\n\\nM@ NeuralAMR-200k\\n\\nM@ NeuralAMR-2M\\n\\nMTM NeuralAMR-20M\\n\\nTreeToStr: Flanigan et al, NAACL 2016\\n\\nTSP: Song et al, EMNLP 2016\\n\\nPBMT: Pourdamaghani and Knight, INLG 2016"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 68,
        "texts": [
          "Final Results (Generation)\\n\\nM@ TreeloStr\\nM@ TSP\\nM@ PBMT\\nMTM Neural\\nAMR M@ Neural\\nAMR-200k\\nM@ Neural\\nAMR-2M\\nMTM Neural\\nAMR-20M\\n\\n{ 269 } ae_ 21 ~ RS eee 7 020 4 ; : ~Lu-oo0\\n\\nTreeToStr: Flanigan et al, NAACL 2016\\nTSP: Song et al, EMNLP 2016\\nPBMT: Pourdamaghani and Knight, INLG 2016"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 69,
        "texts": [
          "Final Results (Generation)\\n\\nM@ TreeloStr\\nM@ TSP\\nM@ PBMT\\nMTM Neural\\nAMR M@ Neural\\nAMR-200k\\nM@ NeuralAMR-2Mm\\nTM Neural\\n[AMR-20Moo{\\n\\n269 yon, ae_\\n21 ~ RS eee a\\n\\nTreeToStr: Flanigan et al, NAACL 2016\\nTSP: Song et al, EMNLP 2016\\nPBMT: Pourdamaghani and Knight, INLG 2016"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 70,
        "texts": [
          "Final Results (Parsing)\\n\\nSBMT: Pust et al., 2015\\nCharLSTM+CAMR: Noord and Bos, 2017\\nSeq2Seq: Peng et al., 2017"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 71,
        "texts": [
          "Final Results (Parsing)\\n\\nM@ SBMT @ CharLSTM+CAMR- & Seq2Seq TM Neural AMR-20 ML 4a\\n- vest=s© 2B\\n- oj a ne0\\n\\nSBMT: Pust et al, 2015\\nCharLSTM+CAMR: Noord and Bos, 2017\\nSeq2Seq: Peng et al., 2017"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 72,
        "texts": [
          "Final Results (Parsing)\\n\\nSBMT: Pust et al, 2015\\nCharLSTM+CAMR: Noord and Bos, 2017\\nSeq2Seq: Peng et al., 2017\\n\\nTM Neural AMR-20 ML 67.1 3 60 S 32 L 4a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 73,
        "texts": [
          "Final Results (Parsing)\\n\\nM @ SBMT @ CharLSTM + CAMR\\n- & Seq2Seq TM Neural AMR\\n- 20 ML 67.1 3 60 S 32 L 4 a\\n\\nvest = s © 2 B\\n\\nSBMT: Pust et al., 2015\\nCharLSTM + CAMR: Noord and Bos, 2017\\nSeq2Seq: Peng et al., 2017"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 74,
        "texts": [
          "How did we do?\\n\\n(Generation) hold: ARGO\\n\\n(person Reference: -of (h -rol ' :aia Hoe _ US officials held an expert group meeting in: ARG2 official) January 2002 in New York.)\\n\\na: ARG1 (meet : ::ARGO\\n\\n(person Prediction: ARG1-of experthiatal habeas)\\n\\nIn January 2002 United States officials held an expert meeting of the group experts in New York.\\n\\nErrors: Disfluency Coverage"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 75,
        "texts": [
          "How did we do? (Generation) hold: ARGO (person Reference: ARGO of (have role...\\nUS officials held an expert group meeting in: ARG2 official) January 2002 in New York.\\n\\nIn January 2002, United States officials held a meeting of the group experts in New York.\\n\\nThe report stated that the British government proposed international regulations that would stop terrorists using freely available information to create a form of biological warfare such as the modified version of the influenza virus."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 76,
        "texts": [
          "Summary\\n- Sequence-to-sequence models for Parsing and Generation\\n- Paired Training: scalable data augmentation algorithm\\n- Achieve state-of-the-art performance on generating from AMR\\n- Best-performing Neural AMR Parser\\n- Demo, Code and Pre-trained Models: http://ikonstas.net\\nThank You"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 77,
        "texts": [
          "Bonus Slides"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 78,
        "texts": [
          "Encoding Linearize -> RNN encoding hold: ARGO (person: ARGO-of (have-role: ARG1 United States: ARG2 official)): ARG1 (meet ep: ARGO (person: ARG1-of expert: ARG2-of group)): time (date-entity 2002 1): location New York"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 79,
        "texts": [
          "Linearize -> RNN encoding\\n- Joken embeddings hold: ARGO (person: ARGO of (have-role: ARG1 United States: ARG2 official) ate: ARG1 (meet: ARGO (person @ee® Gece @ecee: ARG1 of expert: ARG2 of group) | i | i\\ni) Oo oS ~ a Hy: time (date-entity 2002\\n1) a 0 0 o: location New_York G a A a TM ei"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 80,
        "texts": [
          "Linearize -> RNN encoding\\n- Joken embeddings\\n- Recurrent Neural Network (RNN) hold: ARGO (person: ARGO-of (have-role: ARG1 United States A a om 'a: ARG2 official) Th] -> i sho) -> Bl nhs > oh > oa): ARG1 (meet ese: ARGO (person @eee @eee Cree Cove Cers: ARG1-of expert: ARG2-of group) | i | i\\ni) Oo ° ~ a WH: time (date-entity 2002\\n1) G 0 2 °: location New_York G a A 5TM ei"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 81,
        "texts": [
          "Linearize -> RNN encoding- Joken embeddings- Recurrent Neural Network (RNN)- Bi-directional RNN\\n\\nhold:ARGO (person:ARGO-of (have-role:ARG1 United States 7 own Jew Fawn / 2:ARG2 official) eth (> ie alae (> ace (> Beara\\n- Fa Ea, PAPAS:ARG1 (meet ic | |\\n- _| _NSC (store) @Vee) O@8ee) Cee) C898) (Eres:ARG1-of expert:ARG2-of group) | i i i\\ni)\\n\\nOo ° ~ a WH:time (date-entity 2002\\n1) G 0 2 rd:location New_York G a A 5\" ei"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 82,
        "texts": [
          "Linearize\\n- RNN encoding\\n- Joken embeddings\\n- Recurrent Neural Network (RNN)\\n- Bi-directional RNN\\n\\n(ARGO (person: ARGO-of (have-role: ARG1 United States a Jom Jum | a:ARG2 official) (meet ic | Ne S S Ne: ARGO (person @ee) O@8ee CPee Cree (Cree: ARG1-of expert: ARG2-of ae | i i i\\ni)\\n\\nWH: time (date-entity 2002\\n1)\\nrd: location New York"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 83,
        "texts": [
          "Linearize -> RNN encoding\\n- Joken embeddings\\n- Recurrent Neural Network (RNN)\\n- Bi-directional RNN\\n\\nOAC, (S) OQ = (acs) (as) (as) [as]: ARGO (person: ARGO-of (have-role: ARG1 United States 5 own Jew Fawn /:ARG2 official) (- hoe se tye bs)): ARG1 (meet ic | Ne S S Ne: ARGO (person @Vee) O@8ee) Cee) C898) (Eres: ARG1-of expert: ARG2-of ae | i i i\\ni)\\n\\nOo So ~ e WH: time (date-entity 2002\\n1) G 0 2\\nrd: location New York\\nG a M4 oO= e f"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 84,
        "texts": [
          "Decoding RNN Encoding -> RNN Decoding (Beam search)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 85,
        "texts": [
          "Decoding RNN Encoding -> RNN Decoding (Beam search)\\n- init h(®)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 86,
        "texts": [
          "Decoding RNN Encoding -> RNN Decoding (Beam search)\\n- init ho Holding Held\\n- softmax US"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 87,
        "texts": [
          "RNN Encoding -> RNN Decoding (Beam search)\\n- init h₀ Holding a Held the\\n- softmax p(w_i | w_{i-1},\\nh) TL aRa() w_{11}: Holding w_{i2}, Heldsw w_{i3}: Holding. US"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 88,
        "texts": [
          "RNN Encoding -> RNN Decoding (Beam search)\\n\\nHolding ainit h Held the person\\n\\nsoftmax p(wilweih®)\\n\\nHolding wer: Hold aWi2, Helds W22.\\n\\nHold thewi3: Hold Wo3 Held awig.\\n\\nWea: Held the"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 89,
        "texts": [
          "RNN Encoding -> RNN Decoding (Beam search)\\n- init ho\\n\\nHolding a US meeting\\n\\nHeld the person meetings\\n- softmax US meeting expert meet\\n\\nHolding were Hold a ma: The US officials held\\n\\nHelds were: Hold the wk: US officials held a\\n\\nHold was: US officials hold the\\n\\nHeld the wa: US officials will hold a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 90,
        "texts": [
          "Attention"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 91,
        "texts": [
          "ea4 tentio. it S:"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 92,
        "texts": [
          "Attention we: held @ ee @ ameerere the; 74 es meeting hold\\n- rel ei | hy J A | ARGO -> Co & ot | person -> re e | ARGO\\n- of\\n- CE | | a; = soft max(f; (h®, h;)) Ci, = S\\n- aijh \\ a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 93,
        "texts": [
          "we held the meeting of the ARGO person role US official.\\nARG1 meet expert group ARGO of US officials held an expert group meeting in January 2002."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 94,
        "texts": [
          "we held the meeting hola\\n- & I dARGO -> 5 : & cc! | ' quaperson\\n- g \" E | ' ~- ¥iq\\n- i hold ARGO ( person role US official ) ARG1 ( meet expert group ) ARGO of -> § ; § USA Cf officials held an expert group meeting in January 2002"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 95,
        "texts": [
          "we held the meeting hola\\n- : & dARGO -> 5 : &cc!\\n\\nquaperson\\n- g \" E |' ~- ¥iq -i hold ARGO (person role US official) ARG1 (meet expert group) ARGO-of -> § ; § USa Cf officialst\\n\\nheld an expert group meeting in January 2002"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 96,
        "texts": [
          "we held the meeting hola -: & i 1k) dARGO -> 5: &F, PI) a'oul person\\n- g / E |' ~- ¥iq -i hold ARGO (person role US official) ARG1 (meet expert group) ARGO\\n- of -> §; § US a Cf officials 3 held an experts; = soft max (f; (h') h;)) group(s) meeting = ) aijh; ini January 2002"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 115,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Geolocation Prediction\\nGoal\\n- Predict the location of a person in Vancouver.\\n\\nExample: Vancouver keystroke geolocation prediction.\\nCity name: Pacific Spirit."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "- Geolocation prediction with neural networks\\n\\nOverview label\\n\\nInputs messages, metadata, and user network\\n\\nErrata"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "- Geolocation prediction with neural networks\\n\\nOverview label ITEXTRMETA\\n\\nText processes with intention RNN + Attention Embedding\\n\\nInputs messages, metadata, and user network"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "- Geolocation prediction with neural networks\\n\\nOverview\\n- Merge representations with Attention\\n- Text processes with RNN + Attention\\n- Embedding\\n- Inputs messages, metadata, and user network\\n- Location (timezone)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "- Geolocation prediction with neural networks\\n\\nOverview labelmi Output cityname=}\\n\\nMerge representations with Attention TEXT\\n\\nUserNEL: Text processes with RNN + Attention\\n\\nEmbedding Inputs messages, metadata, and user network\\n\\nLocation (time zone)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Our Approach\\n- Geolocation prediction with neural networks\\n\\nOverview\\n\\nQuite a complex model, why?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Geolocation Prediction Target\\n- Twitter users\\n- A popular target in previous works (Cheng et al., 2010; Eisenstein et al., 2010)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Geolocation Prediction Target\\n- Twitter users\\n- A popular target in previous works (Cheng et al., 2010; Eisenstein et al.,\\n2010)\\n- Characteristics\\n- Multiple geolocation clues\\n- Message (tweet)\\n- Metadata\\n- User network\\n- Large-scale data\\n- Ground-truth locations with geotags"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Metadata\\n- Description, location, timezone, etc.\\n- State-of-the-art performances combined with texts (Han et al., 2013, 2014; Jayasinghe et al., 2016; Miura et al.,\\n2016)\\n\\nExample at XXX ... Twitter user"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "User Network\\n- Mention network, friend network, etc.\\n- Prediction with label propagation\\n- State-of-the-art performances combined with texts (Rahimi et al., 2015a; Jayasinghe et al.,\\n2016)\\n\\nExample\\nTokyo\\nVancouver\\nMIStEIGh\\nCREP\\n\\nMention user 2 (null)\\nPerhaps, Mention user 3\\nVancouver\\nTarget user\\nVancouver\\nMention user 4"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Fetalcnr rr rr rr rrr rr rrr aoe eee 71| TEXT&META| :1! 1a1! TEXT\" !i! USERNET 11 I' y 1= | |i !\\nEmbedding \" | I: : ap !|! 1!\\nI' | | City User \\Mi |Word Embedding rd Embedding | | Embedding| |Messages Metadata ft User networ' TD fa ECON eso VTNOT Timezone ff i ed ae iim (timeline) ff 4 Otel Store i' : \" Ietree pre epee nef EE me me mime miemommmmamimimmimnad"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Fetalcnr\\n\\nTEXT&META\\nUSERNET\\n\\nEmbedding\\nCity User\\nWord Embedding\\nEmbedding\\nMessages\\nEto cert TilerETS\\nlocation description timezone\\n2.3\\n- (timeline)\\natelas"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "TEXT Component (2).\\n\\ntimeline\\n- Hierarchical Neural Networks\\n- Layers\\n- Word Embedding\\n- Recurrent Neural Network\\n- Specifically, GRU (Cho et al.,\\n2014)\\n- (Self) Attention\\n\\nAttention\\n- RNN output: SORMAX Ot = Sy (wT us)\\n- Perceptron: EXP Pat messages = tanh (Wag + ba)\\n(timeline)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "label TEXT & META text Merge four representations! 11 1 IPi | Psion, | ! attention | 1 Text processes er 1!\\nI1 Fenny | single text) | vy. RNNy (sing) | I I I' City User iMessages pif Met | | IMessages Metadata.\\nlinked linked (timeline) (location, description, timezone) cities users ff1 I"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Fetalcnr\\n\\nTEXT&META\\n\\n1: Ia1! TEXT\\n\\nEmbedding\\n\\nCity User!\\n\\nWord Embedding\\n\\nEmbedding\\n\\nUser network\\n\\nmessages\\n\\ndescription timezone\\n\\n(timeline)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "USERNET Component (2) FT TT TTT 1 user network\\n- Linked users\\n- Representation\\n- Embedding for each user\\n- Regional celebrities\\n- Linked cities\\n- Embedding for each ground-truth.\\n- City user city of a user\\n- \"unknown\" for unavailable cases\\n- Merge states\\n- Element-wise addition\\n- Attention over N users\\n- User current\\n- Linked network\\n- User"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Unified PFetaltJ Predict CityMerge two representations UTEXT & META | User | ;eee ee eee : ITEXT representation! 1!\\nI aagaae ooo1if i User network | I I . 1: Tae] | representation Embedding \" Ii!\\n\" I: b !f \" 1MW i City User uw 1 rdi i 1!\\nI' ' messages [i ; ar ; i linked linked ae fa COLeeeT Toye description timezone [fF re\\n- i (timeline) 1 i tater users iLn eee eee ee ee ee ee ee ee ee ee ee ee eee"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Data (1)\\n- Two open datasets\\n- TwitterUS (Roller et al.,\\n2012)\\n- W-NUT (Han et al.,\\n2016)\\n\\nUser Network\\n- Uni-directional mention network (Rahimi et al., 2015a, 2015b)\\n- Dataset users + one-hop users\\n- Set undirected edges for mentions\\n\\nExample\\n\\nAlice @ [Gonna] a Bob set edge Bob"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Twitter US © = W-NUT (Cae ttiy) (Cie) #user 279K 782K #tweet 23.8M 9.03M tweet/user 85.6 11.6 #reduced-edge* 2.11M 1.01M reduced-edge/user 7.04 1.29 #city 339 3028* Restricted edges to satisfy one of the following conditions:\\n- Both users have ground truth locations.\\n- One user has a ground truth location and another user is mentioned 5 times or more."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "UserName\\nsto-\\nMetadata Network Key Components\\nlogistic regression, k-d tree\\n\\nLR x (Rahimi et al. 2015a)\\nlogistic regression, k-d tree,\\nMADCEL-B-LR Xx xX Modified Adsorption (Rahimi et al. 2015a)\\nlogistic regression, k-d tree,\\nLR-STACK Xx xX stacking (Han et al. 2013,\\n2014)\\nMADCEL-B- x x x logistic regression, k-d tree,\\nLR-STACK stacking, Modified Adsorption\\n\\nSUB-NN-TEXT x TEXT\\nSUB-NN-UNET x x TEXT, USERNET\\nSUB-NN-META x x TEXT & META\\n\\nProposed Model Xx xX Xx Full model"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Metric rotten Accuracy\\n\\nThe percentage of correctly predicted cities.\\nA relaxed accuracy that takes prediction errors within 161 km as correct predictions.\\n\\nMedian Error Distance* Median value of error distances in predictions.\\n\\nMean Error Distance* Mean value of error distances in predictions.\\n\\nGibsons: Pelee Example\\n\\nPrediction: Vancouver\\n\\nGround-truth: Victoria Bi Surrey.\\n\\nAccuracy = 0.0\\n\\nAccuracy@161 = 1.0\\n\\nError Distance = 94 km\\n\\nError distance evaluations are excluded from this presentation for simplification.\\n\\nGoogle's Map data ©2017 Google Canada Terms"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Results (Twitter US)\\n\\nModel Accuracy\\noeLR 42.0 52.7\\n\\nBaselines\\n- MADCEL-B-LR 50.2 60.1 (implemented)\\n- LR-STACK 50.8 64.1\\n- MADCEL-B-LR-STACK 55.7 67.7\\n- SUB-NN-TEXT 44.9% 55.6% +2876 In\\n- SUB-NN-UNET 51.0 61.5\\n- SUB-NN-META 54.6 67.2\\n\\nProposed Model\\n- 58.5 70.1\\n\\nTAA Io accuracy @ 161 * significant improvement with 5% significance level\\n** significant improvement with 1% significance level"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "NalaAY Carel PAN CU DELAY @161\\n\\nBaselines (reported) Jayasinghe et al. (2016)\\n52.6\\n- LR\\n34.1\\n46.7 +3.8% in Baselines\\n\\nMADCEL-B-LR\\n36.2\\n49.7 accuracy (implemented)\\n\\nLR-STACK\\n51.2\\n64.9\\n\\nMADCEL-B-LR-STACK\\n51.6\\n65.3\\n\\nSUB-NN-TEXT\\n35.4\\n50.3 +4.8% in\\n\\nSUB-NN-UNET\\n38.1\\n53.3 accuracy\\n\\nOur Models\\n\\nSUB-NN-META\\n54.7\\n70.2\\n\\nProposed Model\\n56\\n71.9 Pecuracyi @ier ** significant improvement with 1% significance level"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Analysis of Attention\\n\\n(1) 10 10\\n- Twitter US\\n- Twitter US 8\\n\\nW-NUT 8\\n\\nW-NUT label 6\\n\\nStronger in probability +3 Twitter US 4 density functions\\n\\neos\\n\\n% [iat GS. = % a\\n\\n8002 04 06 08 10 80 02 04 06 08 1.0\\n\\nLy 3 3 i ei 10 10 ut Timeline\\n\\nTwitter US\\n- Twitter US\\n\\nMetadata\\n\\nW-NUT 8\\n\\nW-NUT\\n\\nmessages\\n\\nlocation\\n\\ndescription\\n\\ntimezone"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "10 10\\n- TwitterUs\\n- Twitterus8\\n- W-NUT 8\\n- W-NUT6 Stronger in .4 TwitterUS 4ot Q.\\nTwitterUS W-NUT§o 02 04 06 08 1.0 80 02 04 66 08 1.0 #fuser 279K 782K 10 108 s = W-NUT 8 s = W-NUT(q tweet/user 85.6 11.66h .\\n6}: |.fh al |2 . 2 i8o 02 04 06 08 1.0 806 os 04 06 O8 1.0"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Analysis of Attention | (2) PES probability * 0 density function user network |\\n- Twitter US\\n\\nUser & User Network"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Analysis of Attention layers (2) 108\\n- W-NUT6 aaa Pa Om ONL OEry eer (train) (train) 8.0 OD 0.4 0.6 0.8 1.0"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "Conclusion\\n- Proposed a neural network model for geolocation prediction\\n- Unifies: text, metadata, user network\\n- Improvement over previous state-of-the-art models\\n+2.8-3.8% in accuracy\\n+2.4-6.6% in accuracy\\n- Analysis of attention probabilities:\\n* Capturing statistical characteristics of the datasets"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "Future Works\\n- An extension of the proposed model\\n- Introduction of temporal state\\n- Capture location changes like travel\\n- Application to different tasks\\n- For example, gender analysis or age analysis\\n- Some metadata may not be effective"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          "Thank You!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 30,
        "texts": [
          "References\\n\\n(1) Zhiyuan Cheng, James Caverlee, and Kyumin Lee.\\n2010. You are where you tweet: A content-based approach to geo-locating Twitter users. In Proc. of CIKM\\n2010. pp. 759-768.\\n\\nKyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.\\n2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proc. of EMNLP\\n2014. pp. 1724-1734.\\n\\nJacob Eisenstein, Brendan O'Connor, Noah A. Smith, and Eric P. Xing.\\n2010. A latent variable model for geographic lexical variation. In Proc. of EMNLP\\n2010. pp. 1277-1287.\\n\\nBo Han, Paul Cook, and Timothy Baldwin.\\n2013. A stacking-based approach to Twitter user geolocation prediction. In Proc. of ACL 2013: System Demonstrations. pp.\\n7-12.\\n\\nBo Han, Paul Cook, and Timothy Baldwin.\\n2014. Text-based Twitter user geolocation prediction. Journal of Artificial Intelligence Research 49(1): 451-500.\\n\\nBo Han, Afshin Rahimi, Leon Derczynski, and Timothy Baldwin.\\n2016. Twitter geolocation prediction shared task of the 2016 workshop on noisy user-generated text. In Proc. of W-NUT\\n2016. pp. 213-217."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 31,
        "texts": [
          "References\\n\\nGaya Jayasinghe, Brian Jin, James Mchugh, Bella Robinson, and Stephen Wan.\\n2016. CSIRO Data61 at the WNUT geo shared task. In Proc. of W-NUT\\n2016. pp. 218-226.\\n\\nYasuhide Miura, Motoki Taniguchi, Tomoki Taniguchi, and Tomoko Ohkuma.\\n2016. A simple scalable neural networks based model for geolocation prediction in Twitter. In Proc. of W-NUT\\n2016. pp. 235-239.\\n\\nAfshin Rahimi, Trevor Cohn, and Timothy Baldwin. 2015a.\\nTwitter user geolocation using a unified text and network prediction model. In Proc. of ACL\\n2015. pp. 630-636.\\n\\nAfshin Rahimi, Duy Vu, Trevor Cohn, and Timothy Baldwin. 2015b.\\nExploiting text and network context for geolocation of social media users. In Proc. of NAACL-HLT\\n2015. pp. 1362-1367.\\n\\nStephen Roller, Michael Speriosu, Sarat Rallapalli, Benjamin Wing, and Jason Baldridge.\\n2012. Supervised text-based geolocation using language models on an adaptive grid. In Proc. of EMNLP\\n2012. pp. 1500-1510."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 32,
        "texts": [
          "Any Questions?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 33,
        "texts": [
          "Motivation\\n- Crucial for tasks like:\\n- Disaster Analysis\\n- Disease Analysis\\n- Political Analysis\\n\\nEnables a region specific analysis in:\\n- Sentiment Analysis\\n- User Attribute Analysis"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 34,
        "texts": [
          "Other Geolocation Targets\\n- In previous works:\\n- Wikipedia articles (Overell,\\n2009)\\n- Flicker photos (Serdyukov et al., 2009; Crandall et al.,\\n2009)\\n- Facebook users (Backstrom et al.,\\n2010)\\n- Twitter users (Cheng et al., 2010; Eisenstein et al., 2010)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 35,
        "texts": [
          "Comparison with Previous Approaches\\n- Ensemble approaches\\n- Stacking (Han et al., 2013,\\n2014)\\n- Dongle nodes (Rahimi et al., 2015a, 2015b)\\n- Cascades ensemble (Jayasinghe et al.,\\n2016)\\n\\nThis work:\\n- Neural network\\n- Multiple inputs\\n- Attention mechanism to merge inputs"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 36,
        "texts": [
          "Text and Metadata Component (2)\\n- Location, Description prion\\n- RNN + Attention\\n- Single text\\n- Timezone\\n- Embedding for each timezone\\n- Merge\\n- Attention over four representations\\n\\neprance location description timezone"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 37,
        "texts": [
          "Data (1) (detailed)\\n- Two open datasets\\n-\\n- Twitter US\\n- The dataset of Roller et al. (2012)\\n- 449K Twitter users\\n- North America region\\n- W-NUT\\n- The dataset of W-NUT 2016 geolocation prediction shared task (Han et al.,\\n2016)\\n- 1.02M Twitter users\\n- Worldwide"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 39,
        "texts": [
          "Baselinese LR-regularized logistic regression with k-d tree regions (Roller et al., 2012; Rahimi et al., 2015a)\\n- MADCEL-B-LR\\n- LR with Modified Adsorption (Rahimi et al., 2015a)\\n- Modified Adsorption is a graph-based label propagation algorithm\\n- LR-STACK\\n- Stacking of logistic regression classifiers\\n- Four LR classifiers for messages and metadata\\n- Regularized logistic regression meta-classifier\\n- Similar to previous stacking approaches (Han et al., 2013,\\n2014)\\n- MADCEL-B-LR-STACK\\n- LR-STACK with Modified Adsorption"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 40,
        "texts": [
          "Results (Twitter US)\\n\\nSign Test\\n\\nINCE\\n\\nModel ID\\n\\n@161\\n\\nMeatan\\n- Han et al. (2012)\\n- 26.0 45.0 260 814\\n\\nWing and Baldridge (2014)\\n- 49.2 170.5 703.6\\n\\nBaselines\\n- LR (Rahimi et al. 2015b)\\n- 50 159 686 (reported)\\n- LR-NA (Rahimi et al.\\n2016)\\n- 51 148 636\\n- MADCEL-B-LR (Rahimi et al. 2015a)\\n- 60 77 533\\n- MADCEL-W-LR (Rahimi et al. 2015a)\\n- 60 78 529\\n\\nLR\\n- 42.0 52.7 121.1 666.6\\n\\nBaselines\\n- MADCEL-B-LR (implemented)\\n- 50.2 60.1 66.5 582.8\\n- LR-STACK\\n- 50.8 64.1 42.3* 427.7\\n- MADCEL-B-LR-STACK\\n- 55.7 67.7 45.1 412.7\\n\\nSUB-NN-TEXT\\n- 44.9\" 55.6\" 110.5 585.1**\\n\\nOur Models\\n- SUB-NN-UNET\\n- 51.0 61.5\" 65.0 481.5\"\\n- SUB-NN-META\\n- 54.6\" 67.2\" 46.8 356.3\"\\n- Proposed Model\\n- 58.5\" 70.1** 41.9* 335.7\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 41,
        "texts": [
          "Sign. Test Accuracy\\n- Error Distance\\n\\nMY rer ANC be as .ID 7 ATT Median Mean\\n\\nBaselines\\n\\nMiura et al. (2016) 47.6\\n- 16.1 1122.3 (reported)\\n\\nJayasinghe et al. (2016) 52.6\\n- 21.7 1928.8\\n\\nLR i 34.1 46.7 248.7 2216.4\\n\\nBaselines\\n\\nMADCEL-B-LR ii 36.2 49.7 166.3 2120.6 (implemented)\\n\\nLR-STACK iii 51.2 64.9 0.0 1496.4\\n\\nMADCEL-B-LR-STACK iv 51.6 65.3 0.0 1471.9\\n\\nSUB-NN-TEXT i 35.4 TM 50.3 TM 155.8 1592.6\\n\\nOur Model\\n\\nSUB-NN-UNET ii 38.1 53.3 TM 99.9 1498.6\\n\\n'ume'\"S\\n\\nSUB-NN-META iii = 54.7 70.2 0.0 825.8\\n\\n*Proposed Model iv 56.4 TM 71.9 TM 0.0 780.5"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 42,
        "texts": [
          "Clustering of Attention, and Attention,\\n- k-means with 9 clusters\\n\\nSome typical patterns\\n- Cluster 2 and Cluster 3 balancing Timeline and Location\\n- TwitterUS\\n- ID: 0.843 0.082 0.040 0.035 0.359 0.641\\n- W-NUT\\n- 0.517 0.317 0.081 0.085 0.732 0.268\\n- TwitterUS\\n- 0.432 0.430 0.069 0.069 0.319 0.681\\n- W-NUT\\n- 0.637 0.160 0.097 0.105 0.737 0.263\\n- TwitterUS\\n- 0.593 0.219 0.114 0.075 0.230 0.770\\n- TwitterUS\\n- 0.672 0.214 0.069 0.045 0.365 0.635\\n- W-NUT\\n- 0.741 0.077 0.080 0.102 0.605 0.395\\n- TwitterUS\\n- 0.766 0.099 0.068 0.067 0.222 0.778\\n- W-NUT\\n- 0.800 0.067 0.056 0.078 0.730 0.270"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 43,
        "texts": [
          "Errors with High Confidences\\n- Incorrect Location Field\\n- EX. Location Field: Hong Kong\\n- Ground-truth: Toronto\\n- Perhaps, a house move\\n- Travel\\n- EX. Tweeting about \"San Francisco\"\\n- Ground-truth: Boston"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 44,
        "texts": [
          "Model Configuration (1)\\nTwitter\\nUS\\nW-NUT"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 45,
        "texts": [
          "Model Configuration\\n\\nPre-training\\n- Word Embeddings\\n- skip-gram\\n- learning rate=0.025, window size=5, negative sample size=5, epoch=5\\n\\nNetwork Embeddings\\n- LINE\\n- initial learning rate=0.025, order=2, negative sample size=5, training sample size=100M\\n\\nOptimization\\n- Adam\\n- regularization"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 46,
        "texts": [
          "Training Time\\n- GPU\\n- GeForce GTX Titan X\\n- Titan X\\n- Time\\n- 15-20 hours* Full model\\n- Longer for Twitter US"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 103,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "2TUPA\\n- Transition-based UCCA Parser\\n\\nThe first parser to support the combination of three properties:\\n1. Non-terminal nodes\\n- entities and events over the text\\n\\nYou want RStake a long bath"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "TUPA\\n- Transition-based UCCA Parser\\n\\nThe first parser to support the combination of three properties:\\n1. Non-terminal nodes\\n- entities and events over the text\\n2. Reentrancy\\n- allow argument sharing\\n\\n'You want to take a long bath"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "TUPA\\n- Transition-based UCCA Parser\\n\\nThe first parser to support the combination of three properties:\\n1. Non-terminal nodes\\n- entities and events over the text\\n2. Reentrancy\\n- allow argument sharing\\n3. Discontinuity\\n- conceptual units are split\\n- needed for many semantic schemes (e.g. AMR, UCCA).\\n\\n'You want to take a long bath"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "5 Introduction"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Linguistic Structure Annotation Schemes\\n- Syntactic dependencies (Oepen et al.,\\n2016)\\n- Semantic dependencies\\n- Syntactic (UD)\\n- Semantic (DM)\\n- Bilexical dependencies."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Linguistic Structure Annotation Schemes\\n- Syntactic dependencies\\n- Semantic dependencies (Oepen et al.,\\n2016)\\n- Semantic role labeling (PropBank, FrameNet)\\n- AMR (Banarescu et al.,\\n2013)\\n- UCCA (Abend and Rappoport,\\n2013)\\n- Other semantic representation schemes\\n\\nSemantic representation schemes attempt to abstract away from syntactic detail that does not affect meaning:\\n\\nSee recent survey (Abend and Rapoport, 2017)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "The UCCA Semantic Representation Scheme"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Universal Conceptual Cognitive Annotation (UCCA)\\nCross-linguistically applicable (Abend and Rappoport, 2013).\\nStable in translation (Sulem et al., 2015).\\nSCN\\n\\nIBM happened to choose a company with a crucial vulnerability, despite vetting.\\nIBM chose in mistake a company very vulnerable despite that checked it beforehand."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "10 Universal Conceptual Cognitive Annotation (UCCA)\\nRapid and intuitive annotation interface (Abend et al., 2017).\\nUsable by non-experts. ucca-demo.cs.huji.ac.il\\nFacilitates semantics-based human evaluation of machine translation (Birch et al., 2016).\\nucca.cs.huji.ac.il/mtevali\\n\\nCuan opal oe ara Soy acon oe Ceol---- school counsellor.\\nThe family soon moved to Springfield, Missouri, where he lived together with his younger siblings, Douglas (born\\n1966) and Jie Nea (born 1969).\\nBan ino a conservative household he was raised as Southern Baptist, but has since stated that he does not"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Graph Structure\\n\\nUCCA generates a directed acyclic graph (DAG). Text tokens are terminals, complex units are non-terminal nodes.\\nRemote edges enable reentrancy for argument sharing. Phrases may be discontinuous (e.g., multi-word expressions).\\n- primary edge\\n- remote edge\\n- participant\\n- center\\n- adverbial\\n- function\\n\\nYou want to take a long bath."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Transition-based UCCA Parsing"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "14 Transition-Based Parsing\\n\\nFirst used for dependency parsing (Nivre, 2004).\\nParse text with respect to graph G incrementally by applying transitions to the parser state: stack, buffer and constructed graph.\\nInitial state: stack buffer"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "15 Transition-Based Parsing\\n\\nFirst used for dependency parsing (Nivre, 2004).\\nParse text with respect to graph G incrementally by applying transitions to the parser state: stack, buffer and constructed graph.\\n\\nInitial state: stack buffer\\n\\nTransitions:\\n- SHIFT\\n- REDUCE\\n- NO-DESC\\n- LEFT-EDGE\\n- RIGHT-EDGE\\n- LEFT-REMOTE\\n- RIGHT-REMOTE\\n- SWAP\\n- FINISH\\n\\nSupport non-terminal nodes, reentrancy and discontinuity."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Example\\n- SHIFTstack buffergraph"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "17 Example = RIGHT-EDGE, stack buffer graph x You"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "18 Example => SHIFT stack buffer | You ran graph x You"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Example\\n- Swapstack buffergraph"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Example= RIGHT-EDGE pstack buffer graph P. You want"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "21 Example = REDUCE stack buffer (egraph You want"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Example SHIFTstack buffer graph You want"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Example SHIFTstack buffer"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Example=> NODE-stack buffer FW Jae [a [one attgraph P. You want @ f to"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Example = REDUCE stack buffer graph P. You want to"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Example SHIFTstack buffer© You a long bathgraph. You want to"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Example\\n- NODEcstack buffer © [You | © tate | CO Je [lone [atgraphval You want @/to atake"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Example :=> REDUCE stack buffer graph val You want to take"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "30 Example => SHIFT stack buffer graph val You want @ / to take"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "31 Example = RIGHT-EDGE pstack buffer em [o Te graph val You want to take"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          "Example\\n- SHIFT stack buffer graph val You want to take"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 30,
        "texts": [
          "Example\\n- RIGHT-EDGE- stack buffer [yu [@lergraphval You want to take a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 33,
        "texts": [
          "Example => Swapstack buffer graph value You want J? to C /F take a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 34,
        "texts": [
          "Example = RIGHT-EDGE pstack buffer graph val You want J to C / F take a long"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 35,
        "texts": [
          "Example: => REDUCE stack buffer graph r al You want F P 5 to C c F take a long"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 36,
        "texts": [
          "Example |=> Swapstack buffer @ [You | @ | bathgraph val You want F P 5 to C / F take a long"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 37,
        "texts": [
          "Example = RIGHT-EDGE, stack buffer [ete] graph P A You want F P 5 to C / F take a long"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 38,
        "texts": [
          "41 Example => REDUCE stack buffer 'You bath graph Pp A You want to take a long"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 39,
        "texts": [
          "Example: REDUCE stack buffer 'You | @ | bath | graph A You want to take a long"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 40,
        "texts": [
          "Example=> SHIFTstack buffer graph PA You want F P 5 to C / F take a long"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 41,
        "texts": [
          "Example => LEFT-REMOTE, stack buffer graph P. A You want me * h, F P DTA to saga A Caguss C / F take a long"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 42,
        "texts": [
          "Example => SHIFT stack buffer [You | @ [oath] lgraph P A You want Re My, F P Dae to a A O ess C / F take a long"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 43,
        "texts": [
          "AT Example = RIGHT-EDGE cstack buffer You [@ [bath] lgraph P.\\nA You want ReSS Z p bae to a A O ess C / F take a long bath"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 44,
        "texts": [
          "Example: FINISH stack buffer You | @ | bath | graph P A You want Bg Se F P Dae to take a long bath"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 45,
        "texts": [
          "Training\\n\\nAn oracle provides the transition sequence given the correct graph:\\n\\nYou want\\n- Take a long bath\\n- SHIFT, RIGHT-EDGE\\n- SHIFT\\n- Swap\\n- RIGHT-EDGE\\n- REDUCE\\n- SHIFT\\n- SUIT\\n- NODE\\n- REDUCE\\n- SHIFT\\n- SHIFT\\n- NODE\\n- c\\n- REDUCE\\n- SHIFT\\n- RIGHT-EDGE\\n- p\\n- SHIFT\\n- RIGHT-EDGE\\n- r\\n- REDUCE\\n- SHIFT\\n- SWAP\\n- RIGHT-EDGE\\n- p\\n- REDUCE\\n- SWAP\\n- RIGHT-EDGE\\n- REDUCE\\n- REDUCE\\n- SHIFT\\n- SHIFT\\n- LEFT-REMOTE\\n- SHIFT\\n- RIGHT-EDGE\\n- c\\n- FINISH"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 46,
        "texts": [
          "TUPA Model\\n\\nLearn to greedily predict transition based on current state.\\n\\nExperimenting with three classifiers:\\n- Sparse Perceptron with sparse features (Zhang and Nivre, 2011).\\n- MLP Embeddings + feedforward NN (Chen and Manning, 2014).\\n- BiLSTM Embeddings + deep bidirectional LSTM + MLP (Kiperwasser and Goldberg, 2016).\\n\\nFeatures: words, POS, syntactic dependencies, existing edge labels from the stack and buffer + parents, children, grandchildren; ordinal features (height, number of parents and children)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 47,
        "texts": [
          "51TUPA Model\\n\\nLearn to greedily predict transition based on current state.\\n\\nExperimenting with three classifiers:\\n- Sparse Perceptron with sparse features (Zhang and Nivre, 2011).\\n- MLP Embeddings + feedforward NN (Chen and Manning, 2014).\\n- BiLSTM Embeddings + deep bidirectional LSTM + MLP (Kiperwasser and Goldberg, 2016).\\n\\nEffective \"lookahead\" encoded in the representation.\\n\\nYou want to take a long bath"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 49,
        "texts": [
          "graphstack | @ [You @ lake] | You want to buffer | a long bath\\n- take NODE deal | (ism) stm stm 'stm (sr) ism (ism is tu si) istm) fist istm sim) 'sta) |J TF\\nT) YT) You want to take a long bath"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 50,
        "texts": [
          "56 Experiments Qo a = 2 = ) ac"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 51,
        "texts": [
          "57 Experimental Setup train dev test UCCA Wikipedia corpus (4268 + 454 + 503 sentences).\\n\\nOut-of-domain: English part of English-French parallel corpus, Twenty Thousand Leagues Under the Sea (506 sentences)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 52,
        "texts": [
          "Baselines\\n\\nNo existing UCCA parsers = conversion-based approximation.\\n\\nBilexical DAG parsers (allow reentrancy):\\n- DAGParser (Ribeyre et al., 2014): transition-based.\\n- TurboParser (Almeida and Martins, 2015): graph-based.\\n\\nTree parsers (all transition-based):\\n- MaltParser (Nivre et al., 2007): bilexical tree parser.\\n- Stack LSTM Parser (Dyer et al., 2015): bilexical tree parser.\\n- UPARSE (Maier, 2015): allows non-terminals, discontinuity.\\n\\nUCCA bilexical DAG approximation (for tree, delete remote edges)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 53,
        "texts": [
          "Bilexical Graph Approximation\\n1. Convert UCCA to bilexical dependencies.\\n2. Train bilexical parsers and apply to test sentences.\\n3. Reconstruct UCCA graphs and compare with gold standard.\\n\\nAfter graduation, Joe moved to Paris."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 54,
        "texts": [
          "60 Evaluation\\n\\nComparing graphs over the same sequence of tokens,\\n- Match edges by their terminal yield and label.\\n- Calculate labeled precision, recall and F1 scores.\\n- Separate primary and remote edges.\\n\\ngold predicted\\nH U H\\nAfter, After\\npee GASA ' aePl OS P S| \\A Rograduation \"Joe moved graduation ' P/F \\ BeR C ise moved 'to Paris to Paris\\n\\nPrimary: LP ER LE\\nRemote: EF LR LF\\nYS = 67%\\nF = 60%\\n64%\\nT = 50%\\nF = 100%\\n67%"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 55,
        "texts": [
          "Results TUPAgitstm obtains the highest F-scores in all metrics:\\n\\nPrimary edges Remote edges\\nLP   LR   LF   LP   LR   LF\\nTUPAsSparse    64.5  63.7  64.1/198  134  16\\nTUPAmip       65.2  64.6  64.9 | 23.7  13.2  16.9\\nTUPAsitstm    | 74.4  72.7  73.5 | 47.4  51.6  49.4\\nBilexical DAG (91) (58.3)\\nDAGParser     61.8  55.8  58.6/95  0.5  1\\nTurboParser    | 57.7  46  51.2 | 77.8  1.8 af\\nBilexical tree (91)\\nMaltParser    62.8  57.7  60.2\\n-\\naStack LSTM    | 73.2  66.9  69.9\\n-\\ncaTree (100)\\nUPARSE        60.9  61.2  61.1\\n-\\n\\nResults on the Wiki test set."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 56,
        "texts": [
          "Results Comparable on out-of-domain test set:\\n\\nPrimary edges\\nRemote edges\\nLP  LR  LF\\nLP  LR  LF\\n\\nTUPAs Sparse 59.6  59.9  59.8 | 22.2  7.7  11.5\\nTUPA mipe 62.3  62.6  62.5 | 20.9  63  97\\nTUPA gitstm | 68.7  68.5  68.6 | 38.6  18.8  25.3\\nBilexical DAG (91.3) (43.4)\\nDAG Parser 56.4  50.6  53.4  _  0  0\\nTurbo Parser | 50.3  37.7  43.1]  100  04  08\\nBilexical tree (91.3)\\n-Malt Parser S78 53  65.2\\n-\\n-\\n-\\nStack LSTM | 66.1  61.1  63.5\\n-  \"\\n-\\nTree (100)\\n-UPARSE 52.7  52.8  52.8  =  =  2\\n\\nResults on the 20K Leagues out-of-domain set."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 57,
        "texts": [
          "63\\n\\nConclusione\\n\\nUCCA's semantic distinctions require a graph structure including non-terminals, reentrancy and discontinuity.\\n- TUPA is an accurate transition-based UCCA parser, and the first to support UCCA and any DAG over the text tokens.\\n- Outperforms strong conversion-based baselines.\\n\\nCode: github.com/danielhers/tupa\\nDemo: bit.ly/tupademo\\nCorpora: cs.huji.ac.il/~oabend/ucca.html"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 58,
        "texts": [
          "Conclusione\\n\\nUCCA's semantic distinctions require a graph structure including non-terminals, reentrancy and discontinuity.\\nTUPA is an accurate transition-based UCCA parser, and the first to support UCCA and any DAG over the text tokens.\\nOutperforms strong conversion-based baselines.\\n\\nFuture Work:\\n- More languages (German corpus construction is underway).\\n- Parsing other schemes, such as AMR.\\n- Compare semantic representations through conversion.\\n- Text simplification, MT evaluation and other applications.\\n\\nCode: github.com/danielhers/tupa\\nDemo: bit.ly/tupademo\\nCorpora: cs.huji.ac.il/~oabend/ucca.html\\n\\nThank you!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 59,
        "texts": [
          "66 References\\n- Abend, O. and Rappoport, A. (2013). Universal Conceptual Cognitive Annotation (UCCA). In Proc. of ACL, pages 228-238.\\n- Abend, O. and Rappoport, A. (2017). The state of the art in semantic representation. In Proc. of ACL.\\n- Abend, O., Yerushalmi, S., and Rappoport, A. (2017).\\nUCCAApp: Web-application for syntactic and semantic phrase-based annotation. In Proc.\\nof ACL: System Demonstration Papers.\\n- Almeida, M. S. C. and Martins, A. F. T. (2015).\\nLisbon: Evaluating Turbo Semantic Parser on multiple languages and out-of-domain data. In Proc.\\nof SemEval, pages 970-973.\\n- Banarescu, L., Bonial, C., Cai, S., Georgescu, M., Griffitt, K., Hermjakob, U., Knight, K., Palmer, M., and Schneider, N.\\n(2013). Abstract Meaning Representation for sembanking. In Proc. of the Linguistic Annotation Workshop.\\n- Birch, A., Abend, O., Bojar, O., and Haddow, B. (2016). HUME: Human UCCA-based evaluation of machine translation.\\nIn Proc. of EMNLP, pages 1264-1274.\\n- Chen, D. and Manning, C. (2014). A fast and accurate dependency parser using neural networks. In Proc.\\nof EMNLP, pages 740-750."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 60,
        "texts": [
          "67 References\\n\\nDyer, C., Ballesteros, M., Ling, W., Matthews, A., and Smith, N. A. (2015).\\nTransition-based dependency parsing with stack long short-term memory. In Proc. of ACL, pages 334-343.\\n\\nKiperwasser, E. and Goldberg, Y. (2016).\\nSimple and accurate dependency parsing using bidirectional LSTM feature representations. TACL, 4:313-327.\\n\\nMaier, W. (2015). Discontinuous incremental shift-reduce parsing. In Proc. of ACL, pages 1202-1212.\\n\\nNivre, J. (2004). Incrementality in deterministic dependency parsing.\\nIn Keller, F., Clark, S., Crocker, M., and Steedman, M., editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together, pages 50-57, Barcelona, Spain.\\nAssociation for Computational Linguistics.\\n\\nNivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit, G., Kiibler, S., Marinov, S., and Marsi, E. (2007).\\nMaltParser: A language-independent system for data-driven dependency parsing.\\nNatural Language Engineering, 13(02):95-135.\\n\\nOepen, S., Kuhlmann, M., Miyao, Y., Zeman, D., Cinkova, S., Flickinger, D., Hajic, J., lvanova, A., and Uresova, Z.\\n(2016). Towards comparability of linguistic graph banks for semantic parsing. In LREC.\\n\\nRibeyre, C., Villemonte de la Clergerie, E., and Seddah, D. (2014).\\nAlpage: Transition-based semantic graph parsing with syntactic features. In Proc. of SemEval, pages 97-103."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 61,
        "texts": [
          "68 References\\n\\nSulem, E., Abend, O., and Rappoport, A. (2015).\\nConceptual annotations preserve structure across translations: A French-English case study. In Proc.\\nof S2MT, pages 11-22\\n\\nZhang, Y. and Nivre, J. (2011). Transition-based dependency parsing with rich non-local features.\\nIn Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188-193."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 62,
        "texts": [
          "69 Backup"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 63,
        "texts": [
          "UCCA CorporaWiki\\n\\n20K Train Dev Test\\n\\nLeagues\\n# passages 300 34 33 154\\n# nodes 298,993 33,704 35,718 | 29,315\\n% terminal 42.96 43.54 42.87 42.09\\n% non-term. | 58.33 57.60 58.35 60.01\\n% discont. 0.54 0.53 0.44 0.81\\n% reentrant | 2.38 1.88 2.15 2.03\\n# edges 287,914 32,460 34,336 | 27,749\\n% primary 98.25 98.75 98.74 | 97.73\\n% remote 1.75 1.25 1.26 2.27\\n\\nAverage per non-terminal node\\n# children | 1.67 1.68 1.66 | 1.61\\n\\nCorpus statistics."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 64,
        "texts": [
          "71 Evaluation\\n\\nMutual edges between predicted graph Gp = (Vp, Ep, 2p) and gold graph Gg = (Vz, Eg, fg), both over terminals W = {m,..., Wn}:\\n\\nM(Gp, Gg) = {(e1, e2) ∈ Ep × Eg | y(e1) = y(e2) ∧ lp(e1) = lp(e2)}\\n\\nThe yield y(e) ∈ W of an edge e = (u,\\nv) in either graph is the set of terminals in W that are descendants of v. @ is the edge label.\\n\\nLabeled precision, recall and F-score are then defined as:\\n\\nip = M(Gp, Gg) | p = |M(Gp, Gg)| / [|Ep| + |Eg|]\\n2\\n- LP\\n- LR\\nLF = ----\\nLP + LR\\n\\nTwo variants: one for primary edges, and another for remote edges."
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 98,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Two approaches to summarization\\n\\nExtractive Summarization\\n- Select parts (typically sentences) of the original text to form a summary.\\n- Easier\\n- Too restrictive (no paraphrasing)\\n- Most past work is extractive\\n\\nAbstractive Summarization\\n- Generate novel sentences using natural language generation techniques.\\n- More difficult\\n- More flexible and humane\\n- Necessary for future progress"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Sequence-to-sequence + attention model\\nContext Vector \"beat\"\\nGermany emerged victorious\\n- in 2-0 win against Argentina on Saturday ..."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Sequence-to-sequence + attention model beats Germany emerge victorious\\n- in 2-0 win against Argentina on Saturday ...\\n\\nSource Text Partial Summary"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Sequence-to-sequence + attention model\\n\\nGermany beat Argentina 2-0\\n\\nGermany emerge victorious\\n- in 2-0 win against Argentina on Saturday ..."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Two Problems\\n\\nProblem 1: The summaries sometimes reproduce factual details inaccurately.\\ne.g. Germany beat Argentina 3-2\\n\\nIncorrect rare or out-of-vocabulary word\\n\\nSolution: Use a pointer to copy words.\\n\\nProblem 2: The summaries sometimes repeat themselves.\\ne.g. Germany beat Germany beat Germany beat..."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Germany beat Argentina 20\\n- .\\nGermany emerge victorious in win against Argentina on Saturday.\\nBest of both worlds: Y extraction + abstraction.\\n\\nSource Text\\n[1] Incorporating copying mechanism in sequence-to-sequence learning. Gu et al.,\\n2016.\\n[2] Language as a latent variable: Discrete generative models for sentence compression. Miao and Blunsom, 2016."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Improvements\\n\\nBere tarUNK UNK was expelled from the gaioz nigalidze.\\n\\nwas expelled from the Dubai Open Chess Tournament.\\n\\nDubai Open Chess Tournament the 2015 Rio Olympic Games the 2016 Rio Olympic Games"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Two Problems\\n\\nProblem 1: The summaries sometimes reproduce factual details inaccurately.\\ne.g. Germany beat Argentina 3-2\\nSolution: Use a pointer to copy words.\\n\\nProblem 2: The summaries sometimes repeat themselves.\\ne.g. Germany beat Germany beat Germany beat...\\nSolution: Penalize repeatedly attending to same parts of the source text."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Reducing repetition with coverage\\n\\nCoverage = cumulative attention = what has been covered so far\\n\\nSource Text: Germany emerged victorious in a 2-0 win against Argentina on Saturday\\n\\nSummary: Germany beat\\n\\nModeling coverage for neural machine translation. Tu et al., 2016,\\n\\nCoverage embedding models for neural machine translation. Mi et al., 2016\\n\\nDistraction-based neural networks for modeling documents. Chen et al., 2016."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Reducing repetition with coverage\\n\\nCoverage = cumulative attention = what has been covered so far\\n\\nSource Text: Germany emerge victorious in 2-0 win against Argentina on Saturday\\n\\nSummary: Germany beat\\n1. Use coverage as extra input to attention mechanism.\\n2. Modeling coverage for neural machine translation. Tu et al., 2016,\\n3. Coverage embedding models for neural machine translation. Mi et al., 2016\\n4. Distraction-based neural networks for modeling documents. Chen et al., 2016."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Coverage = cumulative attention = what has been covered so far\\n\\nDon't attend here\\n\\nSource Text: Germany emerged in 2-0 win against Argentina on Saturday\\n\\nSummary: Germany beat\\n1. Use coverage as extra input to attention mechanism.\\n2. Penalize attending to things that have already been covered.\\n\\nResult: repetition rate reduced to [4] Modeling coverage for neural machine translation. Tu et al., 2016, . . .\\n[5] Coverage embedding models for neural machine translation.\\nMi et al., 2016 level similar to human summaries [6] Distraction-based neural networks for modeling documents.\\nChen et al., 2016."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Summaries are still mostly extractive.\\n\\nArticle (truncated): Lagos, Nigeria (CNN) A day after winning Nigeria's presidency, Muhammadu Buhari told CNN's Christiane Amanpour that he plans to aggressively fight corruption that has long plagued Nigeria and go after the root of the nation's unrest.\\nBuhari said he \"ll\" rapidly give attention \"to curbing violence in the northeast part of Nigeria, where the terrorist group Boko Haram operates.\\nBy cooperating with neighboring nations Chad, Cameroon and Niger, he said his administration is confident it will be able to thwart criminals and others contributing to Nigeria's instability.\\n\\nFor the first time in Nigeria's history, the opposition defeated the ruling party in democratic elections.\\nBuhari defeated incumbent Goodluck Jonathan by about 2 million votes, according to Nigeria's independent national electoral commission.\\nThe win comes after a long history of military rule, coups and botched attempts at democracy in Africa's most populous nation."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "ROUGE compares the machine-generated summary to the human-written reference summary and counts co-occurrence of 1-grams, 2-grams, and longest common sequence.\\nNallapati et al. 2016 joss faa aa7 Previous best abstractive result"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Results ROUGE compares the machine-generated summary to the human-written reference summary and counts co-occurrence."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Results ROUGE compares the machine-generated summary to the human-written reference summary and counts co-occurrence.\\nPrevious best abstractive result"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Results ROUGE compares the machine-generated summary to the human-written reference summary and counts co-occurrence.\\n\\nPrevious best abstractive result:\\n- Our improvements worse ROUGE; better human eval."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "The difficulty of evaluating summarization. Summarization is subjective. There are many correct ways to summarize."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "The difficulty of evaluating summarization.\\n\\nSummarization is subjective. There are many correct ways to summarize.\\nROUGE is based on strict comparison to a reference summary. It is intolerant to rephrasing.\\nRewards extractive strategies."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "The difficulty of evaluating summarization.\\nSummarization is subjective.\\nThere are many correct ways to summarize.\\nROUGE is based on strict comparison to a reference summary.\\nIntolerant to rephrasing.\\nRewards extractive strategies.\\n\\nTake first 3 sentences as summary\\n- higher ROUGE than (almost) any published system.\\nPartially due to news article structure."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "CINN Robots tested in\\n\\nA crowd gathers near the entrance of Tokyo's upscale Mitsukoshi Department Store, which traces its roots to a kimono shop in the late 17th century.\\nFitting with the store's history, the new greeter wears a traditional Japanese kimono while delivering information to the growing crowd, whose expressions vary from amusement to bewilderment.\\n\\nIt's hard to imagine the store's founders in the late 1600s could have imagined this kind of employee.\\nThat's because the greeter is not a human\\n- it's a robot. Aiko Chihira is an android manufactured by Toshiba, designed to look and move like a real person."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "What next?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "methods SAFETY"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Human-level summarization & 2a oS asos, % <O\\n- %, < oMOUNT ABSTRACTION methods SAFETY"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Human-level summarization methods SAFETY"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Human-level summarization more high-level understanding? more scalability? better metrics?\\n- iMOUNT ABSTRACTION SWAMP OF BASIC ERRORS\\n\\nmethods SAFETY"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Thank you!\\n\\nBlog post: www.abigailsee.com\\n\\nCode: github.com/abisee/pointer-generator"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 53,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Introduction\\n\\nKeyphrase: Language-specific Models in Multilingual Topic Tracking\\n\\nLeah S. Lay, Fangfang Feng, Margaret Connell, Victor Lavrenko\\n\\nCenter for Intelligent Information Retrieval\\nDepartment of Computer Science\\nUniversity of Massachusetts\\nAmherst, MA 01003\\n{lay, feng, connell, lavrenko}@cs.umass.edu\\n\\nApplications\\n\\nABSTRACT\\n\\nTopic tracking is complicated when the stories in the stream occur in multiple languages.\\nTypically, researchers have trained only English topic models because the training stories have been provided in English.\\nIn tracking, non-English test stories are then machine translated into English to compare them with the topic models.\\nWe propose a native language hypothesis stating that comparisons would be more effective in the original language of the stories.\\nIn this respect, we explore the best ways to compare stories and topics when stories are in multiple languages.\\n\\nWe began with the hypothesis that if two stories originated in the same language, it would be best to compare them in that language, rather than translating them both into another language for comparison.\\nThis simple assertion, which we call the native language hypothesis, is easily tested in the TDT story link detection task.\\n\\nThe picture gets more complex in a task like topic tracking, which begins with a small number of training stories (in English) to define each topic.\\nNew stories from a stream must be placed into these topics.\\nThe streamed stories originate in different languages but are also available in English translation.\\nThe translations have been performed automatically by machine translation algorithms and are inferior to manual translations.\\nAt the beginning of the multilingual stream, native language comparisons cannot be performed because..."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Background\\n\\nPrevious Approaches\\n- 3-step process\\n- Source Text Issues: Language-specific Models in Multilingual Topic Tracking\\n1. Candidates must be acquired from the source text.\\n\\nLeah S. Larkey, Fangfang Feng, Margaret Connell, Victor Lavrenko\\nCenter for Intelligent Information Retrieval\\nDepartment of Computer Science\\nUniversity of Massachusetts\\nAmherst, MA 01003\\n{larkey, feng, connell, lavrenko}@cs.umass.edu\\n\\n* Only able to predict phrases appearing in text\\n\\nABSTRACT\\n\\nTopic tracking is complicated when the stories in the stream occur in multiple languages.\\nTypically, researchers have trained only English topic models because the training stories have been provided in English.\\nIn tracking, non-English test stories are then machine translated into English to compare them with the topic models.\\nWe propose a native language hypothesis stating that comparisons would be more effective in the original language of the stories.\\n\\nWe explore the best ways to compare stories and topics when stories are in multiple languages.\\nWe began with the hypothesis that if two stories originated in the same language, it would be best to compare them in that language, rather than translating them both into another language for comparison.\\nThis simple assertion, which we call the native language hypothesis, is easily tested in the TDT story link detection task.\\n\\nThe picture gets more complex in a task like topic tracking, which must define each topic.\\nNew stories from a stream must be placed into these topics.\\nThe streamed stories originate in different languages, but are also available in English translation.\\nThe translations have been performed automatically by machine translation algorithms, and are inferior to manual translations.\\nAt the beginning of the multilingual stream, native language comparisons cannot be performed.\\n\\nGeneral Terms: Algorithms, Experimentation.\\nKeywords: (Arabic) [TDT] topic tracking, Performance Upper Bound"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Motivation\\n\\nRevisit Keyphrase Generation\\n\\nLanguage-specific Models in Multilingual Topic Tracking\\n\\nHow do humans assign keyphrases\\n- Topic tracking is complicated when the stories in the stream occur in multiple languages.\\nTypically, researchers have trained only English topic models because the training stories have been provided in English.\\nIn tracking, non-English test stories are then machine translated into English to compare them with the topic models.\\n\\nWe propose a native language hypothesis stating that comparisons would be more effective in the original language of the story.\\nWe first test and support the hypothesis for story link detection.\\nFor topic tracking, the hypothesis implies that it should be preferable to build separate language-specific topic models for each language in the stream.\\nWe compare different methods of incrementally building such native language topic models.\\n\\nGet hints from text, copy certain phrases\\n\\nRead & Understand\\n- Can machine simulate this process?\\n- Recurrent Neural Networks [Step 1-3]\\n- Copy Mechanism [Step 4]\\n- Keyphrase hypothesis"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Methodology\\n\\nRecurrent Neural Networks\\n\\nInput Text\\n\\nLanguage-specific Models in Multilingual Topic Tracking\\n\\nTopic tracking is complicated when the stories in the stream occur in multiple languages.\\nTypically, researchers have trained only English topic models because the training stories have been provided in English.\\n\\nIn tracking, non-English test stories are then machine translated into English to compare them with the topic models.\\nWe propose a native language hypothesis stating that comparisons would be more effective in the original language of the story.\\nWe first test and support the hypothesis for story link detection.\\nFor topic tracking the hypothesis implies that it should be preferable to build separate language-specific topic models for each language in the stream.\\n\\nWe compare different methods of incrementally building such native language topic models.\\n- Encoder-decoder model (Seq2seq)\\n- One RNN encoder and one RNN decoder\\n- Gated recurrent units (GRU) cell\\n- Decoder generates multiple short sequences by beam search\\n- Rank them and return the top K results"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Methodology of Recurrent Neural Networks\\n\\nInput Text\\nLanguage-specific Models in Multilingual Topic Tracking\\n\\nTopic tracking is complicated when the stories in the stream occur in multiple languages.\\nTypically, researchers have trained only English topic models because the training stories have been provided in English.\\nIn tracking, non-English test stories are then machine translated into English to compare them with the topic models.\\nWe propose stating that comparisons would be more effective in the original language of the story.\\nWe first test and support the hypothesis for story link detection.\\nFor topic tracking, the idea is that it should be to build separate language-specific topic models for each language in the stream.\\nWe compare different methods of building such native language topic models.\\n\\nProblem of RNN model\\n- RNN Dictionary\\n- Keep everything in memory\\n- Only train vectors for top 50k high-frequency words\\n- Long-tail words are replaced with an \"unknown\" symbol <unk>\\n- Unable to predict long-tail words\\n- Many keyphrases contain long-tail words (2%)\\n- 50k short-tail words\\n- 250k long-tail words"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Methodology Copy Mechanism\\n\\nInput Text\\n\\nLanguage-specific Models in Multilingual Topic Tracking\\n\\nTopic tracking is complicated when the stories in the stream occur in multiple languages.\\nTypically, researchers have trained only English topic models because the training stories have been provided in English.\\nIn tracking, non-English test stories are then machine translated into English to compare them with the topic models.\\nWe propose stating that topic tracking comparisons would be better in the original language of the story.\\nWe first test and hypothesize for story link multiple detection.\\nFor topic tracking, it applies that it should be to build separate language-specific models for each language in the stream.\\n\\nWe compare different methods of building such native language topic models.\\n\\nCopy RNN Model\\n- RNN Dictionary\\n- Copy words from input text\\n- Locate the words of interest by contextual \"topic\" features\\n- Copy corresponding part to output \"multiple\"\\n- Enhance the RNN with extractive ability\\n- Multilingual: 50k short-tail words, 250k long-tail words"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Experimenteg Dataset\\n\\noS- All data are scientific papers in Computer Science domain.\\n\\nTraining Data\\n- Collected from Elsevier, ACM Digital Library, Web of Science etc.\\n- # (Paper) = 14,207\\n- # (Phrase) = 3,011,651\\n- # (Unique word) = 324,163\\n\\nTesting Data\\n- Four commonly used datasets, only use abstract text\\n- Overlapping papers are removed from training dataset\\n\\ninspec | 800 | aarar@eey | set | 1206 | 26.08%\\nMus [ait [1486698 | 089 [rer | S48r%__Kp2ok | 20,000 | 105,476.27 | 66,221 | 39,250 | 37.21%"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Experimentee ee Dataset gee See#(Unique Keyphrase)=324, 163 Length | Number of | Percentage of Terms | Frequency\\n944840 30.88%\\none 160002 5.23%\\n44348 1.45%"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Experiment\\n\\nExperiment Setup\\n\\nEvaluation Methods\\n- Process ground-truth and predicted phrases with Porter stemmer\\n- Macro-average of precision, recall and F-measure @5, @10\\n\\nTasks\\n1. Present phrases prediction\\n- Compare to previous studies: Tf-Idf, TextRank, SingleRank, ExpandRank, KEA, Maui\\n2. Absent phrases prediction\\n- No baseline comparison\\n3. Transfer to news dataset"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Result See Task 1\\n- Predict Present Keyphrase\\n- Batasot | inapes | Krapivin\\n- Method\\n- F@5 | F@10\\n- F@10 | F@10\\n- F@10 | F@10\\n- Tf-idf | 0.221 | (0.313 | 0.129 | 0.160 | 0.136 | 0.184 | 0.128 | 0.194 | 0.102 | 0.126\\n- TextRank | 0.223 | 0.281 | 0.189 | 0.162 | 0.195 | 0.196 | 0.176 | 0.187 | 0.175 | 0.147\\n- SingleRank | 0.214 | 0.306 | 0.110 | 0.153 | 0.140 | 0.173 | 0.135 | 0.176 | 0.096 | 0.119\\n- ExpandRank | 0.210 | 0.304 | 0.110 | 0.152 | 0.132 | 0.164 | 0.139 | 0.170 |\\n- |\\n-\\n- 0.098 | 0.126 | 0.123 | 0.134 | 0.069 | 0.084 | 0.025 | 0.026 | 0.171 | 0.154\\n- Maui | 0.040 | 0.042 | 0.249 | 0.216 | 0.249 | 0.268 | 0.044 | 0.039 | 0.270 | 0.230\\n- RNN | 0.085 | 0.064 | 0.135 | 0.088 | 0.169 | 0.127 | 0.157 | 0.124\\n- GoOvRNNG [nU-2tB ole 01842 | 0.311 | 0.266 | 0.334 | 0.326 | 0.293 | 0.304\\n\\n(24.7%) | (9.3%) | (24.9%) | (23.1%) | (34.1%) | (21.6%) | (66.5%) | (56.7%) | (23.3%) | (13.9%)\\n\\nTake-away\\n1. Naive RNN model fails to compete with baseline models.\\n2. CopyRNN models outperform baseline models and RNN significantly.\\nCopy mechanism can capture key information in source text."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Result Secor\\n\\nExample\\n- Phraseness\\n\\n[Title] Nonlinear Extrapolation Algorithm for Realization of a Scalar Random Process\\n\\n[Abstract] A method of construction of a nonlinear extrapolation algorithm is proposed.\\nThis method makes it possible to take into account any nonlinear random dependences that exist in an investigated process and are described by mixed central moment functions.\\nThe method is based on the V. S. Pugachev canonical decomposition apparatus.\\nAs an example, the problem of nonlinear extrapolation is solved for a moment function of third order.\\n\\n[Ground-truth]\\n\\n6 ground-truth phrases\\n- moment function\\n- nonlinear extrapolation algorithm\\n- canonical decomposition apparatus\\n- scalar random process\\n- nonlinear random dependences\\n- mixed central moment functions\\n\\n[Prediction]\\n\\naccount anestohi nonlinear extrapol\\n\\nCopyRNN\\n\\nexample moment function\\n\\nmethod canon decomposit\\n\\nmixed central moment functions\\n\\nextrapol algorithm\\n\\nmoment function\\n\\nscalar random process\\n\\nnonlinear extrapolation random process\\n\\nnonlinear extrapolation algorithm\\n\\ncentral moment function\\n\\nnonlinear random dependences\\n\\nnonlinear extrapol algorithm\\n\\nproblem mix central moment function\\n\\nprocess central moment\\n\\npugachev canonical decomposition apparatus\\n\\nmix central moment\\n\\nrealization random depends\\n\\ninvestig process\\n\\nscalar random process\\n\\nnonlinear random depend\\n\\nthird order scalar random."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Result So ee Example\\n- Failure of RNN\\n\\n[Title] Meta-level Coordination for Solving Distributed Negotiation Chains in Semi-cooperative Multi-agent Systems\\n\\n[Abstract] A negotiation chain is formed when multiple related negotiations are spread over multiple agents.\\nIn order to appropriately order and structure the negotiations occurring in the chain so as to optimize the expected utility, we present an extension to a single-agent concurrent negotiation framework.\\nThis work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility; however, the performance of each individual agent is tightly related to other agents' cooperation and the system's overall performance.\\nWe introduce a pre-negotiation phase that allows agents to transfer meta-level information.\\nUsing this information, the agent can improve the accuracy of its local model about how other agents would react to the negotiations.\\n\\n[Ground-truth]\\n- ground-truth phrases\\n- multipl agent\\n- negoti framework\\n- negotichain\\n- semi cooper multi agent system\\n- pre negoti\\n- agent\\n- flexibl\\n\\n[Prediction]\\n- pre negoti phase\\n- Tf-ldf multi agent system\\n- INN IN multi agent system\\n- Copy RNN\\n- semi cooper multi agent system\\n- multi agent negoti chain\\n- system s overal perform\\n- multiag system\\n- multiag system negoti\\n- agent system\\n- concurr negot\\n- negoti chain\\n- multipl agent\\n- artifici intellig\\n- individu agent\\n- artifici intellig\\n- pre negoti\\n- other agent s cooper\\n- cooper multi agent system\\n- multi agent\\n- concurr negoti framework\\n- cooper multi agent\\n- semi cooper multi agent system\\n- cooper multi agent system\\n- multipl agent\\n- multip! relat\\n- negoti expect util\\n- negoti chain\\n- distribut artifici intellig\\n- meta level coordin\\n- global negot\\n- negoti solut\\n- meta level coord\\n- in global negoti chain\\n- context semi cooper\\n- iES"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Result\\n\\nExample\\n- Phrases with OOD words\\n\\n[Title] Full-screen ultrafast video modes over-clocked by simple VESA routines and registers reprogramming under MS-DOS.\\n\\n[Abstract] Fast full-screen presentation of stimuli is necessary in psychological research.\\nAlthough Spitczok von Brisinski (1994) introduced a method that achieved ultrafast display by reprogramming the registers, he could not produce an acceptable full-screen display.\\nIn this report, the author introduces a new method combining VESA routine calling with registers reprogramming that can yield a display at 640 x 480 resolution, with a refresh rate of about 150 Hz.\\n\\n[GROUND-TRUTH]\\n- vesa routine calling\\n- fast full screen stimuli presentation\\n- ms dos\\n- full screen ultrafast video modes\\n- psychological research\\n- register reprogramming\\n\\n[PREDICTION]\\n1. register reprogramming\\n2. video modes\\n3. ultrafast display\\n4. screen display\\n5. ultrafast video\\n6. vesa routine\\n7. refresh rate\\n8. routine calling\\n9. ultrafast video modes\\n10. psychological research\\n11. vesa routine calling\\n12. spitczok von\\n13. video modes over clocked\\n14. spitczok von brisinski\\n- Nearly 2% of all the correct predictions contain out-of-vocabulary words"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Result:\\n\\nTask 2\\n- Predict Absent Keyphrase =\\n- Same five test datasets, only use absent keyphrases as ground-truth\\n- Evaluate with recall@10 and recall@50\\n\\nRecall\\n\\n| inspec | 0.0309 0.0610 0.0471 0.0995 0.0945 0.1562 0.1128 0.2015 |\\n| mus    | 0.0498 0.0890 0.0578 0.1157 0.0414 0.0602 0.0427 0.0665 |\\n| vom    | 0.0833 0.1441 0.1253 0.2108"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Result\\nTask 2\\n- Predict Absent Keyphrase\\n\\n[Title] Towards content-based relevance ranking for video search\\n\\n[Abstract] Most existing web video search engines index videos by file names, URLs, and surrounding texts.\\nThese types of video metadata roughly describe the whole video at an abstract level without taking the rich content, such as semantic content descriptions and speech within the video, into consideration.\\nTherefore, the relevance ranking of the video search results is not satisfactory as the details of video contents are ignored.\\nIn this paper, we propose a novel relevance ranking approach for web-based video search using both video metadata and the rich content contained in the videos.\\nTo leverage real content into ranking, the videos are segmented into shots, which are smaller and more semantically meaningful retrievable units, and then more detailed information of video content such as semantic descriptions and speech of each shot is used to improve the retrieval and ranking performance.\\nWith video metadata and content information of shots, we developed an integrated ranking approach, which achieves improved ranking performance.\\nWe also introduce machine learning into the ranking system, and compare it with IR-model (information retrieval model) based methods.\\nThe evaluation results demonstrate the effectiveness of the proposed ranking methods.\\n\\n[Ground-truth]\\n10 absent phrases\\n- video\\n- ego\\n- IR model\\n- content based approach\\n- neural network based ranking\\n- learning based ranking\\n- IR model based ranking\\n- machine learning model\\n- video retrieval\\n\\n[Predictions]\\n2. web search\\n3. content ranking\\n4. content based retrieval\\n5. content retrieval\\n7. relevance feedback\\n8. video ranking\\n9. semantic web\\n10. content based video retrieval\\n11. web metadata\\n12. video analysis\\n13. speech recognition\\n14. content analysis\\n15. speech retrieval"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "ResultBe Task 3\\n- Transfer to News Articles\\n\\nSo far training and testing are only about scientific papers. What if we transfer it to a completely unseen domain?\\nDoes the model learn any universal features? Test the CopyRNN on DUC-2001.\\n- 308 news articles and 2,488 keyphrases.\\n- CopyRNN recalls 766 keyphrases.\\n- 14.3% contain out-of-vocabulary words.\\n- Many names of persons and places are correctly predicted.\\n\\nTFidf 0.270\\nTextRank 0.097\\nSingleRank 0.256\\nExpandRank 0.269\\nKeyCluster 0.140\\nCopyRNN@10 0.164"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Result\\n- Example\\n- Transfer to News Articles\\n- Anti-maoists threaten prosecutor.\\nA death squad opposed to the Shining Path guerrillas has threatened to kill a district attorney if he investigates charges that soldiers massacred dozens of peasants, his office said Tuesday.\\nPolice said members of Shining Path, a Maoist group, killed two policemen and wounded three in jungle raids.\\n- The Rodrigo Franco Command, which has vowed to kill a Shining Path member or sympathizer for every person slain by guerrillas, issued the threat against District Attorney Carlos Escobar on Monday, according to his office in Andean city of Ayacucho.\\nEscobar is investigating charges that troops rounded up dozens of peasants, accused them of being Shining Path members and killed them.\\nThe alleged massacre occurred in May near Cayara, a farming village 24 miles south of Ayacucho.\\n- Officials said the rebel raids occurred Sunday, at a police post and telephone relay station near the jungle city of Pucallpa, 350 miles northeast of Lima.\\nShining Path guerrillas began fighting eight years ago.\\nThe government says more than 69,000 people have been killed and puts the property damage at $26 billion.\\n- The Rodrigo Franco group is named for an official of the government party killed by Shining Path last year.\\nIt became known in July when it claimed responsibility for killing the lawyer for Osman Morote.\\nHe is suspected of being the Shining Path second in command and is in jail on terrorism charges.\\n\\n[Ground-truth]\\n- Shining Path guerrillas;\\n- police post;\\n- rebel raids;\\n- death squad;\\n- property damage;\\n- Rodrigo Franco Command;\\n- District Attorney Carlos Escobar;\\n- Osman Morote;\\n\\n[Predictions]\\n1. Shining Path\\n2. death squad\\n3. District Attorney\\n4. rebel raids\\n5. Osman Morote\\n6. jungle raids\\n7. Rodrigo Franco\\n8. terrorism charges\\n9. relay station\\n10. anti-maoists\\n11. massacred dozens\\n12. Andean city"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "ee ee ee Conclusion & Future Work\\n- Keyphrase generation study based on deep learning methods\\n- First work concerns absent keyphrase prediction\\n- RNN + Copy mechanism\\n- Able to learn cross-domain features\\n- Better model on capturing contextual information\\n- Multiple-output optimization\\n- Long documents, length & diversity penalties on output sequences"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "THANKS! Any question?"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 30,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Historical text normalization\\n\\nWhat is historical text normalization?\\n\\nAttention vs. multi-task learning\\n\\nMotivation\\n\\nSample of a manuscript from Early New High German\\n\\nMarcel Bollmann, Joachim Bingel, Anders Segaard\\n\\nLearning attention for historical normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Historical text normalization\\n\\nWhat is historical text normalization?\\n\\nPrevious work\\nA corpus of Early New High German: Medieval religious treatise \"Interrogatio Sancti Anselmi de Passione Domini\": 50 manuscripts and prints (in German) AB 7 $2 14 \"16 century.\\nVarious dialects.\\n\\nBavarian\\n\\nLow German\\n\\nSample from an Anselm manuscript: http://www.linguistics.rub.de/anselm/\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Historical text normalization\\n\\nWhat is historical text normalization?\\n\\nExamples for historical spellings\\n- Frau (woman): fraw, frawe, frauwe, fraliwe, frow, frouw, vraw, vrow, vorwe, vrauwe, vrouwe\\n- Kind (child): chind, chinde, chindt, chint, kind, kinde, kindi, kindt, kint, kinth, kynde, kynt\\n- Mutter (mother): moder, moeder, mueter, mUeter, muoter, muotter, muter, mutter, mvoter, mvter, mweter\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Historical text normalization\\n\\nWhat is historical text normalization?\\n- wuttAek\\n\\nPrevious work\\n\\nExamples for historical spellings\\n- Frau (woman)\\nfraw, frawe, frawe, frauwe, fraliwe, frow, frouw, vraw, vrow, vorwe, vrauwe, vrouwe\\n- Kind (child)\\nchind, chinde, chindt, chint, kind, kinde, kindi, kindt, kint, kinth, kynde, kynt\\n- Mutter (mother)\\nmoder, moeder, mueter, mUeter, muoter, muotter, muter, mutter, mvoter, mvter, mweter\\n\\nNormalization as the mapping of historical spellings to their modern-day equivalents.\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "ey dedi What is historical text normalization?\\n\\nPrevious work\\n- Hand-crafted algorithms\\n- VARD (Baron & Rayson,\\n2008)\\n- Norma (Bollmann,\\n2012)\\n- Character-based statistical machine translation (CSMT)\\n- Scherrer and Erjavec (2013), Pettersson et al. (2013),...\\n- Sequence labelling with neural networks\\n- Bollmann and Sogaard (2016)\\n\\nNow: \"Character-based neural machine translation\" Marcel Bollmann, Joachim Bingel, Anders Sogaard Learning attention for hist.\\nnormalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Historical text normalization\\n\\nModel description\\n\\nEncoder/decoder models\\n\\nAttention mechanism\\n\\nAttention vs. multi-task learning\\n\\nMulti-task learning\\n\\nAn encoder/decoder model\\n\\nPrediction layer\\n\\nDecoder\\n\\nLSTM | Embeddings\\n\\nLSTM = 7\\n- @ a\\n\\nEncoder\\n\\nEmbeddings\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "F1 Model description\\n\\nEncoder/decoder models\\n\\nAttention mechanism\\n\\nMulti-task learning\\n\\nAn encoder/decoder model\\n\\nAvg. Accuracy\\n\\nBi-LSTM tagger (Bollmann & Segaard,\\n2016) 79.91%\\n\\nGreedy 78.91%\\n\\nBase model Beam 79.27%\\n\\nBeam + Filter 80.46%\\n\\nEvaluation on 43 texts from the Anselm corpus (= 4,000-13,000 tokens each)\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Historical text normalization\\n\\nModel description\\n\\nEncoder/decoder models\\n\\nAttention mechanism\\n\\nAttention vs. multi-task learning\\n\\nMulti-task learning\\n\\nAttentional model\\n\\nk i n d= Prediction layer\\n\\nDecoder\\n\\nAttentional LSTM\\n\\nAttention model\\n\\nBidirectional LSTM\\n\\nEncoder\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "rispreoderidecodermovels (Mt imeemt t it Multi-task learning\\n\\nAttentional model\\n\\nAvg. Accuracy\\n- Bi-LSTM tagger (Bollmann & Segaard,\\n2016) 79.91%\\n- Greedy 78.91%\\n- Base model Beam 79.27%\\n- Beam + Filter 80.46%\\n- Beam + Filter + Attention 82.72%\\n\\nEvaluation on 43 texts from the Anselm corpus (= 4,000-13,000 tokens each)\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Historical text normalization\\n\\nModel description\\n\\nEncoder/decoder models\\n\\nAttention mechanism\\n\\nAttention vs. multi-task learning\\n\\nMulti-task learning\\n\\nLearning to pronounce\\n\\nCan we improve results with multi-task learning?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Historical text normalization\\n\\nModel description\\n\\nEncoder/decoder models\\n\\nAttention mechanism\\n\\nAttention vs. multi-task learning\\n\\nMulti-task learning\\n\\nLearning to pronounce\\n- Idea: grapheme-to-phoneme mapping as auxiliary task\\n- CELEX 2 lexical database (Baayen et al.,\\n1995)\\n- Sample mappings for German:\\n\\nJungfrau\\n- jUN-fraU\\n\\nAbend\\n- ab@nt\\n\\nnicht = nixt\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Historical text normalization\\n\\nModel description\\n\\nEncoder/decoder models\\n\\nAttention mechanism\\n\\nAttention vs. multi-task learning\\n\\nMulti-task learning\\n\\nPrediction layer for CELEX task\\n\\nDecoder LSTM\\n\\nEncoder LSTM\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "t 1 Model description\\n\\nEncoder/decoder models\\n\\nAttention mechanism\\n\\nMulti-task learning\\n\\nAvg. Accuracy\\n- Bi-LSTM tagger (Bollmann & Segaard,\\n2016) 79.91%\\n- Greedy 78.91%\\n- Base model\\n- Beam + Filter 80.46%\\n- Beam + Filter + Attention 82.72%\\n- Greedy 80.64%\\n- Beam 81.13%\\n- MTL model\\n- Beam Filter 82.76%\\n- Beam + Filter + Attention 82.02%\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard Learning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Attention vs. multi-task learning\\nWhy does MTL not improve with attention?\\n\\nHypothesis\\nAttention and MTL learn similar functions of the input data.\\n\"MTL can be used to coerce the learner to attend to patterns in the input it would otherwise ignore.\\nThis is done by forcing it to learn internal representations to support related tasks that depend on such patterns.\"\\n- Caruana (1998), p.112 f.\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "ee matizet Analysis Attention vs. multi-task learning oncision Comparing the model outputs gewarnet uberhiibe\\n- scholt G prandet Uberbroch sollt Base model B prandert Uberbrache sollt B+F pranget Uber sollt B+F+A gewarnt lbergebe sollte G gewarntet Uberbeh\\n- sollte B gewarntet Ubereube\\n- sollte MTL model B+F gewarnt ubergebe sollte B+F+A gewand Uber sollte Target gewarnt Uberhob\\n- sollte Marcel Bollmann, Joachim Bingel, Anders Sogaard Learning attention for hist.\\nnormalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Hist il text. lizatiercacteccr means | ABS \"Attention vs.\\nmulti-task learning Saliency plots Li, Chen, Hovy, and Jurafsky (2016) Base Attention MTL Bi 3 ra- for words > 7 characters, Attention/MTL correlate most a a = =) S= a0 Marcel Bollmann, Joachim Bingel, Anders Sagaard Learning attention for hist.\\nnormalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Hal io ae t oo Analysis\\n\\nAttention vs. multi-task learning\\n\\nConclusion\\n\\nEncoder/decoder models for historical text normalization are competitive.\\n\\nDespite small datasets (~ 4,200\\n- 13,200 tokens per text), beam search & attention improve results further.\\n\\nMTL with grapheme-to-phoneme task helps.\\n\\nAttention and MTL have a similar effect.\\n\\nCan this be reproduced on other tasks?\\n\\nWhat factors affect this (choice of attention mechanism/auxiliary task/...)?\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Historical text normalization\\nEncoder/decoder models\\nAttention vs. multi-task learning\\n\\nThank you for listening!\\nCode @ https://bitbucket.org/mbollmann/acl2017\\n\\nFurther Qs? MJ bollmann@linguistics.rub.de YW @mmbollmann\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\nLearning attention for historical normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "References\\n\\nBaayen, R. H., Piepenbrock, R., & Gulikers, L. (1995). The CELEX lexical database (Release\\n2) (CD-ROM). Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA.\\n\\nBaron, A., & Rayson, P. (2008). VARD 2: A tool for dealing with spelling variation in historical corpora.\\nIn Proceedings of the Postgraduate Conference in Corpus Linguistics.\\n\\nBollmann, M. (2012). (Semi-)automatic normalization of historical texts using distance measures and the Norma tool.\\nIn Proceedings of the Second Workshop on Annotation of Corpora for Research in the Humanities (ACRH-2).\\nLisbon, Portugal.\\n\\nBollmann, M., & Segaard, A. (2016).\\nImproving historical spelling normalization with bi-directional LSTMs and multi-task learning.\\nIn Proceedings of COLING 2016 (pp. 131-139). Osaka, Japan.\\n\\nCaruana, R. (1998). Multitask learning. In Learning to learn (pp. 95-133). Springer.\\nRetrieved from http://dl.acm.org/citation.cfm?id=296635. 296645\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard Learning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "References\\n\\nIlLi, J., Chen, X., Hovy, E., & Jurafsky, D. (2016). Visualizing and understanding neural models in NLP.\\nIn Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: Human language technologies (pp.\\n681-691). Association for Computational Linguistics.\\nRetrieved from http://aclweb.org/anthology/N16-1082 doi:10.18653/v1/N16-1082\\n\\nPettersson, E., Megyesi, B., & Tiedemann, J. (2013). An SMT approach to automatic annotation of historical text.\\nIn Proceedings of the nodalida workshop on computational historical linguistics. Oslo, Norway.\\n\\nScherrer, Y., & Erjavec, T. (2013). Modernizing historical Slovene words with character-based SMT.\\nIn Proceedings of the 4th biennial workshop on balto-slavic natural language processing. Sofia, Bulgaria.\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard Learning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "References\\n\\nIllXu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R.,... Bengio, Y. (2015).\\nShow, attend and tell: Neural image caption generation with visual attention.\\nIn Umlr workshop and conference proceedings: Proceedings of the 32nd international conference on machine learning (Vol.\\n37, pp. 2048-2057). Lille, France. Retrieved from http://proceedings.mlr.press/v37/xuc15.pdf\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard Learning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Disends\\n\\nDealing with spelling variation\\n\\nThe problems with normalization are difficult to annotate with existing tools.\\nIt removes variance and enables re-using data.\\nThere is high variance in spelling but provides a useful annotation layer with none or very little training (e.g.\\nfor corpus query).\\n\\nData normalization is the mapping of historical spellings to their modern-day equivalents.\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for historical normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Attention mechanism: details\\n\\nAttention mechanism follows Xu et al. (2015)\\n\\nZt = at \\* softmax(fatt(a, hy1)) (1)\\n\\nit = σ(Wi[he-1, yt-1, 2] + Di)\\nfe = σ(Wel[he-1, Ye-1, 2] + by)\\nOt = σ(Wol[ht-1, Ye-1, 2] + Bo)\\n\\ngt = tanh(Wo[ht-1, yt-1, 2] + bg)\\nCe = fr O Cri t + it\\ngtht = of tanh(ct)\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "References\\n\\nAppendix\\n\\nDifferences of learned parameters\\n\\n(a) Parameter changes for the attention model\\n\\n(b) Parameter changes for the multi-task model\\n\\nMarcel Bollmann, Joachim Bingel, Anders Sogaard\\n\\nLearning attention for hist. normalization by learning to pronounce"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 38,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Outline\\n- Time expression analysis\\n- Datasets: TimeBank, Gigaword, WikiWars, Tweets\\n- Findings: short expressions, occurrence, small vocabulary, similar syntactic behavior\\n- Time expression recognition\\n- SynTime: syntactic token types and general heuristic rules\\n- Baselines: HeidelTime, SUTime, UWTime"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Time Expression Analysis\\n- Datasets\\n\\nExample time expressions:\\n- TimeBank\\n- Hew\\n- Gigaword\\n- today\\n- WikiWars\\n- February Tweets\\n- the last week\\n- 1\\n- 13 January 1951\\n- Findings\\n- June 30, 1990\\n- Short time expressions\\n- 8 to 20 days\\n- Occurrence the third quarter of 1984\\n- Small vocabulary\\n- Similar syntactic behaviour"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Time Expression Analysis\\n- Datasets\\n- TimeBank: a benchmark dataset used in TempEval series\\n- Gigaword: a large dataset with generated labels and used in TempEval-3\\n- WikiWars: a specific domain dataset collected from Wikipedia about war\\n- Tweets: a manually labeled dataset with informal text collected from Twitter\\n\\nStatistics of the datasets\\n\\nDataset       #Docs   #Words    #TIMEX\\nTimeBank      183     61,418    1,243\\nGigaword      2,452   666,309   12,739\\nWikiWars      22      119,468   2,671\\nTweets        942     18,199    1,127\\n\\nThe four datasets vary in source, size, domain, and text type, but we will see that their time expressions demonstrate similar characteristics."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Time Expression Analysis\\n- Finding\\n- Short time expressions: time expressions are very short. 100\\n- 35... 20\\n- 80% of time expressions contain <3 words>\\n- 90% of time expressions contain <4 words>\\n\\nAverage length of time expressions\\n\\nDataset Average length\\n- TimeBank 2.00\\n- TimeBank\\n- 20\\n- Gigaword 1.70\\n- Tweet 6\\n- WikiWars 2.38\\n\\nNumber of words in time expressions\\n- Tweets 1.5\\n\\nTime expressions follow a similar length distribution\\nAverage length: about 2 words"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Time Expression Analysis\\n- Finding 2- Occurrence: most of time expressions contain time token(s).\\n\\nExample time tokens (red):\\n\\nPercentage of time expressions that contain time token(s) today\\n\\nDataset                 Percentage\\n\\nFridayTimeBank         94.61\\n\\nFebruaryGigaword       96.44\\n\\nthe last week          91.81\\n\\nWikiWars               13 January 1951\\n\\nTweets                 96.01\\n\\nJune 30, 1990         8 to 20 days\\n\\nthe third quarter of 1984"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Time Expression Analysis\\n- Finding 3\\n- Small vocabulary: only a small group of time words are used to express time information.\\n\\nNumber of distinct words and time tokens in time expressions\\n\\nDataset          #Words  #Time tokens\\nTimeBank        130     64\\nGigaword        214     80\\nWikiWars        224     74\\nTweets          107     64\\n\\nNumber of distinct words and time tokens across four datasets\\n\\nnext | year |\\n#Words           #Time tokens\\n2 am :          350     123\\n'year           1i ; ; 10 yrs 'ago\\n\\n45 distinct time tokens appear in all the four datasets.\\n\\nThat means, time expressions highly overlap at their time tokens.\\n\\nOverlap at year"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Time Expression Analysis\\n- Finding\\n4- Similar syntactic behaviour:\\n(1) POS information cannot distinguish time expressions from common text, but\\n(2) within time expressions, POS tags can help distinguish their constituents.\\n- (1) For the top 40 POS tags (10 X 4 datasets), 37 have percentage lower than 20%, other 3 are CD.\\n- (2) Time tokens mainly have NN and RB, modifiers have JJ and RB, and numerals have CD."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Time Expression Analysis\\n- Eureka!\\n- Similar syntactic behaviour: (1) POS information cannot distinguish time expressions from common text, but (2) within time expressions, POS tags can help distinguish their constituents.\\n- (1) For the top 40 POS tags (10 X 4 datasets), 37 have percentage lower than 20%, other 3 are CD.\\n- (2) Time tokens mainly have NN and RB, modifiers have JJ and RB, and numerals have CD.\\n\\nWhen seeing (2), we realize that this is exactly how linguists define part-of-speech for language; similar words have similar syntactic behaviour.\\nThe definition of part-of-speech for language inspires us to define a type system for the time expression, part of language.\\n\\nOur Eureka! moment"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Time Expression Analysis\\n- Summary\\n\\nOn average, a time expression contains two tokens; one is time token and the other is modifier/numeral.\\nAnd the time tokens are in small size.\\n- Idea for recognition\\n\\nTo recognize a time expression, we first recognize the time token, then recognize the modifier/numeral."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Time Expression Analysis\\n- Idea Summary\\nOn average, a time expression contains two tokens; one is a time token and the other is a modifier/numeral.\\nThe time tokens are in small size.\\n- Idea for recognition\\nTo recognize a time expression, we first recognize the time token, then recognize the modifier/numeral.\\n\\nTime token Modifier/Numeral\\n20 days; this week; next year; July 29; ..."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Time Expression Recognition\\n- SynTime\\n- Syntactic token types\\n- General heuristic rules\\n- Baseline methods\\n- HeidelTime\\n- SUTime\\n- UWTime\\n- Experiment datasets\\n- TimeBank\\n- WikiWars\\n- Tweets"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Time Expression Recognition\\n- SynTime\\nSyntactic token types\\nGeneral heuristic rules"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Time Expression Recognition\\n- SynTime\\n- Syntactic token types\\n- A type system\\n- Time token: explicitly express time information, e.g., \"year\"\\n- 15 token types: DECADE, YEAR, SEASON, MONTH, WEEK, DATE, TIME, DAY_TIME, TIMELINE, HOLIDAY, PERIOD, DURATION, TIME_UNIT, TIME_ZONE, ERA\\n- Modifier: modify time tokens, e.g., \"next\" modifies \"year\" in \"next year\"\\n- 5 token types: PREFIX, SUFFIX, LINKAGE, COMMA, IN_ARTICLE\\n- Numeral: ordinals and numbers, e.g., \"10\" in \"next 10 years\"\\n- 1 token type: NUMERAL\\n- Token types to tokens is like POS tags to words\\n- POS tags: next/JJ 10/CD years/NNS\\n- Token types: next/PREFIX 10/NUMERAL years/TIME_UNIT"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Time Expression Recognition\\n- SynTime\\n- General heuristic rules\\n- Only relevant to token types\\n- Independent of specific tokens"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "SynTime\\n- Layout General Heuristic Rules\\n\\nRule level: Y\\n\\nType level: Time Token, Modifier, Numeral\\n\\nToken level: time-related tokens and token regular expressions\\n\\nType level: token types group the tokens and token regular expressions\\n\\nRule level: heuristic rules work on token types and are independent of specific tokens"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Training Text POS-Tagged Text\\n\\nAdd keywords under defined token types and time token. Do not change any rules.\\n- Identify time tokens\\n- Time Token Modifier\\n- Identify modifiers and Numeral Time Segment\\n- Expand numerals by expanding the token regex to time tokens' boundaries\\n- Token, modifier, numerals\\n- Time Expression\\n- Token Regular Extract time expressions\\n- Expression\\n- Time Construction Annotated Text"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "An example: the third quarter of 1984\\nA sequence of tokens: the third quarter of 1984"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "An example: the third quarter of 1984\\n\\nAssign tokens with token types\\n\\nPREFIX\\nNUMERAL\\nTIME UNIT\\nPREFIX\\nYEAR\\n\\nA sequence of tokens: the third quarter of 1984"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "An example: the third quarter of 1984\\n\\nHeuristic Rules\\n\\nIdentify time tokens\\nAssign tokens with token types\\nPREFIX\\nNUMERAL\\nTIME_UNIT\\nPREFIX\\nYEAR\\n\\nA sequence of tokens: the third quarter of 1984"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "An example: the third quarter of 1984\\n\\nIdentify modifiers and numerals by searching time tokens' surroundings\\n\\nHeuristic Rules\\n\\nIdentify time tokens. Assign tokens with token types \"PREFIX\" \"NUMERAL\" \"TIME_UNIT\" \"PREFIX\" \"YEAR\"\\n\\nA sequence of tokens: the third quarter of 1984"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "An example: the third quarter of 1984\\nA sequence of token types PREFIX NUMERAL TIME _UNIT PREFIX YEAR"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "An example: the third quarter of 1984\\n\\nA sequence of token types\\n\\nPREFIX NUMERAL TIME UNIT PREFIX YEAR\\n\\nExport a sequence of tokens + ra sutnes. x 4 as time expression"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "An example: the third quarter of 1984\\nTime expression: the third quarter of 1984"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Time Expression Recognition\\n- Experiments\\n- SynTime\\n- SynTime-I: Initial version\\n- SynTime-E: Expanded version, adding keywords to SynTime-I (Add keywords under the defined token types and do not change any rules.)\\n\\nBaseline methods\\n- HeidelTime: rule-based method\\n- SUTime: rule-based method\\n- UWTime: learning-based method\\n\\nExperiment datasets\\n- TimeBank: comprehensive data in formal text\\n- WikiWars: specific domain data in formal text\\n- Tweets: comprehensive data in informal text"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Overall performance. The best results are in boldface and the second best are underlined.\\nSome results are borrowed from their original papers and the papers indicated by the references.\\n\\nStrict Match\\nRelexed Match\\nDataset\\nMethods\\nPr\\nRe.\\nF\\nPr\\nRe.\\nF\\n\\nHeidelTime (Strotgen et al.,\\n2013) | 83.85 78.99 81.34 | 93.08 87.68 90.30\\nSUTime (Chang and Manning,\\n2013) | 78.72 80.43 79.57 | 89.36 91.30 90.32\\n\\nTimeBank | UWTime (Lee et al.,\\n2014) | 86.10 80.40 83.10 | 94.60 88.40 91.40\\nSynTime-I | 91.43 92.75 92.09 | 94.29 95.65 94.96\\nSynTime-E | 91.49 93.48 92.47 | 93.62 95.65 94.62\\n\\nHeidelTime (Lee et al.,\\n2014) | 85.20 79.30 82.10 | 92.60 86.20 89.30\\nSUTime | 78.61 76.69 76.64 | 95.74 89.57 92.55\\n\\nWikiWars | UWTime (Lee et al.,\\n2014) | 87.70 78.80 83.00 | 97.60 87.60 92.30\\nSynTime-I | 80.00 80.22 80.11 | 92.16 92.41 92.29\\nSynTime-E | 79.18 83.47 81.27 | 90.49 95.39 92.88\\n\\nHeidelTime | 89.58 72.88 80.37 | 95.83 77.97 85.98\\nSUTime | 76.03 77.97 76.99 | 88.43 90.68 89.54\\n\\nTweets | UWTime | 88.54 72.03 79.44 | 96.88 78.81 86.92\\nSynTime-I | 89.52 94.07 91.74 | 93.55 98.31 95.87\\nSynTime-E | 89.20 94.49 91.77 | 93.20 98.78 95.88"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Difference from other Rule-based Methods\\n\\nMethod | SynTime\\n\\nOther rule-based methods\\n\\nRule level | General Heuristic Rules\\n\\nIES | Token level\\n\\nDeterministic Rules | Token level\\n\\nLayout | Type level\\n\\nTime Token, Modifier, Numeral\\n\\nToken level\\n\\nToken level\\n\\n1989, February, 12:55, this year, 3 months ago, ...\\n\\nHeuristic rules work on token types and are independent of specific tokens, thus they are independent of specific domains and specific text types and specific languages.\\n\\nDeterministic Rules directly work on tokens and phrases in a fixed manner, thus the taggers lack flexibility.\\n\\nExample |\\n\\n/the/? [{tag:JJ}]? (SNUM_ORD) /-/? [{tag:JJ}]? /quarter/<PREFIX NUMERAL TIME_UNIT PREFIX YEAR\\n\\nthe third quarter of 1984!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "A simple idea\\n\\nRules can be designed with generality and heuristics\\n\\nHeuristic Rules\\n\\nPREFIX NUMERAL TIME_UNIT PREFIX YEAR for the third quarter of 1984"
        ]
      }
    ]
  },
  {
    "conf": "acl17",
    "idd": 226,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Introduction\\n\\nAttention over multiple source sequences relatively unexplored. This work proposes two techniques:\\n- Flat attention combination\\n- Hierarchical attention combination\\n\\nApplied to tasks of multimodal translation and automatic post-editing.\\n\\nMotivation\\n\\nNo universal method that models explicitly the importance of each input."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Multi-Source Sequence-to-Sequence Learning\\n\\nAny number of input sequences with possibly different modalities.\\n\\n'N/A\\n\\nFigure 1: Multimodal translation example.\\n\\nExamples\\n\\nMultimodal translation, automatic post-editing, multi-source machine translation, ..."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Attentive Sequence Learning\\n\\nIn each decoder step, ej = vi tanh(W, S; + Ush) (1) = compute distribution over encoder states given the decoder state Oj > Te (2)\\n\\nDeeks EXP(E:K) = the decoder gets a context vector to decide about its output G = Sooh (3)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Attentive Sequence Learning\\n\\nIn each decoder step,\\n\\ney = vy tanh(W,5;+ Ushi) (1) = compute distribution over encoder states given the decoder state\\n\\nw = C2)\\n\\nDenes exP(E:k) = the decoder gets a context vector to decide about its output\\n\\nG = 3 ajjh (3)\\n\\nysj = l What about multiple inputs?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Context Vector Concatenation | WU Y Ana\\n\\nWidely used technique [Firat et al., 2016; Zoph and Knight, 2016].\\n\\nAttention over input sequences computed independently.\\n\\nCombination resolved later on in the network"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Flat Attention Combination\\n\\nImportance of different inputs reflected in the joint attention distribution."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Flat Attention Combination one source -> / sourcesej = ve tanh(W,5; + Ushj) > ej = vi tanh(W,5; + U.\" hj) exp(e;) exp(e,) = Sh ep(e) = aN a (amyk=1 ©XP ik, et XP (e?) Tie v 7?q = > agh; > G = Le > oti Uc hyj = l j = 1 » Us uw) project states to a common space = Question: Should uw = ui?\\n(ie. should the projection parameters be shared?)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Hierarchical Attention Combination\\n\\nAttention distribution is factored by input."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Hierarchical Attention Combination\\n1. Compute the context vector:\\nV(k) = T(Y)(Op(k)kel)df = Viet(a; h), where ap = japiits ... using the vanilla attention.\\n2. Compute another attention e = vy, tanh(Wps; + U,'c;\")ea a 5 3 k distribution over the intermediate wo.\\nexp(el) context vectors c;'\" and get the B; = oN ex (2) resulting context vector c(j).\\ny PLE(k) = CkGe = euk = 1 = As in the flat scenario, the context vectors have to be projected to a shared space.\\nSame question arises\\n- should uo = Ui?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Experiments and Results\\n\\nExperiments conducted on multimodal translation (MMT) and automatic post-editing (APE).\\nIn both flat and hierarchical scenarios, we tried both sharing and not sharing the projection matrices.\\nAdditionally, we tried using the sentinel gate [Lu et al., 2016], which enables the decoder to decide whether or not to attend to any encoder.\\n\\nExperiments conducted using Neural Monkey, code available here: https://github.com/ufal/neuralmonkey."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Experiments and Results\\n\\nMMT APE5\\n- BLEU: 3142.8\\n- METEOR: 4802.7\\n- HTER: 6235.2\\n- concat: 244.4\\n- x: x\\n- 30.24.8\\n- 4654.7\\n- 6264.5\\n- 2424.4\\n- x: v\\n- 2934.8\\n- 4544.7\\n- 6234.5\\n- 2434.4\\n- =: v)x\\n- 3094.8\\n- 4714.7\\n- 6244.6\\n- 2444.4\\n- v: v\\n- 29.44.8\\n- 46.94.7\\n- 6254.6\\n- 2424.4\\n- x: x\\n- 32.14.8\\n- 49.14.7\\n- 6234.5\\n- 2414.4\\n- x: v\\n- 2814.8\\n- 4554.7\\n- 6264.6\\n- 2414.4\\n- a: v\\n- x: x\\n- 26.14.7\\n- 42.44.7\\n- 6244.5\\n- 2434.4\\n- Ziv: v\\n- v: v\\n- 2204.7\\n- 3854.6\\n- 6254.5\\n- 2414.4\\n\\nResults on the Multi30k dataset and the APE dataset.\\nThe column 'share' denotes whether the projection matrix is shared for energies and context vector computation, 'sent.' indicates whether the sentinel vector has been used or not."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Example Source: Output with attention: A man sleeping in a green room on a couch.\\nein Mann schläft in einem grünen Raum auf einem Sofa."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Conclusions: The results show both methods achieve comparable results to the existing approach (concatenation of the context vectors).\\n\\nHierarchical attention combination achieved best results on MMT and is faster to train.\\n\\nBoth methods provide a trivial way to inspect the attention distribution w.r.t. the individual inputs.\\n\\nThank you for your attention!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "References\\n- Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.\\n2016. Multi-way, multilingual neural machine translation with a shared attention mechanism.\\nIn Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\\nAssociation for Computational Linguistics, San Diego, CA, USA, pages 866-875. http://www.aclweb.org/anthology/N16-1101.\\n- Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher.\\n2016. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. CoRR abs/1612.01887.\\nhttp://arxiv.org/abs/1612.01887.\\n- Barret Zoph and Kevin Knight.\\n2016. Multi-source neural translation.\\nIn Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\\nAssociation for Computational Linguistics, San Diego, CA, USA, pages 30-34. http://www.aclweb.org/anthology/N16-1004."
        ]
      }
    ]
  }
]
