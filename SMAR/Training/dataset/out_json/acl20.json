[
  {
    "conf": "acl20",
    "idd": 629,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Semantic Dependency Parsing"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Semantic Dependency Parsing\\nDirected Acyclic Graph (DAG)\\nROOT\\nARG2\\nARG1\\ni\\nARG1\\na\\n/\\n=\\n~\\nPN\\nNW\\n\\nThe results were in line with analysts' expectations."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Semantic Dependency Parsing\\nDirected Acyclic Graph (DAG)\\nROOT\\nBV7TM\\n\\nThe results were in line with analysts' expectations.\\nle eepredicate argument (head) (dependent)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Semantic Dependency Parsing\\nDirected Acyclic Graph (DAG)\\nROOT\\nFe ARG17~foTMN\\nThe results were in line with analysts' expectations."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Semantic Dependency Parsing\\nDirected Acyclic Graph (DAG)\\nROOT\\nARG2\\nARG1\\nARG1\\n\\nThe results were in line with analysts' expectations.\\nDELPH-IN MRS (DM)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Semantic Dependency Parsing\\n\\nDirected Acyclic Graph (DAG)\\n\\nROOT\\nARG2\\nARG1\\nBV \"ARG2\\nPxlo\\nTMN t~TM\\n\\nThe results were in line with analysts' expectations.\\n\\nDELPH-IN MRS (DM)\\n\\nOther semantic dependency formalisms:\\n- Predicate-Argument Structure (PAS)\\n- Prague Semantic Dependencies (PSD)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Semantic Dependency Parsing\\n- Graph-based parsers:\\n\\n* Score each possible arc (or set of arcs) and search for the highest-scoring graph.\\n- Current SOTA approaches: (Dozat and Manning, 2018), (Wang et al., 2019), (He and Choi, 2019)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Semantic Dependency Parsing\\n- Graph-based parsers:\\n* Score each possible arc (or set of arcs) and search for the highest-scoring graph.\\n* Current SOTA approaches: (Dozat and Manning, 2018), (Wang et al., 2019), (He and Choi, 2019).\\n- Transition-based parsers:\\n* Incrementally produce a graph by applying a sequence of transitions, locally and greedily chosen.\\n* Lack of global inference that favours error propagation, being less appealing for SDP.\\n* There are fewer recent transition-based SDP contributions (Wang et al.,\\n2018) and they are less accurate than graph-based models."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Our approach\\n- Transition System: Multi-head Left-to-Right\\n- Inspired by (Daniel Fernandez-Gonzalez and Carlos Gómez-Rodriguez, 2019)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Our approach\\n- Transition System: Multi-head Left-to-Right\\n- Inspired by (Daniel Fernandez-Gonzalez and Carlos Gómez-Rodriguez,\\n2019)\\n- Data structure: Focus word pointer i\\n- Transitions: Shift and Attach\\n- pW, W2, W3 ... Wy Shift > 4 i = 2"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Our approach\\n- Transition System: Multi-head Left-to-Right\\n- Inspired by (Daniel Fernandez-Gonzalez and Carlos Gómez-Rodriguez,\\n2019)\\n- Data structure: Focus word pointer i\\n- Transitions: Shift and Attach\\n- pAttach\\n- 3Wi Wh Ys a Wht = 2"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Our approach\\n- Transition System: Multi-head Left-to-Right\\n- Inspired by (Daniel Fernandez-Gonzalez and Carlos Gómez-Rodriguez,\\n2019)\\n- Data structure: Focus word pointer i\\n- Transitions: Shift and Attach\\n- p\\n- Classifier: Pointer Network (Vinyals et al.,\\n2015)\\n- Encoder-decoder neural architecture that uses attention as a pointer to output position p from the input sequence.\\n- If p_i, then create an arc with Attach-p (as long as it satisfies the acyclicity constraint).\\n- If p = i, then a Shift is applied.\\n- Arc labeling is performed by a jointly-trained biaffine classifier (Dozat and Manning, 2017)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Neural Network Architecture"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Neural Network Architecture ROOT, The, results, were; ing .. -yW;"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Neural Network Architecture ROOT, The, results, were; ing ...\\n- pre-trained word embeddings"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were ing -nwy, POS tag embeddings"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were ing..\\n- randomly-initialized character embeddings (CNN)\\n- basic embeddings"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were; using pre-trained lemma embeddings\\n- basic embeddings"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were pre-trained deep contextualized word embeddings (fixed) basic embeddings char + lemma augmentation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were in... BiLSTM Encoder"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were in...\\n\\nLSTM Decoder\\n\\nLSTM Encoder"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were in...\\n- y = score(h,\\n5)\\n- a = softmax(v)\\n\\nLSTM Decoder\\n\\nBiLSTM Encoder"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were;\\n\\ny = score(hj,\\n5)\\na = softmax(v)\\n1 = max(a)\\n\\nLSTM Decoders\\n\\ndamian\\n\\nahyBiLSTM Encoder"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were:\\n- yt = 4\\n- y = score(hj,\\n5)\\n- a = softmax(v)\\n- 1 = max(a)\\n\\nLSTM Decoder\\n- Debtors\\n- BiLSTM Encoder\\n\\nTransition sequence: Shift"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were in... BiLSTM Encoder\\n\\nTransition sequence: Shift"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were ing ...\\n\\ny = score(hj,\\n5)\\na = softmax(v)\\n\\nLSTM Decoder\\n\\nThe results were in...\\n\\nBiLSTM Encoder\\n\\nTransition sequence: Shift"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          "Neural Network Architecture ROOT, the results were in... a BiLSTM Encoder Transition sequence: Shift Attach-1"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 30,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were; score(hj,\\n51) = softmax(v) = max(a)\\n\\nLSTM Decoder\\n\\nThe results were in...\\n\\nBiLSTM Encoder\\n\\nTransition sequence: Shift Attach-1"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 31,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were: y = score(hj,\\n5) a = softmax(v) 2 = max(a)\\n\\nLSTM Decoder\\n\\nBiLSTM Encoder\\n\\nTransition sequence:\\n- Shift\\n- Attach-1\\n- Attach-4"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 32,
        "texts": [
          "Neural Network Architecture\\n\\nThe results were:\\n- nHT LSTM Decoder\\n- BiLSTM Encoder\\n- Transition sequence: Shift Attach-1 Attach-4 Shift"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 33,
        "texts": [
          "Experiments\\n- We test our approach on the SemEval 2015 Task 18 English datasets:\\n- All sentences are annotated with three formalisms: DM, PAS and PSD.\\n- Labelled F-measure scores (LF1) are reported on in-domain (ID) and out-of-domain (OOD) test sets as well as the macro-average over the three formalisms."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 34,
        "texts": [
          "Experiments DM PAS PSD Avg Parser ID OOD ID OOD ID OOD ID OOD\\n\\nDu et al. (2015) tocb+ens 89.1 81.8 91.3 87.2 75.7 73.3 85.3 80.8\\n\\nAlmeida and Martins (2015) co 88.2 81.8 90.9 86.9 76.4 74.8 85.2 81.2\\n\\nPeng et al. (2017) cp 89.4 84.5 92.2 88.3 77.6 75.3 86.4 82.7\\n\\nPeng et al. (2017) cbsmr 90.4 85.3 92.7 89.0 78.5 76.4 87.2 83.6\\n\\nWang et al. (2018) 89.3 83.2 91.4 87.2 76.1 73.2 85.6 81.2\\n\\nWang et al. (2018) To+Ens 90.3 84.9 91.7 87.6 78.6 75.9 86.9 82.8\\n\\nDozat and Manning (2018) cb 91.4 86.9 93.9 90.8 79.1 77.5 88.1 85.0\\n\\nKurita and Sogaard (2019) cp 91.1\\n- 92.4\\n- 78.6\\n- 87.4\\n-\\n\\nKurita and Sogaard (2019) cbsmt+AL 91.2\\n- 92.9\\n- 78.8\\n- 87.6\\n-\\n\\nWang et al. (2019) co 93.0 88.4 94.3 91.5 80.9 78.9 89.4 86.3\\n\\nThis work Tb 92.5 87.7 94.2 91.0 81.0 78.7 89.2 85.8\\n\\nTb = transition-based, Gb = graph-based, Ens = Ensemble, MT = Multitask, RL = Reinforcement learning"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 35,
        "texts": [
          "Experiments DM PAS PSD Avg Parser ID OOD ID OOD ID OOD IW _ OOD Du et al.\\n(2015) tocb + ens 89.1 81.8 91.3 87.2 75.7 73.3 85.3 80.8\\n\\nAlmeida and Martins (2015) co 88.2 81.8 90.9 86.9 76.4 74.8 85.2 81.2\\n\\nPeng et al. (2017) cp 89.4 84.5 92.2 88.3 77.6 75.3 86.4 82.7\\n\\nPeng et al. 0 biM 90.4 89.0 86.4 83.6\\n\\nWang et al. (2018) 89.3 83.2 91.4 87.2 76.1 73.2 85.6 81.2\\n\\nWang et al. (2018) To + Ens 90.3 84.9 91.7 87.6 78.6 75.9 86.9 82.8\\n\\nDozat and Manning (2078) co 14 BI BI US\\n70. > 88.1 85.0\\n\\nKurita and Sogaard (2019) cp 91.1\\n- 92.4\\n- 78.6\\n- 87.4\\n-\\n\\nKurita and Sogaard (2019) cbsmt + AL 91.2\\n- 92.9\\n- 78.8\\n- 87.6\\n-\\n\\nWang et al. (2019) cp 93.0 88.4 94.9 80.9 89.4 86.0\\n\\nThis work to 92.5 87.7 94.2 91.0 81.0 78.7 89.2 85.8\\n\\nTb = transition-based, Gb = graph-based, Ens = Ensemble, MT = Multitask, RL = Reinforcement learning"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 36,
        "texts": [
          "Experiments DM PAS PSD Avg\\n\\nParser ID OOD ID OOD ID OOD IW\\n- oOODDu et al. (2015) tocb+ens 89.1 81.8 91.3 87.2 75.7 73.3 85.3 80.8\\n- Almeida and Martins (2015) co 88.2 81.8 90.9 86.9 76.4 74.8 85.2 81.2\\n- Peng et al. (2017) cp 89.4 84.5 92.2 88.3 77.6 75.3 86.4 82.7\\n- Peng et al. (2017) cbsmr 90.4 85.3 92.7 89.0 78.5 76.4 87.2 83.6\\n- Wang et al. (2018) 1» 89.3 83.2 91.4 87.2 76.1 73.2 85.6 81.2\\n- Nang-e\\n0418) -n 90 84.9 9 87.6 78.6 9 36.9 32.8\\n- Dozat and Manning (2018) cb 91.4 86.9 93.9 90.8 79.1 77.5 88.1 85.0\\n- Kurita and Sagaard (2019) a OT. = 92.4 > 8.6 > 87.4 =\\n- Kurita and Sogaard (2019) cb+mt+rL 91.2\\n- 92.9\\n- 78.8\\n- 87.6\\n-\\n- Wang et al. 019 b 93.0 88.4 94.9 80.9 8.9 89.4 86\\n- This work 1» 92.5 87.7 94.2 91.0 81.0 78.7 89.2 85.8\\n\\nTb = transition-based, Gb = graph-based, Ens = Ensemble, MT = Multitask, RL = Reinforcement learning"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 38,
        "texts": [
          "Experiments DM PAS PSD Avg Parser ID OOD ID OOD ID OOD IW\\n- oOOD Du et al. (2015) tocb: ens 89.1 81.8 91.3 87.2 75.7 73.3 85.3 80.8\\n- Almeida and Martins (2015) cp 88.2 81.8 90.9 86.9 76.4 74.8 85.2 81.2\\n- Peng et al. (2017) cp 89.4 84.5 92.2 88.3 77.6 75.3 86.4 82.7\\n- Peng et al. (2017) cbsmr 90.4 85.3 92.7 89.0 78.5 76.4 87.2 83.6\\n- Wang et al. (2018) 1» 89.3 83.2 91.4 87.2 76.1 73.2 85.6 81.2\\n- Wang et al. (2018) to + Ens 90.3 84.9 91.7 87.6 78.6 75.9 86.9 82.8\\n- Dozat and Manning (2018) cv 91.4 86.9 93.9 90.8 79.1 77.5 88.1 85.0\\n- Kurita and Sogaard (2019) cb 91.1 92.4 78.6 87.4\\n- Kurita and Sogaard (2019) cbsmt + AL 91.2 92.9 78.8 87.6\\n- Wang et al. (2019) co 93.0 88.4 94.3 91.5 80.9 78.9 89.4 86.3\\n- This work to 92.5 87.7 94.2 91.0 81.0 78.7 89.2 85.8\\n- Dozat and Manning (2018) Goscharsiemma 93.7 88.9 93.9 90.6 81.0 79.4 89.5 86.3\\n- Kurita and Sogaard (2019) cb + wt + Al + lemma 92.0 87.2 92.8 88.8 79.3 77.7 88.0 84.6\\n- Wang et al. (2019) cb + charslemma 94.0 89.7 94.1 91.3 81.4 79.6 89.8 86.9\\n- This work Toschar + lemma 93.9 89.6 94.2 91.2 81.8 79.8 90.0 86.9\\n\\nTb = transition-based, Gb = graph-based, Ens = Ensemble, MT = Multitask, RL = Reinforcement learning"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 39,
        "texts": [
          "Experiments DM PAS PSD Avg Parser ID OOD ID OOD ID OOD IW\\n- oOODDu et al. (2015) tocb+ens 89.1 81.8 91.3 87.2 75.7 73.3 85.3 80.8\\n- Almeida and Martins (2015) cp 88.2 81.8 90.9 86.9 76.4 74.8 85.2 81.2\\n- Peng et al. (2017) cb 89.4 84.5 92.2 88.3 77.6 75.3 86.4 82.7\\n- Peng et al. (2017) cbswt 90.4 85.3 92.7 89.0 78.5 76.4 87.2 83.6\\n- Wang et al. (2018) 89.3 83.2 91.4 87.2 76.1 73.2 85.6 81.2\\n- Wang et al. (2018) to+ens 90.3 84.9 91.7 87.6 78.6 75.9 86.9 82.8\\n- Dozat and Manning (2018) cv 91.4 86.9 93.9 90.8 79.1 77.5 88.1 85.0\\n- Kurita and Sogaard (2019) cp 91.1\\n- 92.4\\n- 78.6\\n- 87.4\\n-\\n- Kurita and Sogaard (2019) cb+mt+AL 91.2\\n- 92.9\\n- 78.8\\n- 87.6\\n-\\n- Wang et al. (2019) cb 93.0 88.4 94.3 91.5 80.9 78.9 89.4 86.3\\n- This work 1 92.5 87.7 94.2 91.0 81.0 78.7 89.2 85.8\\n- Dozat and Manning (2018) cbscharstemma 93.7 88.9 93.9 90.6 81.0 79.4 89.5 86.3\\n- Kurita and Sagaard (2019) chmrseistemma 92.0 8 92.8 88.8 79.0 88.0 84.6\\n- Wang et al. (2019) Gbs+charslemma 94.0 89.7 94.1 91.3 81.4 79.6 89.8 86.9\\n- This work Toschar+lemma 93.9 89.6 94.2 91.2 81.8 79.8 90.0 86.9\\n\\nTb = transition-based, Gb = graph-based, Ens = Ensemble, MT = Multitask, RL = Reinforcement learning"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 40,
        "texts": [
          "Experiments DM PAS PSD Avg Parser ID OOD ID OOD ID OOD IW\\n- oOODDu et al. (2015) tocb-ens 89.1 81.8 91.3 87.2 75.7 73.3 85.3 80.8\\n- Almeida and Martins (2015) cp 88.2 81.8 90.9 86.9 76.4 74.8 85.2 81.2\\n- Peng et al. (2017) cb 89.4 84.5 92.2 88.3 77.6 75.3 86.4 82.7\\n- Peng et al. (2017) cbsmt 90.4 85.3 92.7 89.0 78.5 76.4 87.2 83.6\\n- Wang et al. (2018) 1» 89.3 83.2 91.4 87.2 76.1 73.2 85.6 81.2\\n- Wang et al. (2018) to+Ens 90.3 84.9 91.7 87.6 78.6 75.9 86.9 82.8\\n- Dozat and Manning (2018) cv 91.4 86.9 93.9 90.8 79.1 77.5 88.1 85.0\\n- Kurita and Sogaard (2019) co 91.1 * 92.4\\n- 78.6 : 87.4 2\\n- Kurita and Sogaard (2019) cbsmr+AL 91.2 = 92.9\\n- 78.8 = 87.6\\n-\\n- Wang et al. (2019) co 93.0 88.4 94.3 91.5 80.9 78.9 89.4 86.3\\n- This work to 92.5 87.7 94.2 91.0 81.0 78.7 89.2 85.8\\n- Dozat and Manning (2018) Goscharsiemma 93.7 88.9 93.9 90.6 81.0 79.4 89.5 86.3\\n- Kurita and Sogaard (2019) cb+mt+AL+lemma 92.0 87.2 92.8 88.8 79.3 77.7 88.0 84.6\\n- Wang et al. (2019) cbscharstemma 94.0 89.7 94.1 91.3 81.4 79.6 89.8 86.9\\n- This work Toscharslemma 93.9 89.6 94.2 91.2 81.8 79.8 90.0 86.9\\n- Zhang et al. (2019) Toschar+BERT 92.2 87.1\\n-\\n-\\n-\\n-\\n-\\n-\\n- He and Choi (2019) cosiemmarFiairsBERTxs: 94.6 90.8 96.1 94.4 86.8 79.5 92.5 88.2\\n- This work To+charsiemma+BERTsise 94.4 91.0 95.1 93.4 82.6 82.0 90.7 88.8\\n\\nTb = transition-based, Gb = graph-based, Ens = Ensemble, MT = Multitask, RL = Reinforcement learning"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 41,
        "texts": [
          "Experiments DM PAS PSD Avg\\n\\nParser ID OOD ID OOD ID OOD IW _ OOD\\n\\nDu et al. (2015) tocb:ens 89.1 81.8 91.3 87.2 75.7 73.3 85.3 80.8\\n\\nAlmeida and Martins (2015) cp 88.2 81.8 90.9 86.9 76.4 74.8 85.2 81.2\\n\\nPeng et al. (2017) cp 89.4 84.5 92.2 88.3 77.6 75.3 86.4 82.7\\n\\nPeng et al. (2017) cbsmt 90.4 85.3 92.7 89.0 78.5 76.4 87.2 83.6\\n\\nWang et al. (2018) 1» 89.3 83.2 91.4 87.2 76.1 73.2 85.6 81.2\\n\\nWang et al. (2018) to+Ens 90.3 84.9 91.7 87.6 78.6 75.9 86.9 82.8\\n\\nDozat and Manning (2018) cv 91.4 86.9 93.9 90.8 79.1 77.5 88.1 85.0\\n\\nKurita and Sogaard (2019) cv 91.1 * 92.4 . 78.6 : 87.4 :\\n\\nKurita and Sogaard (2019) cb+mr+AL 91.2 = 92.9\\n- 78.8 = 87.6\\n-\\n\\nWang et al. (2019) cp 93.0 88.4 94.3 91.5 80.9 78.9 89.4 86.3\\n\\nThis work 92.5 87.7 94.2 91.0 81.0 78.7 89.2 85.8\\n\\nDozat and Manning (2018) coscharsiemma 93.7 88.9 93.9 90.6 81.0 79.4 89.5 86.3\\n\\nKurita and Sogaard (2019) cb+mt+Al+lemma 92.0 87.2 92.8 88.8 79.3 77.7 88.0 84.6\\n\\nWang et al. (2019) cbscharstemma 94.0 89.7 94.1 91.3 81.4 79.6 89.8 86.9\\n\\nThis work thscharsiemm 93.9 89.6 94.9 81.8 9.8 90.0 86.9\\n\\nZhang et al. (2019) Toschar+BERT: nce 92.2 87.1\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nHe and Choi (2019) co+iemmarFiairsBERTss: 94.6 90.8 96.1 94.4 86.8 79.5 92.5 88.2\\n\\nThis work To+charsiemma+BERTsise 94.4 91.0 95.1 93.4 82.6 82.0 90.7 88.8\\n\\nTb = transition-based, Gb = graph-based, Ens = Ensemble, MT = Multitask, RL = Reinforcement learning"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 42,
        "texts": [
          "Complexity\\n- Given a sentence with n words whose graph has m arcs, our approach will require n Shift transitions plus k Attach-p transitions per word (n * k = m)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 43,
        "texts": [
          "Complexity\\nGiven a sentence with m words whose graph has m arcs, our approach will require n Shift transitions plus k Attach-p transitions per word (n * k = m).\\n\\nIn the worst case (k = n), it could need O(n²) transitions.\\nHowever, in practice, sentences can be parsed in O() transitions.\\nDM PASic ire"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 44,
        "texts": [
          "Complexity\\n- Given a sentence with n words whose graph has m arcs, our approach will require n Shift transitions plus k Attach-p transitions per word (n * k = m).\\n\\nIn the worst case (k = n), it could need O(n²) transitions.\\nHowever, in practice, sentences can be parsed in O(n) transitions.\\n\\nTherefore, since attention computation takes O(n) time to run at each transition, the overall running time complexity is O(n²) (without cycle detection) and O(n log\\nn) (with cycle detection)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 45,
        "texts": [
          "Complexity\\n\\nGiven a sentence with n words whose graph has m arcs, our approach will require n Shift transitions plus k Attach-p transitions per word (n * k = m).\\n\\nIn the worst case (k=n), it could need O(n') transitions.\\nHowever, in practice, sentences can be parsed in O(n) transitions.\\n\\nTherefore, since attention computation takes O(n) time to run at each transition, the overall running time complexity is O(n\") (without cycle detection) and O(n log\\nn) (with cycle detection).\\n\\nMore efficient than the second-order graph-based parser by Wang et al. (2019) (O(n) without cycle detection)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 46,
        "texts": [
          "Takeaways\\n- The proposed multi-head transition system can produce any DAG in quadratic worst-case runtime."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 47,
        "texts": [
          "Takeaways\\n- The proposed multi-head transition system can produce any DAG in quadratic worst-case runtime.\\n- Our transition-based approach outperforms the graph-based parser by Dozat and Manning (2018) and, while being more efficient, matches the accuracy of the SOTA model by Wang et al.\\n(2019)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 48,
        "texts": [
          "Takeaways\\n- The proposed multi-head transition system can produce any DAG in quadratic worst-case runtime.\\n- Our transition-based approach outperforms the graph-based parser by Dozat and Manning (2018) and, while being more efficient, matches the accuracy of the SOTA model by Wang et al.\\n(2019).\\n- Accuracy could be easily boosted by applying multitask learning across the three formalisms (Peng et al.,\\n2017) or by just fine-tuning BERT-based embeddings."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 49,
        "texts": [
          "Transition-based Semantic Dependency Parsing with Pointer Networks\\n\\nDaniel Fernandez-Gonzalez\\nd.fgonzalez@udc.es\\n\\nCarlos Gomez-Rodriguez\\ncarlos.gomez@udc.es"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 290,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Introduction\\n\\nWe present an unsupervised model for aspect extraction, a subtask of aspect based sentiment analysis.\\nThe model is simple, requires word embeddings and a set of predefined aspect terms.\\nWe use a new kind of attention mechanism to quantify the similarity between sentences and aspects."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Introduction\\n\\nAspect-based sentiment analysis is the task of extracting sentiment, i.e., whether a given opinion is positive or negative, on specific aspects of an entity."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Introduction\\n\\nAspect-based sentiment analysis is the task of extracting sentiment, i.e., whether a given opinion is positive or negative, on specific aspects of an entity.\\n\\nExample: \"I really enjoyed the food, but the service was really bad. Great atmosphere!\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Previous work\\n- ABAE (He et al., 2017): Auto-encoders with attention mechanism\\n- AE-CSA (Luo et al., 2019): Like ABAE, but with sememe information\\n\\nUnsupervised deep neural nets fitted on some corpus.\\nAdditionally, require users to manually link discovered aspects to labels.\\nOur model automatically links labels to aspects."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "CAt, Contrastive Attention (CAt%):\\n\\nS: sentence\\n\\nA: aspect\\n\\nGiven a sentence (S) for which we would like to discover the aspect and a set of aspect terms (A)\\n1. Create an attention-weighted average using the similarity between words and aspects.\\n\\natt: attention vector\\n2. Assign the sentence the label of the closest food staff ambience"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "CAt®, The attention is calculated as follows:\\n\\natt = wed rbf(w, a,\\n7)\\n\\nThe attention is thus the sum of all RBF responses to a word, divided by the sum of all RBF responses to all words."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Datasets\\n\\nWe use three datasets of restaurant reviews:\\n1. Citysearch (Ganu et al.,\\n2009)\\n2. SemEval 2014 (Pontiki et al.,\\n2014)\\n3. SemEval 2015 (Pontiki et al.,\\n2015)\\n\\nTo compare to previous work, we only use sentences that express a single aspect, and only use three out of five labels: \"Food\", \"Staff\" & \"Ambience\".\\n\\nWe use the SemEval datasets as development sets, while using the Citysearch dataset as our test set."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Results\\n\\nSERBM, ABAE and AE-CSA are neural networks, while W2VLDA is a topic model-based approach.\\n- SERBM (2015) 86.0 74.6 79.5\\n- ABAE (2017) 89.4 73.0 79.6\\n- W2VLDA (2018) 80.8 70.0 75.8\\n- AE-CSA (2019) 85.6 86.0 85.8\\n\\nMean: mean of word vectors\\nMean 78.9 76.9 77.2\\n\\nAttention 80.5 80.7 80.6\\nAttention: dot-product attention\\nCAt# 86.5 86.4 86.4\\nCar: our full model\\n\\nTable 2: Weighted macro averages across all aspects on the test set of the Citysearch dataset."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Results\\n\\nThe results show that our labeling strategy is already really effective: simply taking the mean of word vectors and picking the word vector of the closest label gives good results.\\nYet, our attention mechanism still adds significant value."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Takeaways:\\n- In-domain word embeddings are really important.\\n- The model does not take into account sense information.\\n- The restaurant domain is probably too easy."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Try it yourself https://github.com/clips/cataS"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 330,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "NLP Datasets\\n- Text Classification\\nEnglish: AGNews, DBPedia, Yahoo Answer (Zhang et al., 2015), TREC (Voorhees and Tice,\\n1999)\\nChinese: THUCNews (Sun et al., 2016), SogouCS (Wang et al., 2008), Fudan Corpus, iFeng and ChinaNews (Zhang and LeCun,\\n2017)\\n- Question Answering\\nEnglish: Open Domain QA (Chen et al.,\\n2017)\\nChinese: NLPCC Shared Task (Duan and Tang, 2017), DuReader (He et al., 2018), Chinese College Entrance Test (Zhang and Zhao,\\n2018)\\n- Summarization\\nEnglish: Newsroom (Grusky et al., 2018), ArXiv and PubMed (Cohan et al., 2018), CNN/Daily Mail dataset (Hermann et al., 2015), Gigaword (Napoles et al., 2012), Xsum (Narayan et al.,\\n2018)\\nChinese: LCSTS (Hu et al., 2015)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Motivations: Current datasets are majorly single-task.\\nCurrent datasets lack diversity, and most of them are in the news domain.\\nThe lack of large-scale Chinese datasets, especially for summarization."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "MATINF: Maternal and Infant (MATINF) Dataset\\n\\nThe largest (1.07 million) joint dataset for three major NLP tasks (and also the only to date):\\n- Text Classification (18 + 3 classes)\\n- Infant Health Care Classification\\n- Question Answering: Why does my baby always stick his tongue out?\\n- Summarization: My baby is almost four months old.\\nIn these few days, I suddenly found that my baby always sticks his tongue out and has a lot of saliva. So what is this?\\n\\nDon't worry, it's normal. Kids are like this. It is also normal for your baby to stick his tongue out.\\nYou don't have to worry too much. Your baby's drooling may be a sign of teeth growing.\\n\\nFigure 1: An example entry from MATINF."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "MATINF: Maternal and Infant (MATINF) Dataset\\n\\nData Cleaning = classes with insufficient samples = entries in which the length of the description field is less than the length of the question field = data with any field longer than 256 characters = human-spotted ill-formed data.\\n\\nSplitting FDS. A Question Description Answer | Max Len. 7:1:2 for training, validation, test\\n\\n# Char 14.72 64.17 66.91 256\\n\\n# Word 9.03 41.70 42.32\\n- Table 1: Average character and word numbers of question, description and answer in MATINF.\\nWe ensure that every field of each entry has at most 256 characters."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "MATINF for Classification\\n\\nTwo fine-grained classification tasks\\n- MatINF-C-Toric\\n- MATINF-C-AGEm Age Classification\\n- postpartum health care 0-18\\n- Topic classification\\n- child allergy 12%\\n- Description + Question -> Class\\n- motion development 2.3%\\n- infant health care\\n- infant psychology\\n- early education\\n- infant feeding\\n- infant nutrition\\n- pregnancy care\\n- family education\\n- kindergarten\\n- pregnancy preparation\\n- infertility problem\\n- vaccination\\n- skin care\\n- infant ulcer\\n- diarrhea\\n- other infant common diseases\\n\\nTable 2: Class names of two subsets and their English translations."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "MATINF for Summarization\\n\\nTask Definition\\n\\nInfant health care\\n\\n\"Why does my baby always stick his tongue out?\"\\n\\nMy baby is almost four months old.\\nIn these few days, I suddenly found that my baby always sticks his tongue out and has a lot of saliva. So what is this?\\n\\nDon't worry, it's normal. Kids are like this. It is also normal for your baby to stick his tongue out.\\nYou don't have to worry too much. Your baby's drooling may be a sign of teeth growing.\\n\\nFigure 1: An example entry from MATINF."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "MATINF for Summarization> Comparisons\\n\\nFirst summarization data in health domain\\n\\nDataset               Lang.     Domain      #Doc\\n-     # Token         Doc. Sum.\\nCNN /DM (2015)       EN       News        312K     781         56\\nNYT (2012)           EN       News        655K     796         45\\nNewsRoom (2018)      EN       News        121M     751         30\\nBigPatent (2019)     EN       Academic    134M     3573        7\\narXiv (2018)        EN       Academic    216K     6914     = 293\\nPubMed (2018)       EN       Academic    133K     3224\\n- 218\\nGigawords (2012)    EN       News        4.02M    31          8\\nLCSTS (2015)        ZH       News        240M     = 104       17\\nXSum (2018)         EN       News        227K     = 431       23\\nMATINF-SUMM         ZH       Health      1.07TMM  42          9\\n\\nTable 5: Comparison of summarization datasets.\\n\"# Token\" indicates the average token numbers of a document and a summary for each dataset."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Multi-task Learning\\n\\nComparisons\\n\\nMulti-task Model\\n\\nMTF-S2S\\n\\nMATINF: avoiding new input, adding only new input + token t t f supervision (Figure 2a and 2b)\\n\\nThe Fake x al (va= a fair and ideal stage for exploring multi-task learning, especially auxiliary and multi-task supervision under a Traditional Main single dataset.\\n\\n# Task? Task 2\\n\\nH + += MTF-S2S: sharing module, not layers\\n\\nToa)\\n\\n(Having specific 1 |specify 2| | Tool 1 Shared Layer\\n- (Shared Module + Task 2 Input Task-spectio 2\\n- Layer sharing\\n- Module sharing\\n\\nFigure 2: The difference between MTF-S2S and traditional multi-task learning."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Multi-task Field-shared Sequence to Sequence (MTF-S2S) Architecture = 2\\n\\nShared question encoder for classification and QA, also used as decoder for summarization:\\n\\nLearning posed:\\n- Batch size, 2G cueVa, b € T, bs,/bs, = Na /ny =+.\\n- Answer ba el Beta Joint loss\\n- Decoder\\n\\nFigure 3: The architecture of MTF-S2S. Note that a common attention mechanism (Luong et al.,\\n2015) is applied when decoding question and answer (in the blue and green boxes), but we do not illustrate it in this figure for clarity."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Experiments\\n- Classification Baselines\\n\\nMethod                      AGE      TOPIC\\nStatistical baseline        TF-IDF + LR   76.88   40.25\\nText CNN (Kim,\\n2014)       90.95   64.41\\nPre-trained LMs\\nDCNN (Kalchbrenner et al.,\\n2014)   90.96   64.60\\nRCNN (Lait et al.,\\n2015)   90.81   63.56\\nfastText (Grave et al.,\\n2017)       87.76   61.81\\n\\nOur task is challenging for the current SOTA models\\nDPCNN (Johnson and Zhang,\\n2017)     91.02   65.92\\nMultitask learning is helpful\\nBERT (Devlin et al.,\\n2019)     90.33   66.95\\nBERT-of-Theseus (Xu et al.,\\n2020)     90.25   66.72\\nERNIE (Sun et al.,\\n2019)       90.42   66.66\\nMTF-S2S (single task)       90.15   63.40\\nMTEF-S2S         90.29   63.59\\n\\nTable 6: Experimental results of baseline methods on MaTtINF-C in terms of accuracy. +: Character-based models."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Coy et al.\\n\\nTable 1: Baselines\\n- BERT Matching = 58.32 | 36.42 | 49.00\\n- Seq2Seq (2014) = 16.62 | 4.53 | 10.37\\n- Seq2Seq + Att (2015) = 19.62 | 5.87 | 13.34\\n- MTF-S2S (single task) = 20.28 | 5.94 | 13.52\\n- MTF-S2S = 21.66 | 6.58 | 14.26\\n\\nBest Passage (upper bound) = significantly improve the performance BERT Matching (2019) [1866 328 10.78].\\n\\nTable 7: Experimental results of baseline methods on MATINF-QA."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "> Summarization: BertAbs outperforms all the other models. Multi-task learning is effective on our dataset.\\n\\nCNN/DM   LCSTS   MaTINF-SUMM\\n\\nMethod                                       R-L  R-2  RL R-1  R-2  RL  R-I  R-2  R-L\\n\\n'TextRank (Mihalcea and Tarau,\\n2004)       37.72 15.59 33.81 24.38 11.97 16.76 35.53 25.78 36.84\\n\\nLexRank (Erkan and Radev,\\n2004)            33.98 11.79 30.17 22.15 10.14 14.65 33.08 23.31 34.96\\n\\nSeq2Seq (Sutskever et al.,\\n2014)\\n-\\n-\\n-\\n-\\n-\\n-   23.05 11.44 19.55\\n\\nSeq2Seq + Att (Luong et al.,\\n2015)         31.33 11.81 28.83 33.80 23.10 32.50 43.05 28.03 38.58\\n\\nWEAN (Ma et al.,\\n2018)\\n-\\n-\\n-   37.80 25.60 35.20 34.63 22.56 28.92\\n\\nGlobal Encoding (Lin et al.,\\n2018)         a = 5 39.40 26.90 36.50 49.28 34.14 47.64\\n\\nBertAbs (Liu and Lapata,\\n2019)            40.21 17.76 37.09\\n-\\n-\\n-   57.31 44.05 55.93\\n\\nMTF-S2S (single task)                      31.36 11.80 28.88 33.75 23.20 32.51 43.02 28.05 38.55\\n\\nMTF-S2S\\n-\\n-\\n-\\n-\\n-\\n-   48.59 35.69 43.28\\n\\nTable 8: Experimental results of baseline methods on CNN/DM (Hermann et al., 2015), LCSTS (Hu et al., 2015), and MATINF-SUMM."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "We propose the largest joint dataset for three major NLP tasks including classification, summarization and question answering.\\n\\nWe propose a simple yet effective multi-task learning model and demonstrate its effectiveness on our dataset.\\n\\nWe hope our work can support future work in Chinese NLP and multi-task learning."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Resource WHUIR GITHUB: https://github.com/WHUIR/MATINE\\n\\nOur dataset can only be used for non-commercial purposes.\\n\\nPlease fill out a request form to obtain the download link!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Thanks!"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 677,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Background"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Text-to-SQL Parsing\\n\\nDatabase: concert singer\\n\\n'Show all countries and the number of singers in each country.'\\n\\nTask: translating natural language utterance to SQL queries.\\n\\nSQL SELECT Country, count(*) FROM Singer GROUP BY Country"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Text-to-SQL Parsing\\n\\ndatabase: concert singer @ ®\\n\\n'Show all countries and the number of singers in each country.\\n\\nSELECT Country, count(*) FROM Singer GROUP BY Country\\n\\nTask: translating natural language utterance to SQL queries.\\n\\nApplication: give people access to vast amounts of databases"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Cross-Domain Text-to-SQL Parsing\\n\\nTrainEE database: concer singer®\\n- 'Show all countries and the number of singers in each country.\\n- Sat SELECT Country, count(*) FROM Singer GROUP BY Country\\n\\ndatabase: farm is®\\n- Please show the different statuses of cities and the average population of cities with each status.\\n- Sat SELECT Status, avg(Population) FROM City GROUP BY Status"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Cross-Domain Text-to-SQL Parsing\\n\\nTrain & database: concert singer®\\n- 'Show all countries and the number of singers in each country.'\\n- SELECT Country, count(*) FROM Singer GROUP BY Country\\n\\ndatabase: farm\\n- 'Please show the different statuses of cities and the average population of cities with each status.'\\n- SELECT Status, avg(Population) FROM City GROUP BY Status\\n\\nGeneralization Challenge: a parser needs to generalize to unseen domains."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Cross-Domain Text-to-SQL Parsing\\n\\nDatabase:\\n- Show all countries and the number of singers in each country.\\nSELECT Country, count(*) FROM Singer GROUP BY Country\\n\\nDatabase:\\n- Please show the different statuses of cities and the average population of cities with each status.\\nSELECT Status, avg(Population) FROM City GROUP BY Status\\n\\nGeneralization Challenge: a parser needs to generalize to unseen domains.\\nApplication: reduce annotation effort for multi-domain interfaces."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Capturing Domain Generalization"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Motivating Example\\n\\nNatural Language Question:\\nFor the cars with 4 cylinders, which model has the largest horsepower?\\n\\nDesired SQL:\\nSELECT T1.model\\nFROM car_names AS T1\\nJOIN cars_data AS T2\\nON T1.make_id = T2.id\\nORDER BY T2.horsepower DESC\\nLIMIT 1\\n\\nSchema:\\n- car_names\\n- model\\n- make\\n- country\\n- foreign keys (known)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Motivating Example\\n\\nNatural Language Question: For the cars with 4 cylinders, which model has the largest horsepower?\\n\\nDesired SQL:\\nSELECT T1.model\\nFROM car_names AS T1\\nJOIN cars_data AS T2\\nON T1.make_id = T2.id\\nORDER BY T2.horsepower DESC\\nLIMIT 1\\n\\nSchema:\\n\\n* Column\\n* Column\\n* Foreign keys (known)\\n\\nSchema Encoding: encode graph-structured database schemas"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Motivating Example\\n\\nNatural Language Question: For the cars with 4 cylinders, which model has the largest horsepower?\\n\\nDesired SQL:\\nSELECT T1.model\\nFROM car_names AS T1\\nJOIN cars_data AS T2\\nON T1.make_id = T2.id\\nORDER BY T2.horsepower DESC\\nLIMIT 1"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Motivating Example\\n\\nNatural Language Question: For the cars with 4-6 cylinders, which model has the largest horsepower?\\n\\nDesired SQL:\\n\\n> SELECT T1.model\\nFROM car_names AS T1\\nJOIN cars_data AS T2\\nON T1.make_id = T2.id\\nORDER BY T2.horsepower DESC LIMIT 1\\n\\nSchema:\\n- model_list\\n- carmakers\\n- take\\n- id\\n- model\\n- make\\n- model_id\\n- maker\\n- full_name\\n- country\\n\\nColumn\\n- Column foreign keys (known)\\n- Question Column linking (latent)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Motivating Example\\n\\nNatural Language Question: For the cars with 4 cylinders, which model has the largest horsepower?\\n\\nDesired SQL:\\n\\nSELECT T1.model FROM car_names AS T1 JOIN cars_data AS T2 ON T1.make_id = T2.id ORDER BY T2.horsepower DESC LIMIT 1\\n\\nSchema:\\n\\nColumns\\n- Column foreign keys (known)\\n- Question > Column linking (latent)\\n- Question > Table linking (latent)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Motivating Example\\n\\nNatural Language Question:\\nFor the cars with cylinders, which field has the largest horsepower?\\n\\nDesired SQL:\\nSELECT T1.model\\nFROM car_names AS T1\\nJOIN cars_data AS T2 ON T1.make_id = T2.id\\nORDER BY T2.horsepower DESC\\nLIMIT 1\\n\\nSchema:\\n- cylinders\\n- horsepower\\n- weight\\n- accelerate\\n- year\\n- model_list\\n- carmakers\\n\\nColumn -> Column foreign keys (known):\\nQuestion > Column linking (latent)\\nQuestion > Table linking (latent)\\nValue Column linking (latent)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Motivating Example\\n\\nNatural Language Question: For the cars with Vlinders, which model has the largest horsepower?\\n\\nDesired SQL:\\nSELECT T1.model\\nFROM car_names AS T1\\nJOIN cars_data AS T2\\nON T1.make_id = T2.id\\nORDER BY T2.horsepower DESC\\nLIMIT 1\\n\\nSchema:\\n- Column foreign keys (known)\\n- Question > Column linking (latent)\\n- Question > Table linking (latent)\\n\\nSchema Linking: capturing latent linking between question and tables/columns\\n- Value\\n- Column linking (latent)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Previous Work: GNNCM aeprogram_id oNprosran ia GO)\\n- @sonoster td gy |) gy seosene tasomester_id 6 @ student_id\\neoon OoBogin et al. (2019a) used GNN to encode database schemas"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Previous Work: GNNGraph\\n\\nShortcomings:\\n- Message propagation is limited to the schema edges such as ancestor and foreign key relations (semester_id, student_id).\\n- Question and schema are not jointly encoded, thus making it hard for schema linking.\\n\\nBogin et al. (2019a) used GNN to encode database schema."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Previous Work: IRNet\\n- IRNet (Guo et al.,\\n2019) used string-match based types (highlighted) to facilitate schema linking."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Previous Work: IRNet\\n\\nShortcomings: Schema Encoder of IRNet does not fully exploit the schema relations.\\n\\nQuestion: Show the book titles and years for all books.\\n\\nType: none | none | Column | none | Column {none} none | Table\\n\\nIRNet captures unary relations instead of binary relations.\\n\\nColumn: year | book title | -- | title\\n\\nSchema Encoder in IRNet (Guo et al.,\\n2019) used string-match based types (highlighted) to facilitate schema linking."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Previous Work: IRNet\\n\\nShortcomings:\\n- Schema Encoder of IRNet does not fully exploit the schema relations.\\n\\nQuestion: Show the book titles and years for all books.\\n\\nType: none | none | Column | none | Column | none | none | Table\\n- IRNet captures unary relations instead of binary relations.\\n\\nColumn: year | book title | title\\n\\nExample\\nSchema Encoder: Unary relation: word 'books' is matched (for some column).\\n\\nIRNet (Guo et al.,\\n2019) used string-match based types to facilitate schema linking.\\n- Binary relation: (highlighted) word 'books' is matched to 'book title'."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "A Unified Framework With Relation-Aware Transformer"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Transformer [Vaswani et al., 2017] \"ooack?\\n[me | [me] [me |ay = softmax; se Attention |< ' * Vdim areas) [me] [me | [mee | vim >» ays Attention == = --;\\na)\\na) Ga) GeaoQ-4 +4 Q-4 o-4 = = = Pa>) ee) eee) ee) The cow eats grass,"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Relation-Aware Transformer (RAT) [Vaswani et al., 2017] [Shaw et al., 2018] \"eee 'Tel(k) + By)\" [me] [me | Gi (Kj + Bij Attention\\n- SSa;; = softmax; -------_ . ;v 7 dim[ me | y = > Qi Y + eij) ston tion <I _ S =\\na) ae + e + eft etaa) Gee) <a The cow eats grass,"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Relation-Aware Transformer (RAT) [Vaswani et al., 2017] [Shaw et al., 2018]\\n- softmax,\\nattention S = a J Vdim Cartons) y = 2 aij (Vj + ij)\\n\\nRelative positional embeddings\\n\\nThe cow eats grass,"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Relation-Aware Transformer (RAT) [Vaswani et al., 2017] [Shaw et al., 2018] xi qi ki, V; (eatems + [me | [me] [ime | [me | as, = softmax, 2!\\n(kj + Bij) attention | = < = a J Vdim ca ra) [me | [me | Yi = de (yj + ey) assertion > SS -- Ga) Gee)} et e + ef eore\\n-\\n\\nArbitrary edge features\\n\\nThe cow eats grass,"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "GNNs vs. Transformers\\n\\nGCN/GGNN\\nGAT\\nRAT\\nTransformer\\n- Message Passing\\n- Neighborhood\\n- Neighborhood\\n- All (w/ diff. edge funcs)\\n- All\\n- Edge features\\n- Yes, neighbors\\n- Yes, neighbors\\n- Yes, all nodes\\n- Induced\\n- Aggregation\\n- Conv/Gating\\n- Self-Attention\\n- Self-Attention\\n- Self-Attention"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "(Binary) Relations for Schema Encoding = Example relations\\n\\nForeign key relations. Column/table correspondence.\\n\\nTable/primary key correspondence.\\n\\nSee our paper for a full list of schema relations."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "(Binary) Relations for Schema Linking\\n- Name-based linking relation\\nE.g., link 'cars' to column 'car_names'\\n- Exact occurrences of tables/columns\\n- Partial occurrences of tables/columns\\n- Value-based linking relation\\nE.g., link value 'Edinburgh' to column 'city'\\n- Values as evidence to generate corresponding columns retrieved quickly via DB indices & textual search"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "Example of RAT Layers as Seen in Table For cars with GS car model mpg cylinders Question Tables Columns"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "Experiments"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          "Spider Dataset\\nMultiple schemas\\nComplex questions\\nATIS, GeoQuery\\nxWikiSQL\\nxSpider20"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 30,
        "texts": [
          "Spider Dataset\\n\\nMultiple schemas\\n\\nComplex questions\\n\\nATIS, GeoQuery\\n\\nxWikiSQL\\n\\nxSpider\\n\\nMuch more challenging task!"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 31,
        "texts": [
          "Results on SpiderModel TestIRNet (Guo et al.,\\n2019) 46.7\\nGlobal-GNN (Bogin et al., 2019b) 47\\nIRNet V2 (Guo et al.,\\n2019) 48.5\\nRAT-SQL (ours) 57.2"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 32,
        "texts": [
          "Results on SpiderModel\\n- TestIRNet (Guo et al.,\\n2019) 46.7\\n- Global-GNN (Bogin et al., 2019b) 47.4\\n- IRNet V2 (Guo et al.,\\n2019) 48.5\\n- RAT-SQL (ours) 87.2\\n\\nWith BERT:\\n- EditSQL + BERT (Zhang et al.,\\n2019) 53.4\\n- GNN + Bertrand-DR (Kelkar et al.,\\n2020) 54.6\\n- IRNet V2 + BERT (Guo et al.,\\n2019) 55.0\\n- RYANSQL V2 + BERT (Choi et al.,\\n2020) 60.6\\n- RAT-SQL + BERT (ours) 65.6\\n\\n+ RAT-SQL achieves the new state-of-the-art performance\\n+ Our non-BERT version is better than most BERT-augmented models"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 33,
        "texts": [
          "1. RATSQL v3 + BERT (DB content used) 69.7 65.6 Microsoft Research (Wang and Shin et al., ACL '20)\\n2. AuxNet + BART (DB content used) 70.0 61.9 [OEE Anonymous ©\\n- urrent\\n3. RATSQL v2 + BERT (DB content used) 65.8 61.9 Microsoft Research Leaderboard (Wang and Shin et al., ACL '20)\\n4. AuxNet + BART 68.0 61.3 Anonymous\\n5. RYANSQL v2 + BERT 70.6 60.6 GE Kakao Enterprise (Choi et al., '20)\\n6. IRNet++ + XLNet (DB content used) 65.5 60.1 Anonymous"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 34,
        "texts": [
          "Ablations\\n\\nImportance of Relations\\n\\n80.0%\\n60.0%\\n40.0%\\n20.0%\\n0.0%\\n\\nRAT-SQL\\nRAT-SQL w/o value-based name-based graph relations\\nw/ schema linking\\n\\nSchema linking features and schema graph relations are crucial."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 35,
        "texts": [
          "Summary: We propose RAT-SQL, a unified framework to address schema encoding and schema linking based on the relation-aware transformer.\\nRAT-SQL achieves the new state-of-the-art performance on Spider. Code: https://github.com/microsoft/rat-sql"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 36,
        "texts": [
          "Demo25"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 37,
        "texts": [
          "cL Association for Computational Linguistics 2020 Annual Conference\\n\\nVideo is Playing"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 576,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Despite the merits of EMRs, many doctors suffer from the time-consuming writing process.\\n\\nTable\\n4. Physician Time Distribution During Office Hours, by Task Category\\n\\nTask Category, by Activity During Office Hours\\n\\nDirect clinical face time 33.1 (1.9\\n-\\n34)\\n\\nWith patient 44.0 (27.0 (25.8\\n- 28.3))\\n\\nWith staff and others (patient not present) 21.2 (5.7 (5.1\\n- 5.8))\\n\\nTest result 16.6 (4.0 (3.6\\n- 6.8))\\n\\nMedication order 2.0 (1.2 (2.1\\n- 2.2))\\n\\nOther 10.0 (1.0 (1.9\\n- 2.2))\\n\\nAdministrative tasks 11.0 (4.1\\n-\\n3)\\n\\nInsurance 1.2 (0.6\\n- 0.7)\\n\\nScheduling 13.5 (5.1 (3.0\\n- 8.0))\\n\\nOther tasks 19.9 (19.2\\n- 21.4)\\n\\nClosed to observation 1.0 (5.5 (4.5\\n- 5.5))\\n\\nOther (aggregated) 90.9 (10.3 (2.0\\n- 521.8))\\n\\nTranscription 29.4 (15.0 (2.2\\n- 29.1))\\n\\nPersonal 9.0 (10.9 (2.0\\n- 6.7))\\n\\nTable from: Christine Sinsky, Lacey Colligan, Ling Li, Mirela Prgomet, Sam Reynolds, Lindsey Goeders, Johanna Westbrook, Michael Tutty, and George Blike.\\n2016. Allocation of physician time in ambulatory practice: a time and motion study in 4 specialties.\\nAnnals of Internal Medicine, 165(11):753-760.\\n- Automatically converting medical dialogues to EMRs is a possible solution.\\n- Dialogue-EMR pairs are scarce, which makes end-to-end method hard.\\n- Extracting information from medical dialogues is an essential and the first step."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "1. Challenge Dialogue Window Annotated Labels\\n\\nSymptom: Premature beat (doctor-pos)\\n\\nPatient: Doctor, could you please tell me is it premature beat?\\n\\nDoctor: Yes, considering your electrocardiogram. Do you feel palpitation or short of breath?\\n\\nPatient: No. Can I do radiofrequency ablation?\\n\\nDoctor: It is worth considering. Any discomfort in chest?\\n\\nSurgery: Radiofrequency ablation. (doctor-pos)\\n\\nPatient: I always have bouts of pain.\\n\\nSymptom: Chest pain (patient-pos)\\n\\nColloquial expression\\n\\nOral expressions are much more diverse than general texts, which will lead to performance degradation of conventional NLP tools.\\n\\nMulti-Turn\\n\\nAvailable information is scattered in various dialogue turns, thus the interaction between turns should be also considered."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Our Contribution: We propose a new dataset, annotating 1,120 doctor-patient dialogues from online consultation medical dialogues with more corpus than 40k labels.\\n\\nWe propose MIE, a medical information extractor based on a novel deep matching model that can make use of the interaction between dialogue turns.\\n\\nMIE achieves a promising overall F-score of 69.28, significantly surpassing several competitive baselines.\\n\\nData and codes are available at https://github.com/nlpir2020/MIE-ACL-2020."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Patient: Doctor, could you please tell me is it premature beat?\\n\\nBackache, perspiration.\\n\\nDoctor: Yes, considering your Electrocardiogram. Do you feel hiccups?\\n\\nPatient: Positive (appear) to nausea patient-negative (absent).\\n\\nPalpitation or short of breath?\\n\\nSymptom: Cyanosis doctor-positive (diagnosed).\\n\\nPatient: No. Can I do radiofrequency again?\\n\\nDoctor: It is worth considering. Any discomfort in chest?\\n\\nAbdominal discomfort.\\n\\nPatient: I always have bouts of pain, patient-positive (done).\\n\\nInterventional treatment patient-positive (done).\\n\\nDoctor: How long did it last?\\n\\nSurgery: Radiofrequency ablation, doctor-positive (suggest).\\n\\nHeart bypass surgery doctor-negative (irrelevant).\\n\\nPatient: Two or three minutes? I don't remember clearly.\\n\\nStent implantation doctor-negative (deprecated).\\n\\nSliding Window B-mode ultrasonography, CT examination, CT angiography, CDFI patient-positive (done).\\n\\nDialogue Window Test doctor-positive (suggest).\\n\\nUltrasonography doctor-negative (deprecated).\\n\\nMRI unknown.\\n\\nDialogue Window notated Labels.\\n\\nThyroid function test.\\n\\nTreadmill test.\\n\\nDialogue Window.\\n\\nPatient: Doctor, could you please tell me is it premature beat?\\n\\nSleep.\\n\\nDoctor: Yes, considering your history.\\n\\nAny discomfort in chest?\\n\\nSmoking."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "a Corpus\\n\\nThe detailed annotation statistics of the dataset\\n\\nDialogue Window\\n\\nSymptom\\n- Surgery\\n- Test\\n- Other info\\n\\nTrain\\n800\\n12931\\n21420\\n839\\n8879\\n1363\\n\\nDev\\n160\\n2587\\n4254\\n119\\n1680\\n259\\n\\nTest\\n160\\n2694\\n4878\\n264\\n1869\\n327\\n\\nTotal\\n1120\\n18212\\n30552\\n1222\\n12428\\n1949\\n\\nThe distribution of status over all labels\\n- Patient-pos\\n- Patient-neg\\n- Doctor-pos\\n- Doctor-neg\\n- Unknown\\n\\nSymptom\\n15119\\n1782\\n1655\\n910\\n11086\\n\\nSurgery\\n169\\n48\\n698\\n10\\n297\\n\\nTest\\n5589\\n303\\n4443\\n44\\n2049\\n\\nOther info\\n550\\n1399\\n-\\n-\\n1505"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Model Medical Dialogue\\n\\nI have difficulty in breathing occasionally.\\n- Candidates symptoms\\n- chest pain\\n- Doctor\\n- positive candidate encounter\\n- Test Ultrasonic: Patient\\n- negative series\\n\\nCandidate scores: MIE is a deep matching model, which contains:\\n- Encoder Module\\n- Matching Module\\n- Aggregate Module\\n- Scorer Module"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "| Moder Medical Dialogue\\n\\nCandidate scores\\n\\nHSEncoder Module = BiLSTM(X)\\n\\nEncoder(U[i]) = WH + H(i), ei\\n\\nEncoder(U[i]) = softmax(a)\\n\\nHe = Encoder(V)\\n\\nAU HS = Encoder(S)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Model Medical Dialogue\\nHi po mente teatime\\n- Smeets\\n- a Gris pattie teedimias\\n- Candidate Encoder\\n- AI\\n- Matching Module\\n- softmax(a,[i])\\n- softmax(as[i])\\n- Spel\\n- LHR\\n- J\\n- poli\\n- AMT"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Model Medical Dialogue\\n5. eqn Cee9 intent | Utterance Encoder\\n- Hi He): saan mete ene\\n\\nCandidates oe maon\\n- Habll-MIE-multiAggregate Module\\n- Was (k]7] = softmax(altMIE-single\\n- h3 3 Z s(t) = Plt, FIsF = concat(q-[i], as)\\n- [i] = concat(qe{i], g)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Model Medical Dialogue\\n\\nCandidates\\n- Scorer\\n- Scorer Module\\n- Loss function\\n- Feedforward\\n- Sigmoid"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "1. Experiments\\n\\nBaselines\\n\\nPlain-Classifier\\n- concatenates all the utterances to obtain a long sequence\\n- uses a Bi-LSTM encoder, using category-specific parameters\\n- uses self-attention to represent it as a single vector\\n- uses feed-forward classifier network\\n\\nMIE-Classifier\\n- reuses MIE model architecture\\n- treats cutt c and cutt s directly as qc and qs respectively\\n- single and multi strategy is also applied.\\n\\nEvaluation\\n\\nWindow-level\\n- Report the micro-average of all the test windows\\n\\nDialogue-level\\n- Report the micro-average of all the test dialogues\\n- (First, merge the results of the windows that belong to the same dialogue.\\nThen, for labels that are mutually exclusive, we update the old labels with the latest ones)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Experiments Main Results (Window-level Dialogue-level Model)\\n\\nCategory\\nFall Category\\n\\nPlain-Classifier MIE-Classifier-single\\n68.20 | 61.60 | 62.87 | 93.23 | 91.77 | 75.36 | 80.96 | 71.87 | 36.67\\nPics\\nO76 | ms | mn | e | ar a |\\n\\nMIE-multi\\n80.42 | 76.25 | 77.77 | 77.21 | 66.04 | 69.75 | 70.24 | 64.96 | 66.40 | 96.86 | 91.52 | 92.69 | 95.31 | 82.53 | 86.83 | 76.83 | 64.07 | 69.28\\n- MIE-multi achieves the best F-score on both window-level and dialogue-level full evaluation metric, as we expected.\\n- Both models using multi-turn interactions perform better than models solely using single utterance information.\\n- Matching-based methods surpass classifier models in full evaluation.\\n- Both MIE models and MIE-classifier models overwhelm Plain-Classifier model, which indicates the MIE architecture is far more effective than the basic LSTM representation concatenating method."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "1. Experiments Error Analysis\\n\\nPoor performance cases\\nSymptom: Limited mobility\\nSymptom: Nausea\\nSymptom: Cardiomyopathy\\nTest: Renal function test\\n\\nThe possible reason is that they rarely appear in the training set, with frequency of 0.63%, 2.63%, 2.38% and 1.25%."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Experiments Case Analysis\\n\\nPatient: I have atrial fibrillation, heart failure, anemia and loss my appetite.\\n\\n(a) attention heat map when the category-item pair attends to the utterance.\\n\\nDoctor: Hello! How long did them last? Did you examine blood routine?\\n\\nPatient: Yes.\\n\\nDoctor: Is there coronary heart disease?\\n\\nThe attention values of the mention of coronary heart disease are relatively high -> MIE can capture the correct category-item pair information in the window.\\n\\nPatient: I have atrial fibrillation, heart failure, anemia and loss my appetite.\\n\\nDoctor: Hello! How long did them last? Did you examine blood routine?\\n\\nPatient: Yes.\\n\\n(b) attention heat map when the status information attends to the utterance.\\n\\nDoctor: Is there coronary heart disease?\\n\\nPatient: No.\\n\\nThe attention values of the expressions related to status such as \"Yes\" and \"No\" are high -> MIE can capture the status information in the window.\\n\\nPatient: I have atrial fibrillation, heart failure, anemia and loss my appetite.\\n\\nDoctor: Hello! How long did them last? Did you examine blood routine?\\n\\nPatient: Yes.\\n\\n(c) The interaction between the fourth utterance and the other utterances.\\n\\nDoctor: Is there coronary heart disease?\\n\\nThe fifth utterance is the most relevant utterance in the window -> MIE successfully obtains the related status information for the category-item pair.\\n\\nIn a nutshell, MIE-multi can properly capture the category-item pair and status information in the window."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Experiments Case Analysis\\n\\nPatient: What is the effect of sinus arrhythmia?\\nDoctor: Sinus arrhythmia is normal in general. Don't care about it unless you feel unwell significantly.\\nPatient: I'm feeling unwell so much (because of the sinus arrhythmia).\\n\\nMIE-single symptom: sinus arrhythmia (unknown)\\nMIE-multi symptom: sinus arrhythmia (patient-positive)\\nVMIE-single fails to handle turn-interaction, and only obtains the category-item information\\n\\nSymptom: Sinus arrhythmia\\nMIE-multi can capture the interaction between different utterances and predicts the label successfully."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "J | Conclusion & Future Work\\n\\nConclusion\\nWe propose a new dataset (1,120 dialogues, 46,151 labels).\\nWe propose MIE, a medical information extractor.\\nMIE achieves a promising overall F-score of 69.28.\\n\\nFuture Work\\nIn the future, we plan to:\\n- leverage the internal relations in the candidate end\\n- try to introduce rich medical background knowledge into our work"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "AH :nN it a THANKS!"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 445,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Summarization metrics\\n- ROUGE\\nMeasures lexical overlap (e.g., unigram, bigram, longest common subsequence) between reference and system summaries.\\n- Pyramid\\nManually labels SCUs (Summarization Content Units) in the reference and system summaries, and measures SCU overlap.\\n- FAR (Facet-Aware Recall, proposed metric)\\nTreats each sentence in the reference summary as a facet, identifies the sentences in the document that express the semantics of each facet as support sentences of the facet, and automatically evaluates extractive summarization methods by comparing the indices of extracted sentences and support sentences (facet overlap)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Benefits of FAR compared to ROUGE: higher human correlation, more interpretable results.\\n\\nCompared to Pyramid: one-time annotation required only for reference summaries (can be further automated), automatic evaluation afterwards.\\n\\nOther benefits: enables fine-grained evaluation (using the category breakdown defined through annotation) and comparative analysis of extractive methods."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Facet-Aware Evaluation Notions\\n\\nFacet: sentence in the reference summary.\\n\\nSupport sentence: sentences in the document that express the semantics of one facet.\\n\\nSupport group: a set of support sentences that can fully cover the information of one facet.\\n\\nFacet-aware mappings (FAMs): mappings from each facet (sentence) in the reference summary to its support sentences in the document.\\n\\nOne facet may have multiple support groups.\\n\\nThere could be more than one set of support sentences that cover the same facet.\\n\\nOne support group may have multiple support sentences.\\n\\nTwo support sentences in one support group if they are complementary and only combining them can fully cover the facet."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Illustration a | Document D* A facet is \"covered\" if any of its Reference Summary R Extracted Summary E support groups is fully covered by the extracted summary.\\n\\nFAR = Lier Any(U(S}, £),\\n- USh € ) EER FAM svt) FAR can* In the example:\\n\\nTwo of three support groups FAM1 'Any (@, 2, %) of facet 1 are covered.\\n\\nEa => a* Facet 2 cannot be covered as document sentence 4 is missing in the extracted summary:\\n\\nFacet-Aware Evaluation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Manual FAM Creation: we annotate the FAMs of 150 document-summary pairs from the test set of CNN/Daily Mail.\\n\\n310 non-empty FAMs are created by three annotators with high agreement (pairwise Jaccard index 0.714) and further verified to reach consensus.\\n\\nDiscovered categories:\\n- Noise: The facet is noisy and irrelevant to the main content.\\n- Low-abstraction: The facet can be mapped to its support sentences.\\n- High-abstraction: The facet cannot be mapped to its support sentences."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Category '#Samples -- #Facets Example (full documents, reference summaries, and the FAMs can be found in Appendix\\nC)\\n\\nReference: \"Furious 7\" opens Friday. (unimportant detail)\\n\\nReference: Click here for all the latest Floyd Mayweather vs Manny Pacquiao news. (not found)\\n\\nNoise (N) 4127.3%) 137 27.1%) in the document)\\n- Reference: Vin Diesel: \"This movie is more than a movie\". (random quotation)\\n- Reference: \"I had a small moment of awe,\" she said. (random quotation)\\n- Reference: Willis never trademarked her most-famous work, calling it \"my gift to the city\".\\n- Support: Willis never trademarked her most-famous work, calling it \"my gift to the city.\"\\n\\n30 (61.2%) (identical)\\n\\nM=1\\n\\nLow Abstraction (L) 89 (59.3%) 275 (88.7%)\\n- Reference: Thomas K.\\nJenkins, 49, was arrested last month by deputies with the Prince George's County sheriff's office, authorities said.\\n\\n35 (11.3%)\\n- Support: Authorities said in a news release Thursday that 49-year-old Thomas K.\\nJenkins of Capitol Heights, Maryland, was arrested last month by deputies with the Prince George's County sheriff's office.\\n(compression)\\n- Reference: College-bound basketball star asks girl with down syndrome to high school prom.\\nPictures of the two during the \"prom-posal\" have gone viral. (highly abstractive)\\n\\nHigh Abstraction (H) 20 (13.3%) 59 (11.7%)\\n- Reference: While Republican Gov.\\nAsa Hutchinson was weighing an Arkansas religious freedom bill, Walmart voiced its opposition.\\nWalmart and other high-profile businesses are showing their support for gay and lesbian rights.\\n(unable to find support sentences)\\n\\nTable 2: Category breakdown of Facet-Aware Mappings (FAMs).\\nNearly 60% samples are of low abstraction while more than a quarter of samples contain noisy facets.\\n/ denotes the average number of support sentences."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Results on Neural Extractive Methods\\n\\nMethod              ROUGE-1     ROUGE-2     ROUGE-L     FAR\\nLead-3             41.9        19.6        34.8        50.6*\\nFastRL(E)          41.6        20.3        35.5        50.8\\nBanditSum          42.7        20.2        35.8        44.7\\nNeuSum             42.7        22.1        36.4        51.2*\\nUnifiedSum(E)     42.6        20.7        35.5        54.8\\nRefresh            42.8        20.3        39.3        51.3\\nOracle             53.8        32.1        48.1        84.8*\\n\\nHuman evaluation also favors UnifiedSum(E).\\n\\nTable 3: Performance comparison of extractive methods under ROUGE FI and Facet-Aware Recall (FAR).\\n\\nIn addition, FAR has higher Spearman's coefficient than ROUGE (0.457 vs 0.44).\\n\\nMethod              1st         2nd         3rd\\nLead-3             26.8%       46.3%       26.8%\\nNeuSum             29.3%       39.0%       31.7%\\nUnifiedSum(E)     37.8%       52.4%       9.8%\\n\\nTable 4: Proportions of system ranking in human evaluation.\\n\\nFAR shows better human correlation than ROUGE and prefers UnifiedSum(E)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Fine-grained Evaluation Method\\n\\nN L H L+H*\\n\\nOne can evaluate both extractive and abstractive methods using any metrics according to the noise / low-abstraction / high-abstraction categories.\\n- Lead-3 34.1 41.9 249 38.9\\n- FastRL(B) 335 41.6 312 398\\n- BanditSum 35.3 42.7 34.1 41.2\\n- NeuSum 34.9 42.7 30.7 40.6\\n- Refresh 35.7 428 32.2 40.9\\n- UnifiedSum(E) 34.2 42.6 31.3 40.6\\n- PG 32.6 40.6 27.5 38.2\\n\\n* Findings\\n- FastRL(E+A) 35.1 40.8 29.9 38.8\\n- UnifiedSum(E+A) 34.2 42.4 29.2 40.1\\n\\nNoisy samples hinder training and evaluation.\\n\\nTable 5: ROUGE-I!\\nFI of extractive and abstractive methods on noisy, low abstraction (L), high abstraction (H), and high quality (L +\\nH) samples."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "0.60 Evidence: Performance of O88 MertRefresh, FastRL(E), NeuSum are quite close to Lead-3 under FAR, but they generally have higher RR.\\n\\nImplication:\\n\\n(1) These methods might have learned to extract sentences that are not the support sentences.\\n\\n(2) They might extract redundant support sentences that happen to have token matches with other facets."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Automatic FAM Creation:\\n\\nSentence Regression\\n* Sentence Regression Method\\nPrecision Recall F1\\n* Typically transforms abstractive\\nLead-3 61.0 33.7 43.4 summaries to extractive labels (binary\\nGreedy ROUGE-I F1 58.2 30.8 40.3 labels indicating whether a sentence\\nshould be extracted) heuristically using\\nROUGE:\\n- ROUGE-LEI B89 53.1 66.5\\n- ROUGE-2 F1 36.6 52.3 65.2\\n- ROUGE-L Recall 89.3 53.7 67.1\\n- ROUGE-L Precision 77.2 45.5 57.2\\n- ROUGE-L F1 87.8 53.5 66.5\\n- ROUGE-AVG F1 90.0 53.9 67.4\\n\\n* We evaluate existing sentence regression methods to automatically create FAMs.\\n\\nTable 6: Performance of sentence regression approaches regarding support sentence discovery.\\nHigh precision and low recall are often observed."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Feasibility we are operating on Comparing FAR scores computed\\n- ROUGE-1: 705 37.1 333 720 714 60.0 884 943 86.7\\n- ROUGE-2: 11.0 25.7 200 434 65.7 46.7 884 65.7 60.0\\n- ROUGE-L: 340 543 467 375 429 200 623 429 467\\n- ROUGE-AVG: 496 43 46.7 461 657 467 832 82.9 733\\n\\nThe FAMs created by ROUGE-1/AVG F1 have high correlation with human annotations, estimated FAR scores by Pearson's r, Spearman's p, and Kendall's r.\\nN denotes the number of support groups.\\n\\nTraining a linear regression model Method\\n- FAR AutoFAR AutoFAR-L\\n- FAR vs. AutoFAR(-L) using different estimated FAR scores\\n- BanditSum: 447 448 44.7\\n- Lead-3: 50.6 51.3 45.6 97.6 (42.9)\\n- FastRL(E): 508 51.0 43.1\\n- NeuSum: 512 49.9 443 77.1 (54.3)\\n- Refresh: 513 517 46.2\\n- UnifiedSum(E): 548 54.5 46.9 60.0 (46.7)\\n\\nTable 8: FAR prediction via linear regression.\\nAutoFAR(-L) denotes the results on the human-annotated subset (entire CNN/Daily Mail dataset)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Takeaways & Future Work\\n\\nTakeaways:\\n- A facet-aware evaluation metric for extractive summarization\\n- Better human correlation while still being automatic\\n- A manually curated list of facet-aware mappings between support sentences and reference summaries on CNN/Daily Mail\\n- Data and tools for evaluation at https://github.com/morningmoni/FAR\\n\\nFuture Work:\\n- Better methods for automatic FAM creation (e.g., embedding-based methods)\\n- Evaluation on multi-document summarization and multiple reference summaries"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Thank you for your attention!"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 485,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "A wave of research on \"bias\" Recent vital work demonstrates that NLP systems exhibit \"bias\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "A wave of research on \"bias\"\\n\\nRecent vital work demonstrates that NLP systems exhibit \"bias.\"\\n\\nThis work struggles to define \"bias.\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "A wave of research on \"bias\"\\nRecent vital work demonstrates that NLP systems exhibit \"bias.\"\\nThis work struggles to define \"bias.\"\\nAs a field, we must be careful and precise about what we mean by \"bias.\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "We take careful stock of work on \"bias\" in NLP.\\n\\nWe survey 146 papers on \"bias\" in NLP, focusing on text.\\n\\nFor each paper, we categorize:\\n- its stated motivations\\n- its proposed techniques"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "We take careful stock of work on \"bias\" in NLP.\\n\\nWe survey 146 papers on \"bias\" in NLP, focusing on text. For each paper, we categorize:\\n- its stated motivations\\n- its proposed techniques\\n\\nFinding: papers often lack clear and consistent conceptualizations of \"bias\".\\n\\nAfterward: a potential path forward."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "The landscape of \"bias\" in NLP\\n- NLP task Papers\\n- Embeddings (type-level or contextualized)\\n- Coreference resolution\\n- Language modeling or dialogue generation\\n- Hate-speech detection\\n- Sentiment analysis\\n- Machine translation\\n- Tagging or parsing\\n- Surveys, frameworks, and meta-analyses\\n- Other"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "The landscape of \"bias\" in NLP task Papers\\n\\nEmbeddings (type-level or contextualized) 54\\n\\nCategory Motivation\\n\\nCoreference resolution 20\\n- Language modeling or dialogue generation 17\\n- Allocational harms 30\\n- Hate-speech detection 17\\n- Stereotyping 50\\n- Sentiment analysis 15\\n- Other representational harms 52\\n- Machine translation 8\\n- Questionable correlations 47\\n- Tagging or parsing 5\\n- Vague/unstated 23\\n- Surveys, frameworks, and meta-analyses 20\\n- Other 22"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "eprom: The landscape of \"bias\" in NLP NLP task Papers\\n- Papers Embeddings (type-level or contextualized) 54\\n- Category Motivation\\n- Coreference resolution 20\\n- Language modeling or dialogue generation 17\\n- Allocational harms 30\\n- Hate-speech detection 17\\n- Stereotyping 50\\n- Sentiment analysis 15\\n- Other representational harms 52\\n- F Tagging or parsing 5\\n- Vague/unstated 23\\n- Surveys, frameworks, and meta-analyses 20\\n- Surveys, frameworks, and 20\\n- Other 22 meta-analyses"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Leno\\n\\nThe landscape of \"bias\" in NLP\\n\\nNLP task Papers\\n- Embeddings (type-level or contextualized) 54\\n- Category Motivation\\n- Coreference resolution 20\\n- Language modeling or dialogue generation 17\\n- Allocational harms 30\\n- Hate-speech detection 17\\n- Stereotyping 50\\n- Sentiment analysis 15\\n- Other representational harms 52\\n- Machine translation 8\\n- Questionable correlations 47\\n- Tokenization 8\\n- Surveys, frameworks, and meta-analyses 20\\n- Other 22"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Papers' motivations may be vague or imprecise.\\n\\nBiased embeddings can perpetuate systematic biases in society, discriminate against different groups of users, and promote social injustice.\\n\\nBiased outputs or discriminatory behaviors might offend users or result in negative user experiences.\\n\\nBiased algorithms risk taking problematic actions, affecting important downstream applications such as hiring."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Papers sometimes give no normative reasoning.\\n\\nModels should not rely on demographic attributes expressed in the text to make predictions.\\n\\nModels that rely on demographic attributes in the text yield higher error rates."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Papers for the same task may conceptualize \"bias\" differently.\\nGender/racial \"bias\" sometimes looks at text written about different groups and text written by different groups."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Papers for the same task may conceptualize \"bias\" differently.\\n\\nGender/racial \"bias\" sometimes looks at text written about different groups, and text written by different groups.\\n\\nWord embeddings papers have been motivated by hiring/résumé filtering, stereotyping, under-representation/under-recognition of women, and more, but generally all actually measure stereotyping."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Papers' motivations and techniques may not be well-matched. Many papers are motivated by allocational harms\\n- hiring, credit, etc. ... but rarely ever measure them."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Papers' motivations and techniques may not be well-matched. Many papers are motivated by allocational harms\\n- hiring, credit, etc. ... but rarely ever measure them.\\nTherefore, we still know little about what allocational harms NLP systems give rise to."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "A potential path forward"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Recommendation 1: Analyze language and social hierarchies together\\n- Social hierarchies: those resulting from unjust distributions of resources and power"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Recommendation 1: Analyze language and social hierarchies together\\n- Social hierarchies: those resulting from unjust distributions of resources and power\\n- A vast literature outside NLP shows us that language plays a role in maintaining social hierarchies"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Recommendation 1: Analyze language and social hierarchies together\\n- Social hierarchies: those resulting from unjust distributions of resources and power\\n- A vast literature outside NLP shows us that language plays a role in maintaining social hierarchies\\n- Language names social groups and transmits stereotypes [Maass 1999]\\n- Language choices shape narratives and discourses [Rosa 2019]\\n- Language ideologies enable linguistic discrimination and justify existing social hierarchies (Lippi-Green 2012, Rosa and Flores 2017, Craft 2020)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Recommendation 1: Analyze language and social hierarchies together\\n- Social hierarchies: those resulting from unjust distributions of resources and power.\\n- A vast literature outside NLP shows us that language plays a role in maintaining social hierarchies.\\n- Ask: How are social hierarchies, language ideologies, and NLP systems coproduced? [Benjamin 2020]"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Recommendation 2: Articulate conceptualizations of \"bias\"\\n- Provide explicit statements of why system behaviors that are described as \"bias\" are harmful, in what ways, and to whom."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Recommendation 2: Articulate conceptualizations of \"bias\"\\n- Provide explicit statements of why system behaviors that are described as \"bias\" are harmful, in what ways, and to whom.\\n- Be explicit about normative reasoning."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Recommendation 2: Articulate conceptualizations of \"bias\"\\n- Provide explicit statements of why system behaviors that are described as \"bias\" are harmful, in what ways, and to whom\\n- Be explicit about normative reasoning\\n\\nExplicit conceptualizations and normative reasoning:\\n- Ensure that motivations and quantitative techniques are well-matched\\n- Enable open community discussions of inherently normative questions\\n- Enable reflection on what researchers identify as \"bias\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Recommendation 3: Examine language use in practice\\n\\nLanguage is necessarily situated, and different social groups have different experiences"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Recommendation 3: Examine language use in practice\\n- Language is necessarily situated, and different social groups have different experiences.\\n- Center work on the lived experiences of members of communities affected by NLP systems."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "Recommendation 3: Examine language use in practice\\n- Language is necessarily situated, and different social groups have different experiences.\\n- Center work on the lived experiences of members of communities affected by NLP systems.\\n- Interrogate the power relations between technologists and affected communities."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "Conclusion\\n\\nPapers often lack clear, consistent conceptualizations of \"bias\".\\nMotivations and techniques may not always be well-matched.\\n\\nRecommendations:\\n- Reorient around relationships between social hierarchies, language ideologies, and technology.\\n- Articulate conceptualizations of \"bias\", including normative reasoning.\\n- Examine language use in practice by centering communities."
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 165,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Neural Machine Translation (NMT)\\nNMT framework il aimait\\nmanager Embed 0 0 Null he loved to eat\\nBlunsom et al. (2013)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Multi-Domain Neural Machine Translation\\n\\nMotivation:\\n- NMT is data hungry.\\n- Data scarcity in certain domains (e.g., medical, laws domains).\\n\\nSolution:\\n- Combine training data from multiple domains and benefit NMT model by sharing knowledge (Haddow et al. 2012)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Multi-Domain Neural Machine Translation\\n\\nChallenges:\\n- Enforcing knowledge sharing lacks adaptivity to each individual domain.\\n\\nExample:\\n- Failure to handle word-level ambiguity across domains: The word \"articles\" has different meanings in laws and media domains.\\nLaws \"Article 37 The freedom of marriage ...\"(a clause of a legal document) \"S=+ +R RNA.\" Media \"..\\nworking on an article about the poems ...\"(a piece of writing) *.. S-BAF RHE...\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Multi-Domain Neural Machine Translation\\n\\nQuestion: How to adaptively capture domain-shared and domain-specific knowledge to improve multi-domain NMT?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Transformer Models (Vaswani et al.\\n2017)\\n\\nFeedforward Network based Encoder-Decoder\\n\\nEncoded Inputs\\n\\nOutputs\\n\\nEncoder\\n\\nDecoder\\n\\nModule\\n\\nInputs\\n\\nEncoded Inputs\\n\\nOutputs (shifted right)\\n\\nFull Attention\\n\\nMasked Attention"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Transformer Models (Vaswani et al.\\n2017)\\n* Dot-Product Attention: He love to eat\\nValue\\nQuery\\nKey\\nAttention(Q,V)\\nLinear Combination\\nOutput"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Transformer Models (Vaswani et al.\\n2017)\\n\\nMulti-Head Attention: Point-Wise Linear Concat"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Transformer Models (Vaswani et al.\\n2017)\\n\\nBenefits:\\n- High Efficiency (Parallel and Feedforward Structures)\\n- Capture Long-term Dependency (Attention Module)\\n- Enable Deeper Representation Learning"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Our Proposed Method"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Word-Level Domain Proportion"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Word-Level Domain Proportion\\n\\nLaw Domain: 30%\\nMedia Domain: 70%\\nLinear article"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Word-Level Domain Proportion D(x) = (1\\n- €)\\n- softmax(Rx) + 2* € (0,1): Smoothing parameter.\\n* k: Number of domains\\n* x: Word vector.\\n* R: Weight matrix of the softmax layer.\\n* Law Domain:\\n* Media Domain:\\n* a Linear article"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Word Level Adaptive Domain Mixing\\n\\nMixing the point-wise linear transformations:\\nXout = Wx binaries Asi Point-wise Linear"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Word Level Adaptive Domain Mixing\\n\\nMixing the point-wise linear transformations:\\n\\nXour = Wx Xin =* W;\\n\\nWeight matrix for the j-th domain."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Word Level Adaptive Domain Mixing\\n\\nMixing the point-wise linear transformations:\\n\\( X_{our} = W X_{in} = X_{our} = L_{j} n_{1} D_{j}(X_{in}) \\)\\nWeight matrix for the j-th domain.\\n\\( D_{j}(X_{in}) \\): The proportion of \\( X_{in} \\) for the j-th domain.\\n\\nLineararice | ~ Law Domain: Media\\nDomain: 30% 70%\\nC\\n- article\\nSesiuieys article feelers\\nPoint-wise Linear\\nCcarticle"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Word Level Adaptive Domain Mixing\\n\\nMixing the point-wise linear transformations:\\n\\nkXour = WXin = Xour = Σ dj=1 Dj(Xin) Wj\\n- Wj: Weight matrix for the j-th domain.\\n- Dj(Xin): The proportion of Xin for the j-th domain.\\n\\nWord Level Mixing"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Layer-wise Domain Mixing\\n\\nWord embedding in deep layer has context information.\\n\\nGetting context information.\\n\\nSame Domain.\\n\\nProportion.\\n\\nInput = Embedding."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Domain proportion is different at each attention layer.\\n\\nDomain = mac encoder\\n\\nOutput Resi ham Domain Feed\\n\\nDomain Nx roi Same a Recut am\\n\\nDomain, Multihead Proportion\\n\\nSoftmax Linear Set Input\\n\\nOutput ppt Embedding Lo Embedding\\n\\nInputs Outputs (shifted right)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "Training min L = Leen(H, F,\\nD) + Linix(H, F,\\nD) * H: Encoder module.\\nF: Decoder module.\\nD: Domain proportion.\\nLgen: Cross entropy loss for translation.\\nLmix: Cross entropy loss over the smoothed domain labels of words."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Domain Proportion of One Example En2De: TED Domain (white) vs. Medical Domain (black)\\n\\nShifted Output\\n\\nInput\\n- Different Word has different domain proportion\\n- We can identify the domain easily at the deep layers thanks to the contextual information"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Histograms of the Domain Proportions\\n\\nWithin each histogram, 0 means Medical domain, and 1 means TED domain.\\n\\nLayer\\n- l 2 3 4 5 6 Encoder\\n- Decoder"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Keep borrowing\\n\\nTop Layers: The meaning of knowledge from the medical domain is well understood and shared across domains.\\n\\nKnowledge Sharing in Layer 2: Phrase Layer 1; Single word\\n\\nBottom and one such phenomenon that is well-focused on recently is sound, simple.\\n\\nComplicated More Phrase, Word common in TED."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Keep borrowing Top Layers: The meaning of knowledge is well understood and has little semantic medical domain do not need to borrow and is shared across domains.\\n\\nKnowledge Sharing in Layer 2: Phrase Layer 1; Single word Top TED.\\n\\nBorrowing little knowledge\\n- Bottom. One such that we focused on recently is sound. Simple, complicated more Phrase Word Word common in TED."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Validation Perplexity * English to German 100; it --- News + TED 80} <= * MTL 2 f <-> Advi 60 4 -<- PARLa ', = WDC w/ WL 5 ao 'serspeszecc.\\n- Mixing: Encoderom SS\\n- Mixing: E/DC\\n0 10 20 30 40 50 60\\n\\nEpoches *\\n- Converge Faster and Better *\\n- Avoids Plateau"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Testing BLEU Scores English to German\\n\\nMethod                       News   TED*    WDC: (Zeng et al,\\n2018) Direct Training. No   26.09   6.15*\\nWL: Weighted Label (Zeng et al,\\n2018)                       TED '490   20.00*\\nMTL: Multi-Task Learning                                      SS une bie See*\\nAdvL: Adversarial Learning                                     ie z 2 2 a*\\nPAdvL: Partially Adversarial Learning                           pat Wi ue ae+ eg\\n\\nOur Domain Mixing Methods\\n\\nEncoder                        27.78   30.30\\nEncoder + WL                27.67   30.11\\nE/DC                             27.58   30.33\\nE/DC + WL                   27.55   30.22"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "Testing BLEU Scores\\n\\nEnglish to French\\n\\nChinese to English\\n- Wethod TED: 28.22\\n- Laws: 51.98 | 380 | 238 | 264\\n- Medical: 7.03 | 53.73\\n- News: 688 | 31.99 | 8.12 | 4.17\\n- Medical + TED: 39.21 | 53.40\\n- Speech: 3.33 | 490 | 18.63 | 3.08\\n\\nEmbedding based Methods\\n- Thesis: 590 | 555 | 4.77 | 11.06\\n- MTL: 30.14 | 53.37\\n- Mixed: 48.87 | 26.92 | 16.38 | 12.09\\n- AdvL: 39.54 | 53.46\\n\\nEmbedding based Methods\\n- PAdvL: 30.56 | 53.23\\n- MTL: 49.14 | 27.15 | 16.34 | 11.80\\n- WoC + WL: 30.79 | 53.85\\n- AdvL: 48.93 | 26.51 | 16.18 | 12.08\\n\\nOur Domain Mixing Methods\\n- PAdvL: 48.72 | 27.07 | 15.93 | 12.23\\n- Encoder: 40.30\\n- woc + WL: 42.16 | 25.81 | 15.29 | 10.14\\n- Encoder + WL: 40.43 | 54.14\\n\\nOur Domain Mixing Methods\\n- E/DC: 40.52 | 54.28\\n- Encoder: 50.21 | 27.94 | 16.85 | 12.03\\n- E/DC + WL: 40.60 | 54.39\\n- Encoder + WL: 50.11 | 27.48 | 16.79 | 11.93\\n- OO E/DC: 50.64 | 28.48 | 17.41 | 11.71\\n- E/DC + WL: 50.04 | 28.17 | 17.60 | 11.59"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Github: https://github.com/HMJiangGatech/MD_NMT_mixing_model"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 749,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Fine-grained entity typing OntoNotes BBN Ge FR/ ge l Athlete)\\n- event) (food)\\n- Z(]) BIR) wea) 2AOS) | all\\n5) | 3 8a) ) BB) BIEN 2\\n3) (8g} 8+ R Weischedel, A Brunstein (2005): BBN pronoun conference and entity type corpus. LDC.\\nD Gillick, N Lazic, K Ganchey, J Kirchner, D Huynh (2014): Context-dependent fine-grained entity type tagging. ArXiv."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Fine-grained entity typing\\n\\nHe is interred at Forest Lawn Memorial Park in Hollywood Hills, Los Angeles, CA.\\n\\nMention representation."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Amulti-label classification problem?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "A multi-label classification problem?\\nInput: an entity mention span x."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "A multi-label classification problem?\\n\\nInput: an entity mention span x\\n\\nOutput: a set of types Y\\n\\nCommonly modeled as a set of binary linear classifiers."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "A multi-label classification problem?\\n\\nInput: an entity mention span x\\n\\nOutput: a set of types Y\\n\\nCommonly modeled as a set of binary linear classifiers x ∈ R^t, y ∈ R^n (Each type y has a learned embedding y), Y = {y | x · y > 0}"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "A multi-label classification problem?\\n\\nInput: an entity mention span x\\n\\nOutput: a set of types Y\\n\\nCommonly modeled as a set of binary linear classifiers x ∈ R?. y ∈ R (Each type y has a learned embedding y), Y = {y | x\\n- y > 0}\\n\\nIgnored tree structure of the ontology!\\n\\nSupertypes must be present if a subtype is predicted"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "LSS Mention representation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "LSS Mention representation ER D00e : Ee || ee"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Mention representation him = MaxPool(h,\\n- + -, h,) Guar aeEREDfa"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Mention representation m = MaxPool(hy, ...,\\nh) c = Attention(m, hy... -1-41, ...n) small size Pretrained model (ELMo)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Mention representation F(x,y) = x + y + m = MaxPool(by,\\nh) c = Attention(m, hy, ... 11,741.) a da; Pretrained model (ELMo) re aise"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "tOModeling hierarchy"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Modeling hierarchy. Supertypes must be present if a subtype is predicted."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Modeling hierarchy. Supertypes must be present if a subtype is predicted.\\nWe would like to predict in a coarse-to-fine manner."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Modeling hierarchy\\n\\nSupertypes must be present if a subtype is predicted.\\n\\nWe would like to predict in a coarse-to-fine manner how.\\n\\nFirst, we predict whether x is a person (person) (other)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Modeling hierarchy\\n\\nSupertypes must be present if a subtype is predicted. We would like to predict in a coarse-to-fine manner.\\n\\nFirst, we predict whether x is a person (person) (other).\\n\\nThen, we predict whether x is an artist (artist)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Modeling hierarchy\\n\\nSupertypes must be present if a subtype is predicted. We would like to predict in a coarse-to-fine manner.\\n- First, we predict whether x is a person (other).\\n- Then, we predict whether x is an artist.\\n- Lastly, we predict whether x is an actor."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "LSS Ranking siblings on type hierarchyomyf"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "sas Ranking siblings on type hierarchy\\n\\nOn each level, positive type ranks above negative type"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Ranking siblings on type hierarchy\\nOn each level, positive type ranks above negative type\\nThreshold (multi-label)?\\nIf we set threshold to 0, we fall back to classification"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Ranking siblings on type hierarchy\\n\\nOn each level, positive type ranks above negative type\\n\\nF(x,\\ny) + F(y,\\nx)\\n\\nThreshold (multi-label)?\\n\\nIf we set threshold to 0, we fall back to classification\\n\\nPositive type ranks above parent"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "Ranking siblings on type hierarchy\\n\\nOn each level, positive type ranks above negative type.\\n\\nThreshold (multi-label)?\\n\\nIf we set the threshold to 0, we fall back to classification.\\n\\nParent:\\n\\nPositive type ranks above parent.\\n\\nParent ranks above negative types."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "Ranking siblings on type hierarchy\\n\\nOn each level, positive type ranks above negative type.\\n\\nThreshold (multi-label)?\\n\\nIf we set threshold to 0, we fall back to classification.\\n\\nParent!\\n\\nPositive type ranks above parent.\\n\\nParent ranks above negative types:\\n\\nperson < entity < other\\n\\nyes\\n\\nartist < person < athlete\\n\\nactor < artist < author"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "a... Sa Multi-level margins person artist athlete actor author * we \"agi (alifa"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "Le Multi-level margins person artist athlete person ~ entity < other net ath artist < person < athlete actor ~ < artist < author i'ah (=a\"agi (ali- /a»"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "Multi-level margins"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "Multi-level margins\\nF(x,y) > F(x,9)\\nZi vF(x,9) > F(x,y')\\n(1-a)ey\\n- /a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          "Multi-level margins\\nF(x,y) > F(x,9)\\nAOS F(x,9) > F(x,y')\\nSeSS+ Controls precision/recall tradeoff\\nFi[oS\\n- F(x,\\ny) + F\\nD+ a[(1-a)&\\n- F(x,\\n9) + F(x)\\nye yeaf = aMae ale\\n- /a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 30,
        "texts": [
          "Type hierarchyomyf &re 8 @) «+.e/a»"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 31,
        "texts": [
          "Type hierarchy actor <: artist omg fe «Byte BYR) ite/a»"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 32,
        "texts": [
          "Type hierarchy\\nactor <: artist\\nartist <: person\\n=/'actor\\n!<: athlete\\nbergon) the «a)\\na) se\\nBy\\nfa)\\nesi-\\n/a»"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 33,
        "texts": [
          "ReType hierarchy\\n- actor\\n- artists\\n- person\\n- athlete\\n- author"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 34,
        "texts": [
          "Subtyping relation constraint op-"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 35,
        "texts": [
          "Subtyping relation constraint\\n\\nLearn the relation \"is subtype of\" (<:)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 36,
        "texts": [
          "Subtyping relation constraint omeX+ Learn the relation \"is subtype of\" (<: ) 8r(y.y') =y\"Ry!?\\nGaris) @es) --- ven) (aed) --.j oe RR- /a>"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 37,
        "texts": [
          "Subtyping relation constraint\\nomoN+\\n\\nLearn the relation \"is subtype of\" (<:) ir(y,y') = y Ry'? (ihiets) --- vend) (food) ---\\n\\nAntisymmetric and transitive!\\ni) riei- /a»"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 38,
        "texts": [
          "Subtyping relation constraint\\n\\nomeS+ Learn the relation \"is subtype of\" (<: )\\n\\nAntisymmetric and transitive!\\n\\nComplEx (Tecullateval, 2016): q ~ -- /a»"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 39,
        "texts": [
          "Subtyping relation constraint comes/+\\nLearn the relation \"is subtype of\" (<: ) Person) iriyy') =y'Ry'? Gthiete)\\n--- event) (food)\\nAntisymmetric and transitive!\\n\\nComplEx (Trouillon et al., 2016): q Eir(y,y!) = Re(r-(y Oy') yec?*+\\nLearning objective: e/a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 40,
        "texts": [
          "Subtyping relation constraint\\n\\nLearn the relation \"is subtype of\" (<: ) person)\\n\\nAntisymmetric and transitive!\\ni) ComplEx (Trouillon et al., 2016): qr(y,y') = Re(r-(y O y') yec?\\n\\nLearning objective: r(y,g) > 0, r(y,5) < 0"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 41,
        "texts": [
          "Subtyping relation constraint\\n\\nLearn the relation \"is subtype of\" (<: ) Person) ir(y,y') = y'Ry'? fathiste) --- event) (food)\\n\\nAntisymmetric and transitive!\\ni) ai fe)\\n\\nComplEx (Trouillon et al., 2016): ar(y,y!) = Re(r-(y Oy') yec?\\n\\nLearning objective: ys r(y,g) > 0 yey r(y,y') < 0 ge r(y,5!) < 0 e/a"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 42,
        "texts": [
          "Coarse-to-fine decoding\\n\\nHe is interred at Forest Lawn Memorial Park in Hollywood Hills, Los Angeles, CA.\\n\\nMention representation"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 43,
        "texts": [
          "Datasets\\n\\nDataset         Train    Dev      Test     #Levels     #Types     Multi-path?\\nAIDA            2,492    558      1,383    3           187        single-path\\n\\nBBN             84,078   2,000    13,766   2           56         multi-path\\n\\nOntoNotes       251,039  2,202    8,963    3           89         multi-path\\n\\nFIGER           2,000,000 10,000  563      2           113        multi-path"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 44,
        "texts": [
          "WH Ren et al. (2016)\\nAbhishek et al. (2017)\\nBE Ren et al. (2016)\\nAbhishek et al. (2017)\\nWH Zhang et al. (2018)\\nMM Lin and Ji (2019)\\nOurs\\nWH Zhang et al. (2018)\\nMM Lin and Ji (2019)\\nOurs\\n\\n80\\n90\\n80\\n70\\n70\\n60\\n60\\n50\\n50\\n40\\n40\\n\\nBBN OntoNotes FIGER\\nBBN OntoNotes FIGER\\nStrict Accuracy Micro-F1-"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 45,
        "texts": [
          "Summary: Fine-grained entity typer that takes the hierarchy of the ontology into account.\\nMulti-level learning to rank.\\nAccompanying coarse-to-fine decoder.\\nResults in state-of-the-art results across datasets."
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 418,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Bias in Coreference Resolution\\n\\nThe task of determining which references in text resolve to the same real-world entity.\\nThe nurse and the doctor visited her mother."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Bias in Coreference Resolution\\n\\nThe task of determining which references in text resolve to the same real-world entity.\\nThe nurse and the doctor visited her mother. The nurse and the doctor visited her mother."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "What is Gender?\\nBinary\\nFolk notions of gender\\n- Gender is NOT Immutable\\nAdapted from Keyes (2018). In perfect correspondence to gendered linguistic forms"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "The concept of Gender is complex\\n\\nSociological Gender\\n\\nLinguistic Gender\\n\\n\"Mädchen\"\\n\\nInternal experience of gender\\n\\nGrammatical gender in German\\n\\nExpression of gender to the world\\n\\nReferential gender\\n- he, she, etc.\\n\\nSocial gender\\n\\nLexical gender\\n- mother, uncle, long hair, etc.\\n\\nSir, etc.\\n\\n(Ackerman, 2019; Kramarae and Treichler, 1985; West and Zimmerman, 1987; Butler, 1990; Risman, 2009; Serano, 2007)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "The concept of gender is complex.\\n- Sociological gender\\n- Linguistic gender: \"Mädchen\"\\n- Internal experience of gender\\n- Grammatical gender in German\\n- Expression of gender to the world\\n- Referential gender: he, she, etc.\\n- Social gender\\n- Lexical gender: mother, uncle, long hair, etc.\\n\\nWe should take care about what notions of gender are being utilized and when.\\n(Ackerman, 2019; Kramarae and Treichler, 1985; West and Zimmerman, 1987; Butler, 1990; Risman, 2009; Serano, 2007)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Ablation Study\\n- Maybe Ambiguous Pronoun Dataset\\n\\nMrs. Rebekah Johnson Bobbitt was the younger sister of Lyndon B. Johnson, 36th President of the United States.\\nBorn in 1910 in Stonewall, Texas, she worked in the cataloging department of the Library of Congress in the 1930s before her brother entered politics.\\n\\nInspired by the GAP dataset (Webster et al., 2018)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Ablation Study\\n- Maybe Ambiguous Pronoun Dataset\\n\\nMrs. Rebelee Lennon Azaneit was the younger sister of Kyndern-By M. Boot! T.\\nSchneider Jonson; 36th President of the United States.\\nBorn in 1910 in Stonewall, Texas, she worked in the cataloging department of the Library of Congress in the 1930s before her brother entered politics.\\n\\nOrig\\n- Pro\\n- Name Inspired by the GAP dataset (Webster et al., 2018)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "MAP Ablation Study Result\\n\\n1.0e@ ORIG v.s.\\nPro: 0.9o + £ FT©\\n\\nAnnotators rely heavily on social gender references.\\n\\n0.5e@ ORIG v.s.\\n- Name &\\n- Sem &\\n- Addr:\\n\\nName is another significant cue\\n\\nAblation Fields"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Bias in Model Definitions\\n\\nGender-inclusive Coreference Dataset"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Bias in Model Definitions\\n\\nGender-inclusive Coreference Dataset\\n- Manually annotated naturally occurring and gender-inclusive coreference dataset.\\n- Consists of 95 documents from:\\n- Articles from Wikipedia about people with non-binary gender identities\\n- Articles from LGBTQ periodicals\\n- Fan-fiction stories from Archive Of Our Own\\n\\n\"Kind of a breeze, wasn't it?\" Evans' voice echoes in the arched hall and Cronal's shoulders jump, their frame still a tense and anxious mess.\\n\\n\"Oh,\" they sigh, \"I suppose so.\\nIt wasn't necessarily hard.\" Cronal answers, putting forth a vaguely forced smile, smiling with the assumed purpose of making Soul comfortable with the interaction, a defense mechanism.\\n\\n\"I guess, for a final, it was easier than I expected... everyone made it sound like it'd be difficult.\"\\n\\n\"If by everyone, you mean Black Star, then yeah,\" Soul chuckles, \"he doesn't really do well on 'em... bad test-taker.\""
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Coref system Performance on GiCoref\\n\\nMeasure Names\\n- Al2 HE\\n- sfdD\\n- SfdN\\n- SfdS\\n- wm\\n- Precision\\n- Recall\\n\\n1.0 arton Coreference Systems:\\n0.6 © AllenNLP system (Gardner et al.,\\n2017)\\n0.4 ° Hugging face (Wolf,\\n2017)\\n0.0 ° Stanford deterministic system (Raghunathan et al.,\\n2010)\\n- © Stanford statistical system (Clark and Manning,\\n2015)\\n0.2 © Stanford neural system (Clark and Manning,\\n2016)\\n\\nEvaluation metric: LEA (Moosavi and Strube,\\n2016)\\nBest F1 score of 34%; SfdN achieves F1 60% on CoNLL-2012"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Conclusion\\n\\nDataset + datasheet: https://github.com/TristaCao/into_inclusivecoref\\n\\nPrimary Limitations:\\n- English only\\n- No grammatical gender\\n- Generally uses a Western conceptualization of gender.\\n\\nMain Takeaways:\\n- The concept of gender is complex. We should take care about what notions of gender are being utilized and when.\\n- Beware that any annotation of a concept as complex as gender is going to reflect the positionality of the annotator.\\n- Bias can enter at many stages: data collection, annotation, and model development."
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 618,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Motivation:\\n\\nBilingual lexicon induction: useful for machine translation with limited parallel data.\\n\\nCross-lingual word embeddings: simple cross-lingual transfer learning for arbitrary NLP tasks.\\n\\nMost BLI systems rely on expanding the initial seed dictionary of aligned word pairs via self-learning.\\n\\nIdea: introduce a classifier that improves the procedure at several points."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Motivation\\n- why classification?\\n1. Enables integration of heterogeneous features into the self-learning procedure\\n\\nTranslation pair (En\\n- Hr) --> Feature vector\\n\\nAligned cosine: 0.69\\n\\nlove\\n- ljubav\\n\\nEdit distance: 5\\n\\nSubword overlap: 0.141\\n\\nFrequency of wl: 0.00042"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Motivation\\n- why classification?\\n2.\\nReduces noise in new translation pairs added to the seed dictionary during self-learning by filtering bad candidates as\\n3.\\nFraming BLI as a classification problem can yield further benefits (Irvine and Callison-Burch, 2017; Heyman et al., 2017)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "CLAssyMapP workflow\\n- unaligned monolingual embeddings\\n- the initial seed dictionary (set of word translation pairs)\\n\\nInitial Align Embeddings using D\\n- W1, W2\\n\\nInitial Train Classifier C on D\\n- N iterations\\n- Align Embeddings using D\\n- W1, W2\\n\\nOutput: C, W1, W2"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Some Technical Details\\n\\nThe AlignEmbeddings step aligns two monolingual embedding spaces using a dictionary of translation pairs.\\nWe used VECMAP (Artetxe et al.,\\n2018) as a robust, high-performing system.\\n\\nThe TrainClassifier step uses an arbitrary classifier to classify a translation pair as a correct or incorrect candidate.\\nWe used an MLP with a single hidden layer; the scores of the word pairs are confidence scores of the classifier."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Some More Technical Details\\n\\nSampling examples for the classifier\\n\\nPositive examples are pairs from the dictionary; for each positive example, two negative examples are generated:\\n1. an orthographically similar pair\\n2. a pair with high cosine similarity in the aligned space\\n\\nFeatures: edit distance, cosine similarity in aligned space, embeddings PCA, character n-gram overlap, specific character n-grams, subword similarity, word frequencies"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Applying CLASsyMApP for BLI\\n- two variants\\n\\na Using output embeddings for each test word, its neighbours are sorted by cosine similarity of output embeddings W1, W2.\\nThis evaluates the quality of aligned embeddings, which can be applied to many other tasks.\\n\\nb Using reranking, top neighbours are retrieved as in the previous approach, but are augmented by orthographically similar neighbours.\\nThe classifier reranks the top of the augmented result list for better BLI performance, but is limited in application to BLI."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Main Results\\n\\nVecMap (sup)\\n\\nVecMap    ClassyMap\\n\\nTR-HR\\n- 030  .160/.171  .200/.227\\n\\nDE-TR\\n- 050  .207/.203  .221/.268\\n\\nTR-FI  034  .200/.176  .217/.235\\n\\nTR-RU  .028  123/152  .162/.203\\n\\nFI-HR  049  .249/.195  .252/.278\\n\\nDE-HR  058  .229/.206  .246/.268\\n\\nDE-RU  At  .193/.208  .212/.239\\n\\nEN-X  Are  357/.325  .375/.401\\n\\nNo EN  089  310/.286  .322/.353\\n\\nAll  2,  -321/.296  .334/.365\\n\\nTable: P@1 BLI scores for a selection of language pairs.\\nThe a/b score format denotes using only output embeddings (a), or reranking (b)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Additional Experiments\\n\\nSeed dictionary size: we ran experiments with sizes 500, 1k, 3k, 5k.\\nResults expectedly generally better with larger dictionaries.\\nCLASSYMAP was still useful even for the largest dictionaries.\\n\\nNumber of iterations:\\n(a) Without reranking\\n(b) With reranking\\n\\nFigure: BLI performance (P@1) of CLASSYMAP for varying numbers of self-learning iterations."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Conclusion: We confirm the usefulness of classifiers both as part of self-learning and as part of the final word retrieval for BLI.\\n\\nWide range of questions for future research:\\n- more sophisticated classifiers/features (e.g. character level transformers)\\n- finer linguistic analysis on the importance of disparate features and model parameters over different language pairs\\n- considering alternative non-linear methods that induce bilingual spaces (Glavaš and Vulic, ACL 2020; Mohiuddin and Joty, 2020)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "come cesses giyabonga Saja Saiwe ankeetetls, lesekkir ederim s 8 thanl-wniilaclas Bz! : q ings-s! tzekue .\\ncana De onthe ian go Tall Malti aga oh tS top un kp ;obrigado g S50 om neZab o Merc"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 216,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "REAL WORLD DATA FROM GROUNDING FROM IMPROVE ASR RESULTS.\\nSEGWAY TO NEW, MULTIPLE SOURCES ARBITRARY VISUAL CAPABILITIES FOR SMART CONTEXT DEVICES (NOT LIPREADING)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "The task\\nWe use How? dataset, a collection of instructional YouTube videos\\nWe use the 300 hour set (184949 samples, 297 hours)\\nYouTube subtitles are provided as ground truths\\nVideo lengths from 0.1 to 110 seconds\\nAudio features: 40 Filterbank + 3 pitch\\nVideo features: 2048D ResNext based feature vectors"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Character or subword prediction\\nA * Transformer architecture\\n[ Encoder Layer * Use encoder for audio\\n- TM representation\\n---+ Use decoder to generate\\n[ Decoder Layer i: characters or subwords\\nes Down-sampling\\n@ © * 6 encoder layers\\n© Character or Audio subword clo memes\\n* 4 decoder layers transcription"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Challenge scaled dot-product very memory intensive -> O((sequence length)²)\\n\\nDataset contains inputs with lengths varying from 12 to 11000 frames.\\n\\nSimple Solutions:\\n- Filter input under a maximum length\\n- Throw away over 50h of data\\n- Frame stacking (e.g. concatenate consecutive frames)\\n- Chunk inputs to n=15 seconds"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Audio-only Results\\n\\nLevel Parameters\\n\\nData preprocessing\\n\\nsamples hours WER\\n\\nCharacter LE=6, LD=4, H=6, d=480 >.5 & <15 sec 173048 249 33\\n\\nBeam search (5 beams)\\n\\nSubword LE=6, LD=4, H=6, d=480 >.5 & <15 sec 173048 249 29.7\\n\\nBeam search (5 beams)\\n\\nCharacter LE=6, LD=4, H=6, d=480 Chunk data to 15 sec 194500 294 32.3\\n\\nBeam search (5 beams)\\n\\nSubword LE=6, LD=4, H=6, d=480 Chunk data to 15 sec 194500 294 29.9\\n\\nBeam search (5 beams)\\n\\nCharacter LE=6, LD=4, H=6, d=480 Stack 4 consecutive frames 183987 296 28.3\\n\\nBeam search (5 beams)\\n\\nSubword LE=6, LD=4, H=6, d=480 Stack 4 consecutive frames 183987 296 26.1\\n\\nBeam search (5 beams)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Multiresolution learning L = ¥ * Lsupwora + 1\\n-\\nY) * \"character Character] Subword prediction] prediction A * Different mistakes on character and subword level prediction\\n\\nEncoder Layer | Ff * Use multitask learning to combine SSx character and subword loss Mx\\n\\nDecoder Layer. * Character level recognition as a fine- 'as ierained regularizer\\n\\n* Down-sampling & é eo a©\\n\\n* Subword recognition is the main task\\n\\nCharacter © Subword Rudlofemes Transcription\\n\\nTranscription Tied decoder for character and subwords"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Audio-only Results\\n\\nLevel Parameters\\nData preprocessing\\nsamples\\nhours\\nWER\\n\\nCharacter\\nLE=6, LD=4, H=6, d=480\\n>.5 & <15 sec\\n173048\\n249\\n33\\n\\nBeam search (5 beams)\\n\\nSubword\\nLE=6, LD=4, H=6, d=480\\n>.5 & <15 sec\\n173048\\n249\\n29.7\\n\\nBeam search (5 beams)\\n\\nCharacter\\nLE=6, LD=4, H=6, d=480\\nChunk data to 15 sec\\n194500\\n294\\n32.3\\n\\nBeam search (5 beams)\\n\\nSubword\\nLE=6, LD=4, H=6, d=480\\nChunk data to 15 sec\\n194500\\n294\\n29.9\\n\\nBeam search (5 beams)\\n\\nCharacter\\nLE=6, LD=4, H=6, d=480\\nStack 4 consecutive frames\\n183987\\n296\\n28.3\\n\\nBeam search (5 beams)\\n\\nSubword\\nLE=6, LD=4, H=6, d=480\\nStack 4 consecutive frames\\n183987\\n296\\n26.1\\n\\nBeam search (5 beams)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Audio-visual fusion\\nAttention-based fusion\\nIntuition: Use attention to emphasize important cross-modal information\\nImage features\\n2048D features extracted from a 3D ResNeXt-101 architecture trained on action detection (Kinetics dataset)\\nWe also experimented with 2048D aggregated bounding box features from a Faster-RCNN"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Attention-based Fusion\\nFusion = a | VS 5 Baa 2 eee A2 8 EAE)\\nCharacter Subword & 2 os es es | ay prediction\\nprediction os Encoder Layer\\naM ecoder Layer\\n| 1\\n- a i | Decoder Layer ie a (atest)\\nCharacter SubwordTiec ela\\nTranscription\\nTranscription Audio frames"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Results\\n\\nLevel Video features Fusion system Audio preprocessing WER\\n- over audio Subword\\n- None (audio-only) Frame stacking 26.1\\n- Subword ResNext Attention-based Frame stacking 25.0 3.45%\\n- Subword Faster RCNN Attention-based Frame stacking 24.1 7.66%\\n\\nLevel Video features Fusion system Audio preprocessing WER\\n- over audio Multiresolution & None (audio-only) Frame stacking 213\\n- Multiresolution\\n- Faster RCNN Attention-based Frame stacking 22.3 ~4.69%\\n- Multiresolution ResNext Attention-based Frame stacking 20.5 3.76%\\n\\n(Caglayan et al,\\n2019) WER\\n- over audio LSTM (audio only) 19.2\\n- LSTM (audio-video) 18.4 3.13%"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Conclusions\\n\\nMultiresolution training can significantly improve ASR results and speed up subword ASR convergence.\\n\\nWhy?: Character level task acts as a regularizer on subword prediction.\\n\\nVisual information improves ASR results.\\n\\nFeatures from actions better than objects.\\n\\nSignificant contribution of visual information (a ~ 0.38)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Future Work\\n- Limit aggressive input preprocessing\\n- Experiment with sparse transformer variants (e.g. Reformer)\\n- More elaborate attention-based fusion schemes\\n- Investigate the regularizing effects of multiresolution loss on larger datasets"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 268,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "H.\\npel Introduction required training data in available data neural Ct, Ge enough data core ree nee ery eerie ee ose Scere Cer"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "w..\\nst Ham Introduction personne sho eee Db IX any required aetraining all possible data datasets Ds available data <C> $ subpar performance coerce eee ary ere ese Sec cr)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "ose IntreductionDb most similar SN, required auxiliary 9 Mp Leentraining data WG, all possible data 9 datasets available enough suitable data\\n- good performance core eee ery eee nee ers DCL eer"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "w.. tat Ham Contributions & talk preview\\n\\nuniverstat Hamburg\\n\\nMethods: Ndata = in = NN = ABCD\\n\\nit similarity -> @ iz == NM NK i F 8 score amp ce 2 * a clustering Ya x comparison"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "w. sat Hom Contributions & talk preview Universität Hamburg\\n\\nMethods: S = data\\n\\nRelated work: x |=| \" ABC a = Previous approaches\\n\\nDi1 Sovany require parallel labels? Q-- > or . 3\\na)\\n- and are thus limited to NM NN score automatically taggable gaa _(NNIample jae aes gn eusiehig tasks (Bjerva,\\n2017) a x comparison"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Contributions & talk preview\\n\\nEAN Universitat Hamburg\\n\\nMethods: N = data\\n\\nRelated work: A\\n- a ~s ABC a = Previous approaches\\n\\nDi1 Svany require parallel labels?\\n\\noo . 3 a and are thus limited to\\n\\nNN NN score automatically taggable data_ (NNI; A data F example\\n\\nAW\\n- tasks (Bjerva,\\n2017)\\n\\na x comparison\\n\\nExperiments: m= Our methods compute the similarity of any two sequence tagging datasets= Our dataset similarity correlates with the change in MTL performance"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Methods"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "BBR ecrscony Text overlap= Count labels for identical words across datasets\\n- Ce label fi i\\n- Compare label frequencies\\n- Fast\\n- ieee= Uses only words contained in both datasets\\n\\n=> Cannot handle ambiguity or multi-word labels"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "or Unventitit vantwg\\n- EXAMPLE for the text overlap approach; = {N noun, V verb, X other} Ly = {NN noun, {NNP} noun plural, VB verb, ADP adposition, DT determiner} QQ) x WwW eo @ @ veWv N ® the process to process the example data requires counts of words (2) DT WN ADP VB DT WNP ADP DT WN vB DY NW the process to find the words for this example took a while ceo ML ane ete ne eer eee ee re ee CoC"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "or Unvenitit vantwg\\n- EXAMple for the text overlap approach\\n\\nL = {N noun, V verb, X other}\\nLy = {NN noun, {NNP} noun plural, VB verb, ADP adposition, DT determiner}\\n\\n(1) x X W a x X W w W N V N x X W w the process to process the example data requires counts of words\\n\\n(2) DT NN ADP VB OT @NB ADP DT WN vB DT WN the process to find the words for this example took a while"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "aves tonoug\\n- EXAMPLE for the text overlap approach\\nRESEARCH | OER LENE | O&R BUBUNG\\n\\nAssociative, count-based representation\\n- Confusion matrix\\n\\nTable: Counts for dataset 1 (left) and 2 (right)\\nTable: Confusion matrix\\n_______s © @ @|>N VX Word NN (SNP) VB ADP DT imie } :101 process 1 VB1 to 1 \"ADP.1 example 1 oD1 words 1 5core rete ery eerie ee eee eer 7"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "ne Example for the text overlap approach\\n\\nAssociative, count-based representation\\n- confusion matrix\\n\\nTable: Counts for dataset 1 (left) and 2 (right)\\n\\nTable: Confusion matrix\\n- N xX, dNV NNN V X Word NN WNP) VB ADP DT ww lis 1s2 the 2 NP)1°91 process 11 to 1 'ADP.1 example 1 DT 41 words 1"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Example for the text overlap approach\\n\\nAssociative, count-based representation\\n- Confusion matrix\\n\\nTable: Counts for dataset 1 (left) and 2 (right)\\n\\nTable: Confusion matrix\\n- The process\\n- Example\\n- Words"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "oo := Use contextual word embeddings data [NNa3 6 example = Compare labels of pairwise closest embeddings across datasets => Can use all tokens in both datasets => Handles ambiguity and multi-word labels"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "oounvenitit vamturg -- EXAMPLE for the vector space approach\\n\\nTable: Most similar words and their labels\\n\\nLabel 1  Token 1  Token 2  Label 2\\n\\nTable: Confusion matrix\\n\\nthe the OT NV x|zN process process NN\\n\\nNNxe to to \"ADP\" (NNP'@ process find\\n\\nNBx the the DT \"ADP.N example example\\n\\nNN oTN data words 'NNPVv requires took\\n\\nVB xDero eee eee rte See crt"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Unveuitit vamturg -- EXAMPLE for the vector space approach\\n\\nTable: Most similar words and their labels\\n\\nLabel 1    Token 1    Token 2    Label 2\\n\\nTable: Confusion matrix & the DT N x|>N process\\n\\nprocess NN am | 2xe to to \"ADP\" NP)| 1v process _ find VB VB.\\n\\nifag the the DT \"ADP. 1N example example NN oT 2N data words\\n\\nNNP@ requires took VB xBcc ree eee ery eee Sener crn"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Example for the vector space approach\\n\\nTable: Most similar words and their labels\\n\\nLabel 1   Token 1   Token 2   Label 2\\n\\nTable: Confusion matrix of the DT process\\n\\nNN    NN    NN    NN\\n\\nADP    (NP)    VB    find\\n\\nVB    data\\n\\nWords    NNP"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "roUnvenitit vomtug -- EXAMPLE for the vector space approach\\n\\nTable: Most similar words and their labels\\n\\nLabel 1 Token 1 Token 2 Label 2\\n\\nTable: Confusion matrix\\n\\nthe the oD 0 @ @| =N process process NN nw | 2 2x6 to to \"ADP\" (NP)| 1 1@ process find VB.\\n2 2x the the DT 1J4N example example NN oT 2/2N data words 'NNP,@ requires took VB u 13 2 3/8core eee ey eee Oc crn 7)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "BR ooe csnoue Similarity measures\\n\\nNormalized Mutual Information:\\nNMG Gye, ML; L)) Liat Uijnr Z lo8e (#)\\nNMI(L, L joint = ALL) = rae aN,\\n- Vier Lj=1 z logy (2%)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "Similarity measures\\n\\nNormalized Mutual Information: NMI(L, L') =\\n- Σ p(x) log(p(x))\\n\\nShared Vocabulary: Text Overlap measure: SV = |V ∩ V'| / (|V| + |V'|)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "Similarity measures\\n\\nNormalized Mutual Information:\\nNMI(L, L') =\\n- ∑ P(x) log P(x)\\n\\nShared Vocabulary: Text Overlap measure:\\nSV = |V∩U| / |V∪U|\\n\\nUnified Unidirectional Embedding measure:\\nUUE = 2 * NMI_forward + NMI_backward"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "Wasa, Experimental setup = Goal: Test if dataset similarity correlates with MTL performance change = Sample smaller, distinct sets as training & auxiliary data = Compute STL & MTL test score and pairwise dataset similarity = 26784 tests in total across 5 POS tagging and 9 NER datasets"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "Datasets\\nID\\nDataset\\nTokens\\nTags\\n\\nPART-OF-SPEECH TAGGING DATASETS\\n- BNC British National Corpus\\n111973625\\n- WSJ Penn Treebank Wall Street Journal\\n1286980\\n45\\n- BC Penn Treebank Brown Corpus\\n1162358\\n45\\n- EWT UD English Web Treebank\\n2548547\\n- GSD UD German GSD\\n2978367\\n\\nNAMED-ENTITY RECOGNITION DATASETS\\n- ONT English OntoNotes Release 5.0\\n2001102 (37\\n- CNLE\\n- CoNLL'03 Shared Task (English)\\n3014189\\n- CNLG\\n- CoNLL'03 Shared Task (German)\\n3103189\\n- EPG Part of EUROPARL (German)\\n110405\\n- sGEN GermEval 2014 NER Shared Task\\n59100524\\n- GMB Groningen Meaning Bank 2.2.0\\n1354149\\n7\\n- SEC SEC filings\\n542568\\n- WIKI Wikigold\\n391528\\n- WNUT W-NUT'17 Shared Task\\n1017361B"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "- Unvenitit vamturg -- Controlled environment\\n- Pairwise NMljoint similarity scores between various dataset samples\\n- 4 datasets split into 3 distinct samples each\\n- Vector space approach using contextual BERT embeddings\\n- Highest similarity: Identical samples\\n- Samples from the same original dataset are highly similar\\n- Samples within the same task are more similar than between tasks\\n- Similarity measure matches intuition"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "ontug POS tagging results | The accuracy between STL and MTL vs. TO NM joining Train data.\\n\\nAux. data\\n\\nKendall's τ: 0.71 + 0.054\\n\\np-values < 0.005\\n\\nsimilarity score"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "wi. sat Ham POS tagging results\\nuniversitst Hamburg so ageR FORSCHUNG | OER LENE | eR BLBUNG\\n\\nA accuracy between STL and MTL vs. UUE NMloint similarity using BERT\\n\\nTrain. data\\n- EWT al om[am teeeg .54\\n- Aux. data\\n- Kendall's 7: 0.69 + 0.12\\n- p-values < 0.0052\\n- BNC0\\n- GSD0.3 0.4 0.5 0.6 0.7 similarity score"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "BB aaeccseiy NER results\\n\\nFORSCHUNG | eR ENE | ex DUNG\\n\\nA Fi score between STL and MTL vs. UUE NMl joint similarity using BERT 10.0.\\n\\nAux. data: @ cnle e75) Mf CNic ko : \"EPG Train. data 5.0 @ GEN\\n\\nA sees CNLE * ONT == CNLG = Kendall's 7: 0.55 + 0.1125 * . saw ERG = p-values < 0.001:\\n- GEN 0.0\\n7. =s:= ONTik ; = WIKI\\n- 2.5 ~ e0.10 0.15 0.20 0.25 similarity score"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "Conclusion"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "BB vacssoney Takeaway\\n1. New methods to compare\\n2. Newsimilarity measures\\n3.\\nQuickly find the best sequence tagging correlate with MTL auxiliary dataset(s) for MTL datasets performance change or TL before training your neural network!\\n\\nNN,\\n- di 2-NMI- SV IN LN-} ee best= NMI + SV main ES- ue 2-NMlpivg * NMlowet data |~ ae=| ms SNM pg + NOM atagithub.com/uhh-It/seq-tag-simcera"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          "Thank you for watching"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 500,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "(Answer-conditional) Question generation\\n\\nAs of 2015, Universal is the only studio to have released three billion-dollar films in one year; this distinction was achieved in 2015 with Furious 7, Jurassic World and Minions.\\n\\nAlong with Jurassic World and Furious 7, what billion-dollar film was released by Universal in 2015?\\n\\nA Minions\\n\\nQA: C x Q7AQG: CxA7Q"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Question generation for QA\\n\\nQuestion generation has many applications.\\n- Chatbots.\\n- Tutoring systems.\\n\\nIn focus:\\n- Creation of synthetic training data for extractive QA models."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Question generation for QA: Key intuition\\n\\nOn Tesla's 75th birthday in 1931, Time magazine put him on its cover.\\nThe cover caption \"All the world's his power house\" noted his contribution to electrical power generation.\\nHe received congratulatory letters from more than 70 pioneers in science and engineering, including Albert Einstein.\\n- Who appeared on Time magazine's cover on his 75th birthday?\\n- Which famous scientist was on the cover of Time magazine in 1931?\\n- Which mad scientist received more than 70 people congratulating him on his birthday?\\n- What famous scientist was also 75?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Research questions\\n- For QA training, should(n't) we promote sample diversity?\\n- How about top-p (nucleus) sampling?\\n- Samples tokens from the top p (e.g., 75%) cumulative probability mass.\\n- Is likelihood maximization a good strategy?\\n- Decoding: Beam search.\\n- Evaluation: BLEU/ROUGE/METEOR.\\n- Can top-p question generation for QA be intrinsically evaluated?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Model: ROBERTA masked LM (Liu et al.,\\n2019) fine-tuned for conditional generation."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Generation Diversity-prompting generation: top-p sampling.\\nAdditionally, we also limit the number of vocabulary items to sample from atk (20 in our experiments).\\n\\nBaseline: Beam search."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Experiments\\n\\nPrimary dataset: SQuAD1-Du: 76k train, 18k dev, 12k test instances.\\n\\nModel sizes: ROBERTa base and large.\\n- We train each model with different amounts of training data (5%, 20%, 50%, 100%).\\n- Eight trained models in total (e.g., base-50%).\\n- To find out if sample diversity is useful across varying model capacity and resource availability."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Experiments (contd.)\\n\\nIntrinsic evaluation:\\n- On the dev set.\\n- Metrics: BLEU, ROUGE, METEOR.\\n\\nExtrinsic evaluation:\\n- Train a QA model with the synthetic questions generated from dev.\\n- BERT-wwm\\n- Measure QA performance on the test set; metric: F1 score."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "SQuAD1-Du results\\n\\nROUGE-4 QAF1 20%\\n\\nBase model\\n\\nROUGE-4 QAF1"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "SQuAD1-Du results (contd.)\\n\\nROUGE-4\\n\\nQAF 120% so = seen best, pal pS pels pegs Be best pel pes pes pes Large model\\n\\nROUGE-4\\n\\nQAF L100% no S a ee7"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "A second dataset: NewsQA\\nExtractive QA over CNN news articles:\\n76k train, 4k test instances.\\nWe apply the SQUAD model zero-shot, treat train as dev, and follow the same evaluation methodology as before."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "ROUGE-1 Qari380 aoBase model 100%\\nROUGE-1 QAFL109 oo330 Large model 100%"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "Augmentation of human annotations dataset train source QA FyGT (dev) 86.3\\nSYNTH 86.1\\nSQUAD IDU 5, SYNTH 86.4\\nSYNTH* + GT 88.6\\nGT (train) 67.9\\nNewsQa SYNTH 63.8\\nSYNTH* + GT 69.2"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Question generation metrics and QA results\\n\\nSQuAD\\n\\n1-Du\\n\\nNewsQA\\n\\n0.50 ee )3 i= a \"§ 0.00» 0.00\\n- # of generators\\n- # of generators"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "Intrinsic evaluation of top-p sampling for QA\\n\\nMain idea: Convex combination of accuracy and diversity.\\n\\nConvenient to have each component metric in [0, 1]."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "Accuracy and diversity\\nAccuracy: P(GT)\\nDiversity: P(GT € top-p)\\nFinal metric: w:P(GT) + (1\\n- w):P(GT € top-p), where w € [0,1]"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "Question generation metrics and QA results\\nSQuAD\\n1\\n- Du\\nNewsQA\\n\\n0.50\\noe)\\n3\\n- 5\\n0.00\\n0.00\\n\\n# of generators\\n# of generators"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "Conclusion\\n- Diversity improves quality of question generation for QA.\\nFuture work:\\n- Diversity with no significant drop in accuracy.\\n- Controlled diversity."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "References\\n- Jacob Devlin, Ming-Wel Chang, Kenton Lee, and Kristina Toutanova.\\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL\\n2019.\\n- Xinya Du, Junru Shao, and Claire Cardie. Learning to Ask: Neural Question Generation for Reading Comprehension. ACL\\n2017.\\n- Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The Curious Case of Neural Text Degeneration. CLR\\n2020.\\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. Know What You Don't Know: Unanswerable Questions or SQUAD. ACL\\n2018.\\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingjie Du, Mandar Joshi, Dangi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\nRoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint,\\n2019.\\n- Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Phils Bachman, and Kaheer Suleman.\\nNewsQA: A Machine Comprehension Dataset. Workshop on Representation Learning for NLP 2017."
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 543,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "1. Introduction\\n\\nNatural Language Inference (NLI) aka, Recognizing Textual Entailment [Dagan+, 2013]\\n\\nDoes a premise P entail a hypothesis H?\\n\\nP: There is no white dog leaning on the fence.\\n\\nH1: There is no white Maltese dog leaning on the fence. Entailment\\n\\nH2: There is no dog leaning on the fence. Non-entailment"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "1. Introduction\\nState-of-the-art Deep Neural Networks (DNN) for NLI\\n\\nRecent progress on neural models often updates the SOTA NLI model.\\n- Tricot\\n- See\\n- Electric\\n- Teams\\n- GLUE [Wang+ 2019]\\n\\nLeaderboard: https://gluebenchmark.com/leaderboard"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "1. Introduction\\n\\nState-of-the-art Deep Neural Networks (DNN) for NLI\\n\\nRecent progress on neural models often updates the SOTA NLI model:\\n- Human baseline (92.8)\\n- T5 92.2\\n- ALBERT+DAAF+NAS 91.6\\n- ERNIE 91.4\\n- ELECTRA-Large 91.3\\n\\nGLUE [Wang et al. 2019] Leaderboard: https://gluebenchmark.com/leaderboard"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "1. Introduction\\n\\nGeneralization concern about DNN-based NLI\\n\\nSOTA DNN models fail to perform challenging inferences\\n\\nExample (downward inference with negation)\\n\\nP: There is no white dog leaning on the fence.\\n\\nH1: There is no white Maltese dog leaning on the fence.\\n\\nEntailment 5"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "1. Introduction\\n\\nGeneralization concern about DNN-based NLI\\n\\nSOTA DNN models fail to perform challenging inferences\\n\\nExample (downward inference with negation)\\n\\nP: There is no white dog leaning on the fence.\\nH1: There is no white Maltese dog leaning on the fence. Entailment...\\nbecause DNN models might learn undesired biases [Gururangan+2018] and heuristics [McCoy+2019]."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "1. Introduction\\n\\nGeneralization concern about DNN-based NLI\\n\\nSOTA DNN models fail to perform challenging inferences\\n\\nExample (downward inference with negation)\\n\\nP: There is no white dog leaning on the fence.\\n\\nH1: There is no white maltese dog leaning on the fence.\\n\\nEntailment... because DNN models might learn undesired biases [Gururangan+2018] and heuristics [McCoy+2019]\\n\\nQuestion: To what extent can DNN models learn the compositional generalization capacity underlying NLI from training instances?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "1. Introduction\\n\\nSystematicity [Fodor and Pylyshin, 1988]\\n\\nSystematicity: The ability to understand a sentence is connected to the ability to understand certain other sentences.\\n\\nSystematicity of Inference:\\n\\nIf you can infer from P & Q & R to P, you can also infer from P & Q to P."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "1. Introduction\\n\\nSystematicity [Fodor and Pylyshin, 1988]\\nSystematicity: The ability to understand a sentence is connected to the ability to understand certain other sentences.\\nSystematicity of Inference:\\nIf you can infer from P & Q & R to P, you can also infer from P & Q to P.\\nIf models obtain systematicity of inference, they should learn inferences from only a small number of training instances.\\n\\nDo neural models learn systematicity of inference in natural language?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Our aim of this study is to investigate the systematic generalization ability of DNN-based NLI models.\\nFocus on inferences well studied in logic and formal semantics: monotonicity."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "2. Monotonicity\\n\\nMonotonicity reasoning [van Benthem, 1983; Icard and Moss, 2014]\\n\\nReplacements with more general (or specific) phrases license entailment\\n- Upward inferences: inferences from specific to general phrases\\nP: Some [dogs] ran in the park\\nH1: Some [animals] ran in the park\\nEntailment\\nH2: Some [white beagles] ran in the park | Non-Entailment\\nanimal run in the park\\ndog white beagle"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "2. Monotonicity\\n\\nMonotonicity (Downward monotone)\\n\\nDownward inferences: order reversing inferences from general to specific phrases\\n\\nP: No [dogs] ran in the park\\n\\nH1: No [white beagles] ran in the park\\n\\nEntailment\\n\\nH2: No [animals] ran in the park\\n\\nNon-Entailment\\n\\nrun in the park\\nanimal\\nbeagle"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "2. Monotonicity\\n\\nMonotonicity\\n\\nUpward inferences: inferences from specific to general phrases\\nP: Some dogs ran in the park\\nH: Some animals ran in the park\\nEntailment\\n\\nDownward inferences: inferences from general to specific phrases\\nP: No dogs ran in the park\\nH: No white beagles ran in the park\\nEntailment\\n\\nTo handle monotonicity, models should systematically capture\\n1. monotonicity direction of quantifiers (upward/downward)\\n2. lexical and structural replacement (general/specific)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 12,
        "texts": [
          "2. Monotonicity\\n\\nProductivity of monotonicity\\n\\nIf a propositional object is embedded in another downward context, the polarity of words over its scope can be reversed again.\\n\\nP: All [workers] joined for a French dinner\\nH: All [new workers] joined for a French dinner\\n\\nEntailment\\n\\nP: Not [all [new workers]] joined for a French dinner\\nH: Not [all [workers]] joined for a French dinner\\n\\nEntailment\\n\\nTo handle monotonicity, models should systematically capture\\n1. monotonicity direction of quantifiers (upward/downward)\\n2. lexical and structural replacement (general/specific)\\n3. productivity (recursiveness)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 13,
        "texts": [
          "Key Idea\\n\\nTo evaluate the systematic generalization ability of DNN-based NLI models on monotonicity and its productivity, we propose a new evaluation protocol where we\\n- synthesize monotonicity inference datasets\\n- systematically control which patterns are shown to the models during training and which are left unseen"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 14,
        "texts": [
          "3. Method\\n\\nAutomatic monotonicity dataset creation\\n1. Generate a premise by using a context-free grammar\\n\\nExamples of context-free grammar rules\\n\\nN + {dogs, ...}, IV > {ran, ...}, TW > {chased, ...}, Q > {some, ...}, NPQ N | QN Sbar, S > NP IV, Sbar > which TV NP"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 15,
        "texts": [
          "3. Method\\n\\nAutomatic monotonicity dataset creation\\n1. Generate a premise by using a context-free grammar\\n\\nExamples of context-free grammar rules\\n\\nN > {dogs, ...},\\n|\\V > {ran, ...},\\nVV > {chased, ...},\\nQ > {some, ...},\\nNPQ N | QN Sbar, S > NP IV, Sbar > which TV NP\\n\\nSame dogs ran (n =\\n1)\\n\\nSome dogs which chased some dogs ran (n =\\n2)\\n\\nSame dogs which chased some dogs which chased some dogs ran (n =\\n3)\\n\\n17"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 16,
        "texts": [
          "3. Method\\nAutomatic monotonicity dataset creation\\n1. Generate a premise by using a context-free grammar\\n\\nExamples of context-free grammar rules\\n\\nN -> {dogs, ...},\\nV -> {ran, ...},\\nV -> {chased, ...},\\nQ -> {some, ...},\\nNP -> Q N | Q N\\nShar, S -> NP IV,\\nSbar -> which T V N P\\n\\nSome dogs ran (n=1)\\nSome dogs which chased some dogs ran (n=2)\\nSome dogs which chased some dogs which chased some dogs ran (n=3)\\n2. Rephrase the premise and generate hypotheses\\n\\nP: Some [dogs] ran\\nH: Some [animals] ran\\nEntailment\\nH': Some [white dogs] ran\\nNon-entailment"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 17,
        "texts": [
          "3. Method\\n\\nHow to test systematicity\\n\\nTrain A\\n\\nFix a quantifier and feed various phrase replacements\\n\\nSome puppies ran\\n\\nSome white dogs ran\\n\\nLex ~® and Some dogs ran"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 18,
        "texts": [
          "3. Method\\nHow to test systematicity\\n\\nTrain A Fix a quantifier\\nTrain B Fix a phrase replacement\\n- and feed various phrase replacements\\n- and feed various quantifiers\\n\\nSome puppies ran\\nSome white dogs ran\\nSeveral puppies ran\\nNo dog ran\\nLex \"AA; Lex ¥ Lex ¥\\n\\nSome dogs ran\\nSeveral dogs ran\\nNo puppy ran"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 19,
        "texts": [
          "3. Method\\n\\nHow to test systematicity\\n\\nTrain A Fix a quantifier\\n\\nTrain B Fix a phrase replacement\\n- and feed various phrase replacements\\n- and feed various quantifiers\\n\\nSome puppies ran\\nSome white dogs ran\\nSeveral puppies ran\\nNo dog ran\\n\\nLex ~& \"AAS\\nLex ¥ Lex ¥\\nSome dogs ran\\nSeveral dogs ran\\nNo puppy ran\\n\\nTest unseen combinations of quantifiers and phrase replacements\\n- Several white dogs ran\\n- No white dogs ran\\n- Several dogs ran\\n- No dogs ran"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 20,
        "texts": [
          "3. Method\\n\\nHow to test productivity\\n\\nTrain A Depth 1\\n\\nTrain B Depth 2\\n\\nSome puppies ran. Some dogs which chased some puppies.\\n\\nSome dogs ran. Some dogs which chased some dogs."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 21,
        "texts": [
          "3. Method\\nHow to test productivity\\n\\nTrain A Depth 1\\nTrain B Depth 2\\n\\nSome puppies ran\\nSome dogs ¥ {Lex, Adj, Prep,..} which chased some puppies | ran\\nSome dogs ran } {Lex, Adj, Prep,...}\\nSome dogs which chased some dogs ran\\n\\nTest Unseen depths\\n\\nSome dogs which chased some dogs | which followed some puppies) ran {Lex, Adj, Prep,...}\\nSome dogs which chased some dogs | which followed some dogs | ran"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 22,
        "texts": [
          "4. Experiments\\n\\nExperimental setting:\\n- Models\\n- LSTM [Hochreiter and Schmidhuber, 1997]\\n- TreeLSTM [Tran and Cheng, 2018]\\n- BERT-based NLI [Devlin+, 2018]\\n- Datasets\\n- Train/Test = 300,000/20,000\\n- Entailment: Non-entailment = 1:1 (Chance rate: 0.5)\\n- Upward: Downward = 1:1\\n- Evaluation metrics: the average accuracy of 5 runs"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 23,
        "texts": [
          "4. Experiments\\n\\nExperiment 1: Systematicity\\n\\nTrain A. 1 quantifier x All replacements\\nP: Some puppies ran.\\n> H: Some dogs ran.\\nP: Some white dogs ran.\\n> H: Some dogs ran.\\n\\nTrain B. All quantifiers x 1 replacement\\nP: Several puppies ran.\\n> H: Several dogs ran.\\nP: No dogs ran.\\n> H: No puppies ran.\\n\\nTest: Unseen combinations\\nA1 + B\\nA2 + B\\nA3 + B\\nA4 + B\\nP: Several white dogs ran.\\n> H: Several dogs ran.\\nP: No dogs ran.\\n> H: No white dogs ran.\\n\\nBERT\\nLSTM\\nTreeLSTM\\n\\nBERT generalizes to unseen combinations of quantifiers and phrase replacements."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 24,
        "texts": [
          "4. Experiments\\n\\nExperiment 1: Systematicity\\n\\nTrain\\nGradually add Train A to the training set\\nTrain A. 1 quantifier x All replacements\\nP: Some puppies ran.\\n> H: Some dogs ran.\\n= P: Some white dogs ran.\\n> H: Some dogs ran.\\n\\nTrain B. All quantifiers x 1 replacement\\nP: Several puppies ran.\\n> H: Several dogs ran.\\n= P: No dogs ran.\\n> H: No puppies ran.\\n\\nTest\\nUnseen combinations\\nP: Several white dogs ran.\\n> H: Several dogs ran.\\n\\nP: No dogs ran.\\n> H: No white dogs ran.\\n\\nBERT\\nLSTM\\nTreeLSTM-\\nBERT generalizes to unseen combinations of quantifiers and phrase replacements-\\nThe accuracy is better as more training data are fed into models."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 25,
        "texts": [
          "4. Experiments\\n\\nExperiment 1: Systematicity\\n\\nWhen testing models on slightly different syntactic structures:\\n- A+B\\n- A+B\\n- AV+B\\n- AG+B\\n- AL+B\\n- A2+B\\n- AS+B\\n- A+B\\n- AL+B\\n- AQ+B\\n- AS+B\\n- A+B\\n- A: +B\\n- AQ+B\\n- AS+B\\n- A+B\\n\\n(a) Subject nouns\\n\\n(b) +Adverbs:\\n\\n(c) +Prepositional phrases\\n\\n(d) Object nouns\\n\\nP: A dog ran\\n\\nP': Today a dog ran\\n\\nP'': In the park, a dog ran\\n\\nPp\": I saw a dog\\n\\nH: An animal ran\\n\\nH': Today an animal ran\\n\\nH'': In the park, an animal ran\\n\\nH''': I saw an animal\\n\\nEntail\\n\\nEntail\\n\\nEntail\\n\\nEntail\\n- The accuracy of all models significantly decreased\\n- This decrease becomes larger as the syntactic structures in the test set become different from those in the training set"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 26,
        "texts": [
          "4. Experiments\\n\\nExperiment 2: Productivity\\n- Train Dev/Test BERT LSTM Tree\\n- LSTM depth 1 100 100 100\\n- depth 1 depth 2 100 99.8 99.5\\n- depth 3 75.2 754 86.4\\n- depth 2 depth 4 55.9 57.7 58.6\\n- depth 5 49.9 45.8 48.4\\n- depth 1 100 100 100\\n- depth 2 100 95.1 99.6\\n- depth 2 depth 3 100 85.2 97.7\\n- depth 4 77.9 59.7 68.0\\n- depth 3 depths 53.5 55 49.6\\n- All models generalize to one level deeper depth\\n- But they fail to generalize to two levels deeper"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 27,
        "texts": [
          "4. Experiments\\n\\nWhen MultiNLI [Williams+2018] is added to the training set\\n\\nTrain Dev/Test\\nBERT          LSTM       TreeLSTM\\ndepth1       46.9      47.2      43.4\\ndepth2       46.2      48.3      49.5\\nMNLI depth3\\n-         46.8      48.9      41.0\\ndepth4       48.5      50.6      48.5\\ndepths       48.9      49.3      48.8\\nMNLI         846       647       70.4\\ndepth1       100       100       100\\ndepth2       100       893       998\\ndepth2       depth3    67.8      66.7      76.3\\ndepth4       46.8      47.1      50.7\\nMNLI depths   41.2      46.7      47.5\\nMNLI         84.4      39.7      63.0\\n- Only the BERT maintains the performance on MultiNLI while improving the performance on monotonicity inferences."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 28,
        "texts": [
          "4. Experiments\\n\\nWhen MultiNLI [witiams+2018) is added to the training set\\n\\nTrain Dev/Test\\n- BERT\\n- depth 1: 46.9 47.2 43.4\\n- depth 2: 46.2 48.3 49.5\\n- depth 3: 46.8 43.9 41.0\\n- depth 4: 48.5 50.6 48.5\\n- depth 5: 48.9 49.3 48.8\\n- LSTM\\n- depth 1: 84.6 64.7 70.4\\n- depth 2: 100 100 100\\n- depth 2: 100 39.3 99.8\\n- depth 3: 67.8 66.7 76.3\\n- depth 4: 46.8 47.1 50.7\\n- depth 5: 41.2 46.7 47.5\\n\\nMNLI 84.4 39.7 63.0\\n\\nBut all models still fail to generalize to two level deeper"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 29,
        "texts": [
          "Conclusion\\n\\nMotivation\\n\\nEvaluating whether DNN can learn the systematicity of NLI\\n\\nApproach\\n\\nControlling seen/unseen patterns with synthesized monotonicity datasets\\n\\nMain results\\n- The generalization ability of DNN is limited to cases where the syntactic structures are similar to those in the training set.\\n- BERT might have the ability to memorize different types of datasets.\\n\\nThanks!\\n\\nHitomi Yanaka hitomi.yanaka@riken.jp\\n\\nCode: https://github.com/verypluming/systematicity"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 205,
    "slides": [
      {
        "section_index": 0,
        "slide_index": 0,
        "texts": [
          "Introduction: Hierarchical Text Classification\\n- Hierarchical Text Classification (HTC) aims to categorise a textual description within a set of labels that are organized in a structured class hierarchy.\\n- Amazon store\\n- Gaming keyword\\n- Layer 1 (L1)\\n- Layer 2 (L2)\\n- Layer 3 (L3)"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 1,
        "texts": [
          "Related Work: HTC\\n\\nIn HTC, there are two kinds of strategies:\\n- Local: exploits local information per layer of the class taxonomy.\\n- Global: employs a single model for all the classes and levels.\\n\\nNeural models show excellent performance for both approaches (Kowsari et al., 2017; Sinha et al., 2018), as expected.\\nBUT... they have a large number of parameters and extended training time."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 2,
        "texts": [
          "Related Word: efficiency in text classification\\n\\nFor improving efficiency, there are studies focused only in flat-classification (without hierarchies):\\n- Joulin et al. (2017)\\n- Howard and Ruder (2018)\\n\\nListed strategies are still underdeveloped for HTC, and the most recent and effective methods are still computationally expensive (Yang et al., 2019).\\n\\nHow can we improve HTC at a lower computational cost?"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 3,
        "texts": [
          "Efficient strategies for hierarchical text classification\\n1. Model Architecture:\\n- HTC as a sequence-to-sequence problem using a bidirectional GRU unit.\\n- For our experiments, we use the attention proposed by It usually barks."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 4,
        "texts": [
          "Efficient strategies for hierarchical text classification\\n\\nAuxiliary task:\\n- The usual HTC predicts in this order: L1 + L2 > L3\\n- We also predict the inverse order of the class hierarchy: L3\\n- L2 > L1\\n- Inspired by Sutskever et al. (2014), who reverse the order of words in a sequence for better error propagation\\n- Experimental note: We alternate the training of the model between the auxiliary and the main task every 2 epochs"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 5,
        "texts": [
          "Efficient strategies for hierarchical text classification.\\n3. Class-definition embeddings for external knowledge integration\\n\\nFor each class c in any level l of the hierarchy, we could obtain a raw text definition and compute a vector representation (CDV).\\n\\nClass -> Amphibian Definition: a cold-blooded vertebrate animal of a class that comprises the frogs, toads, newts, and salamanders."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 6,
        "texts": [
          "Efficient strategies for hierarchical text classification\\n\\n3.1 Parent node conditioning (PNC):\\n- We predict the highest-level class and then use its CDV representation as an additional input (alongside the encoder outputs) to the attention layer for the prediction of the next level class.\\n\\n3.2 Adapted beam search:\\n- We reorder the beams based on the sum of the log probabilities with the cosine distance between the sentence embedding and the class-definition embedding (CDV)."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 7,
        "texts": [
          "Experiments:\\n\\nDatasets\\n- Web of Science (WOS; Kowsari et al.,\\n2017)\\n- DBpedia (Sinha et al.,\\n2018)\\n\\nWOS   DBpedia\\nNumber of documents   46,985   342,782\\nClasses in level 1   7   9\\nClasses in level 2   143   70\\nClasses in level 3   NA   21910"
        ]
      },
      {
        "section_index": 0,
        "slide_index": 8,
        "texts": [
          "Experiments: Results\\n\\nwos DBpedia\\n- seq2seq baseline 78.84 + 0.17\\n- 95.12 + 0.01\\n\\nAuxiliary task\\n- 78.93 + 0.52\\n- 95.21 + 0.16\\n\\nIndividual strategies | Parent node conditioning (PNC)\\n- 79.01 + 0.18\\n- 95.26 + 0.09\\n\\nBeam search (original)\\n- 78.90 + 0.25\\n- 95.25 + 0.01\\n\\nBeam search (modified)\\n- 78.90 + 0.28\\n- 95.26 + 0.01\\n\\nwos DBpedia\\n- Auxiliary task + PNC [7M params.]\\n- 79.79 + 0.45\\n- 95.23 + 0.13\\n\\nBeam search (original) + PNC\\n- 79.18 + 0.19\\n- 95.30 + 0.10\\n\\nCombined strategies | Beam search (modified) + PNC\\n- 79.18 + 0.23\\n- 95.30 + 0.11\\n\\nAuxiliary task + PNC + Beam search (orig.)\\n- 79.92 + 0.51\\n- 95.26 + 0.12\\n\\nAuxiliary task + PNC + Beam search (mod.)\\n- 79.87 + 0.49\\n- 95.26 + 0.12\\n\\nReported values are averaged across five runs, and * indicates Almost Stochastic Dominance (Dror et al.,\\n2019) over the baseline."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 9,
        "texts": [
          "Experiments WOS DBpedia.\\n- Beam search (original) + PNC *79.18 £0.19 *95.30 + 0.10\\n- Combined strategies\\n- 1 military task + PNC + Beam search (orig.) *79.92 £0.51 *95.26 + 0.12\\n- Previous work\\n- HDLtTex (Kowsari et al.,\\n2017) [SB params.] 76.58 92.10\\n- Sinha et al. (2018) [34M params.] 77.46 93.72\\n\\nOur approach uses up to 7M parameters.\\nReported values are averaged across five runs, and * indicates Almost Stochastic Dominance (Dror et al.,\\n2019) over the baseline."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 10,
        "texts": [
          "Conclusions\\n- By following the sequence-to-sequence learning paradigm, it is possible to create robust models for HTC with few parameters and short training time.\\n- Prediction in a bottom-up scheme strengthens the model capacity to learn robust representations.\\n- Classes' textual definitions encoded in a word vector space allow knowledge integration at (1) each prediction step and (2) adapted beam search."
        ]
      },
      {
        "section_index": 0,
        "slide_index": 11,
        "texts": [
          "Thank you!\\nKervy Rivas Rojas\\nRM\\nk:ivas@pucp.pe\\ny @nautilus\\nt814"
        ]
      }
    ]
  },
  {
    "conf": "acl20",
    "idd": 457
  },
  {
    "conf": "acl20",
    "idd": 339
  },
  {
    "conf": "acl20",
    "idd": 346
  },
  {
    "conf": "acl20",
    "idd": 393
  },
  {
    "conf": "acl20",
    "idd": 154
  },
  {
    "conf": "acl20",
    "idd": 329
  },
  {
    "conf": "acl20",
    "idd": 48
  },
  {
    "conf": "acl20",
    "idd": 15
  },
  {
    "conf": "acl20",
    "idd": 513
  },
  {
    "conf": "acl20",
    "idd": 194
  },
  {
    "conf": "acl20",
    "idd": 501
  },
  {
    "conf": "acl20",
    "idd": 128
  },
  {
    "conf": "acl20",
    "idd": 591
  },
  {
    "conf": "acl20",
    "idd": 91
  },
  {
    "conf": "acl20",
    "idd": 688
  },
  {
    "conf": "acl20",
    "idd": 664
  },
  {
    "conf": "acl20",
    "idd": 663
  },
  {
    "conf": "acl20",
    "idd": 252
  },
  {
    "conf": "acl20",
    "idd": 227
  },
  {
    "conf": "acl20",
    "idd": 478
  },
  {
    "conf": "acl20",
    "idd": 141
  },
  {
    "conf": "acl20",
    "idd": 724
  },
  {
    "conf": "acl20",
    "idd": 536
  },
  {
    "conf": "acl20",
    "idd": 220
  },
  {
    "conf": "acl20",
    "idd": 86
  },
  {
    "conf": "acl20",
    "idd": 645
  },
  {
    "conf": "acl20",
    "idd": 230
  },
  {
    "conf": "acl20",
    "idd": 648
  },
  {
    "conf": "acl20",
    "idd": 684
  },
  {
    "conf": "acl20",
    "idd": 584
  },
  {
    "conf": "acl20",
    "idd": 332
  },
  {
    "conf": "acl20",
    "idd": 580
  },
  {
    "conf": "acl20",
    "idd": 594
  },
  {
    "conf": "acl20",
    "idd": 668
  },
  {
    "conf": "acl20",
    "idd": 65
  },
  {
    "conf": "acl20",
    "idd": 314
  },
  {
    "conf": "acl20",
    "idd": 158
  },
  {
    "conf": "acl20",
    "idd": 175
  },
  {
    "conf": "acl20",
    "idd": 443
  },
  {
    "conf": "acl20",
    "idd": 296
  },
  {
    "conf": "acl20",
    "idd": 217
  },
  {
    "conf": "acl20",
    "idd": 209
  },
  {
    "conf": "acl20",
    "idd": 328
  },
  {
    "conf": "acl20",
    "idd": 600
  },
  {
    "conf": "acl20",
    "idd": 528
  },
  {
    "conf": "acl20",
    "idd": 59
  },
  {
    "conf": "acl20",
    "idd": 257
  },
  {
    "conf": "acl20",
    "idd": 610
  },
  {
    "conf": "acl20",
    "idd": 360
  },
  {
    "conf": "acl20",
    "idd": 599
  },
  {
    "conf": "acl20",
    "idd": 596
  },
  {
    "conf": "acl20",
    "idd": 558
  },
  {
    "conf": "acl20",
    "idd": 735
  },
  {
    "conf": "acl20",
    "idd": 474
  },
  {
    "conf": "acl20",
    "idd": 606
  },
  {
    "conf": "acl20",
    "idd": 210
  },
  {
    "conf": "acl20",
    "idd": 271
  },
  {
    "conf": "acl20",
    "idd": 76
  },
  {
    "conf": "acl20",
    "idd": 383
  },
  {
    "conf": "acl20",
    "idd": 90
  },
  {
    "conf": "acl20",
    "idd": 42
  },
  {
    "conf": "acl20",
    "idd": 110
  },
  {
    "conf": "acl20",
    "idd": 299
  },
  {
    "conf": "acl20",
    "idd": 671
  },
  {
    "conf": "acl20",
    "idd": 106
  },
  {
    "conf": "acl20",
    "idd": 469
  },
  {
    "conf": "acl20",
    "idd": 312
  },
  {
    "conf": "acl20",
    "idd": 308
  },
  {
    "conf": "acl20",
    "idd": 266
  },
  {
    "conf": "acl20",
    "idd": 202
  },
  {
    "conf": "acl20",
    "idd": 778
  },
  {
    "conf": "acl20",
    "idd": 231
  },
  {
    "conf": "acl20",
    "idd": 704
  },
  {
    "conf": "acl20",
    "idd": 256
  },
  {
    "conf": "acl20",
    "idd": 130
  },
  {
    "conf": "acl20",
    "idd": 711
  },
  {
    "conf": "acl20",
    "idd": 114
  },
  {
    "conf": "acl20",
    "idd": 702
  },
  {
    "conf": "acl20",
    "idd": 673
  },
  {
    "conf": "acl20",
    "idd": 181
  },
  {
    "conf": "acl20",
    "idd": 340
  },
  {
    "conf": "acl20",
    "idd": 179
  },
  {
    "conf": "acl20",
    "idd": 362
  },
  {
    "conf": "acl20",
    "idd": 549
  },
  {
    "conf": "acl20",
    "idd": 417
  },
  {
    "conf": "acl20",
    "idd": 551
  },
  {
    "conf": "acl20",
    "idd": 100
  },
  {
    "conf": "acl20",
    "idd": 415
  },
  {
    "conf": "acl20",
    "idd": 477
  },
  {
    "conf": "acl20",
    "idd": 649
  },
  {
    "conf": "acl20",
    "idd": 690
  },
  {
    "conf": "acl20",
    "idd": 626
  },
  {
    "conf": "acl20",
    "idd": 345
  },
  {
    "conf": "acl20",
    "idd": 588
  },
  {
    "conf": "acl20",
    "idd": 382
  },
  {
    "conf": "acl20",
    "idd": 184
  },
  {
    "conf": "acl20",
    "idd": 526
  },
  {
    "conf": "acl20",
    "idd": 656
  },
  {
    "conf": "acl20",
    "idd": 47
  },
  {
    "conf": "acl20",
    "idd": 392
  },
  {
    "conf": "acl20",
    "idd": 124
  },
  {
    "conf": "acl20",
    "idd": 409
  },
  {
    "conf": "acl20",
    "idd": 79
  },
  {
    "conf": "acl20",
    "idd": 215
  },
  {
    "conf": "acl20",
    "idd": 105
  },
  {
    "conf": "acl20",
    "idd": 453
  },
  {
    "conf": "acl20",
    "idd": 171
  },
  {
    "conf": "acl20",
    "idd": 700
  },
  {
    "conf": "acl20",
    "idd": 401
  },
  {
    "conf": "acl20",
    "idd": 701
  },
  {
    "conf": "acl20",
    "idd": 379
  },
  {
    "conf": "acl20",
    "idd": 300
  },
  {
    "conf": "acl20",
    "idd": 16
  },
  {
    "conf": "acl20",
    "idd": 206
  },
  {
    "conf": "acl20",
    "idd": 483
  },
  {
    "conf": "acl20",
    "idd": 40
  },
  {
    "conf": "acl20",
    "idd": 703
  },
  {
    "conf": "acl20",
    "idd": 466
  },
  {
    "conf": "acl20",
    "idd": 752
  },
  {
    "conf": "acl20",
    "idd": 607
  },
  {
    "conf": "acl20",
    "idd": 348
  },
  {
    "conf": "acl20",
    "idd": 593
  },
  {
    "conf": "acl20",
    "idd": 767
  },
  {
    "conf": "acl20",
    "idd": 223
  },
  {
    "conf": "acl20",
    "idd": 270
  },
  {
    "conf": "acl20",
    "idd": 532
  },
  {
    "conf": "acl20",
    "idd": 480
  },
  {
    "conf": "acl20",
    "idd": 634
  },
  {
    "conf": "acl20",
    "idd": 70
  },
  {
    "conf": "acl20",
    "idd": 222
  },
  {
    "conf": "acl20",
    "idd": 178
  },
  {
    "conf": "acl20",
    "idd": 628
  },
  {
    "conf": "acl20",
    "idd": 57
  },
  {
    "conf": "acl20",
    "idd": 422
  },
  {
    "conf": "acl20",
    "idd": 119
  },
  {
    "conf": "acl20",
    "idd": 287
  },
  {
    "conf": "acl20",
    "idd": 236
  },
  {
    "conf": "acl20",
    "idd": 111
  },
  {
    "conf": "acl20",
    "idd": 125
  },
  {
    "conf": "acl20",
    "idd": 375
  },
  {
    "conf": "acl20",
    "idd": 242
  },
  {
    "conf": "acl20",
    "idd": 581
  },
  {
    "conf": "acl20",
    "idd": 68
  },
  {
    "conf": "acl20",
    "idd": 491
  },
  {
    "conf": "acl20",
    "idd": 241
  },
  {
    "conf": "acl20",
    "idd": 180
  },
  {
    "conf": "acl20",
    "idd": 734
  },
  {
    "conf": "acl20",
    "idd": 432
  },
  {
    "conf": "acl20",
    "idd": 335
  },
  {
    "conf": "acl20",
    "idd": 11
  },
  {
    "conf": "acl20",
    "idd": 235
  },
  {
    "conf": "acl20",
    "idd": 559
  },
  {
    "conf": "acl20",
    "idd": 334
  },
  {
    "conf": "acl20",
    "idd": 14
  },
  {
    "conf": "acl20",
    "idd": 258
  },
  {
    "conf": "acl20",
    "idd": 538
  },
  {
    "conf": "acl20",
    "idd": 292
  },
  {
    "conf": "acl20",
    "idd": 566
  },
  {
    "conf": "acl20",
    "idd": 307
  },
  {
    "conf": "acl20",
    "idd": 199
  },
  {
    "conf": "acl20",
    "idd": 472
  },
  {
    "conf": "acl20",
    "idd": 740
  },
  {
    "conf": "acl20",
    "idd": 394
  },
  {
    "conf": "acl20",
    "idd": 108
  },
  {
    "conf": "acl20",
    "idd": 743
  },
  {
    "conf": "acl20",
    "idd": 761
  },
  {
    "conf": "acl20",
    "idd": 514
  },
  {
    "conf": "acl20",
    "idd": 747
  },
  {
    "conf": "acl20",
    "idd": 386
  },
  {
    "conf": "acl20",
    "idd": 157
  },
  {
    "conf": "acl20",
    "idd": 510
  },
  {
    "conf": "acl20",
    "idd": 237
  },
  {
    "conf": "acl20",
    "idd": 373
  },
  {
    "conf": "acl20",
    "idd": 404
  },
  {
    "conf": "acl20",
    "idd": 353
  },
  {
    "conf": "acl20",
    "idd": 642
  },
  {
    "conf": "acl20",
    "idd": 43
  },
  {
    "conf": "acl20",
    "idd": 390
  },
  {
    "conf": "acl20",
    "idd": 116
  },
  {
    "conf": "acl20",
    "idd": 121
  },
  {
    "conf": "acl20",
    "idd": 60
  },
  {
    "conf": "acl20",
    "idd": 555
  },
  {
    "conf": "acl20",
    "idd": 602
  },
  {
    "conf": "acl20",
    "idd": 451
  },
  {
    "conf": "acl20",
    "idd": 563
  },
  {
    "conf": "acl20",
    "idd": 132
  },
  {
    "conf": "acl20",
    "idd": 83
  },
  {
    "conf": "acl20",
    "idd": 81
  },
  {
    "conf": "acl20",
    "idd": 384
  },
  {
    "conf": "acl20",
    "idd": 377
  },
  {
    "conf": "acl20",
    "idd": 278
  },
  {
    "conf": "acl20",
    "idd": 705
  },
  {
    "conf": "acl20",
    "idd": 243
  },
  {
    "conf": "acl20",
    "idd": 741
  },
  {
    "conf": "acl20",
    "idd": 426
  },
  {
    "conf": "acl20",
    "idd": 517
  },
  {
    "conf": "acl20",
    "idd": 661
  },
  {
    "conf": "acl20",
    "idd": 416
  },
  {
    "conf": "acl20",
    "idd": 527
  },
  {
    "conf": "acl20",
    "idd": 71
  },
  {
    "conf": "acl20",
    "idd": 72
  },
  {
    "conf": "acl20",
    "idd": 728
  },
  {
    "conf": "acl20",
    "idd": 196
  },
  {
    "conf": "acl20",
    "idd": 471
  },
  {
    "conf": "acl20",
    "idd": 467
  },
  {
    "conf": "acl20",
    "idd": 193
  },
  {
    "conf": "acl20",
    "idd": 427
  },
  {
    "conf": "acl20",
    "idd": 515
  },
  {
    "conf": "acl20",
    "idd": 486
  },
  {
    "conf": "acl20",
    "idd": 458
  },
  {
    "conf": "acl20",
    "idd": 87
  },
  {
    "conf": "acl20",
    "idd": 542
  },
  {
    "conf": "acl20",
    "idd": 489
  },
  {
    "conf": "acl20",
    "idd": 189
  },
  {
    "conf": "acl20",
    "idd": 674
  },
  {
    "conf": "acl20",
    "idd": 406
  },
  {
    "conf": "acl20",
    "idd": 186
  },
  {
    "conf": "acl20",
    "idd": 56
  },
  {
    "conf": "acl20",
    "idd": 211
  },
  {
    "conf": "acl20",
    "idd": 476
  },
  {
    "conf": "acl20",
    "idd": 742
  },
  {
    "conf": "acl20",
    "idd": 521
  },
  {
    "conf": "acl20",
    "idd": 748
  },
  {
    "conf": "acl20",
    "idd": 682
  },
  {
    "conf": "acl20",
    "idd": 504
  },
  {
    "conf": "acl20",
    "idd": 712
  },
  {
    "conf": "acl20",
    "idd": 147
  },
  {
    "conf": "acl20",
    "idd": 62
  },
  {
    "conf": "acl20",
    "idd": 319
  },
  {
    "conf": "acl20",
    "idd": 203
  },
  {
    "conf": "acl20",
    "idd": 249
  },
  {
    "conf": "acl20",
    "idd": 529
  },
  {
    "conf": "acl20",
    "idd": 126
  },
  {
    "conf": "acl20",
    "idd": 325
  },
  {
    "conf": "acl20",
    "idd": 488
  },
  {
    "conf": "acl20",
    "idd": 326
  },
  {
    "conf": "acl20",
    "idd": 632
  },
  {
    "conf": "acl20",
    "idd": 133
  },
  {
    "conf": "acl20",
    "idd": 282
  },
  {
    "conf": "acl20",
    "idd": 462
  },
  {
    "conf": "acl20",
    "idd": 428
  },
  {
    "conf": "acl20",
    "idd": 204
  },
  {
    "conf": "acl20",
    "idd": 93
  },
  {
    "conf": "acl20",
    "idd": 461
  },
  {
    "conf": "acl20",
    "idd": 29
  },
  {
    "conf": "acl20",
    "idd": 170
  },
  {
    "conf": "acl20",
    "idd": 577
  },
  {
    "conf": "acl20",
    "idd": 707
  },
  {
    "conf": "acl20",
    "idd": 54
  },
  {
    "conf": "acl20",
    "idd": 669
  },
  {
    "conf": "acl20",
    "idd": 69
  },
  {
    "conf": "acl20",
    "idd": 564
  },
  {
    "conf": "acl20",
    "idd": 413
  },
  {
    "conf": "acl20",
    "idd": 35
  },
  {
    "conf": "acl20",
    "idd": 172
  },
  {
    "conf": "acl20",
    "idd": 746
  },
  {
    "conf": "acl20",
    "idd": 80
  },
  {
    "conf": "acl20",
    "idd": 644
  },
  {
    "conf": "acl20",
    "idd": 722
  },
  {
    "conf": "acl20",
    "idd": 366
  },
  {
    "conf": "acl20",
    "idd": 484
  },
  {
    "conf": "acl20",
    "idd": 317
  },
  {
    "conf": "acl20",
    "idd": 197
  },
  {
    "conf": "acl20",
    "idd": 537
  },
  {
    "conf": "acl20",
    "idd": 2
  },
  {
    "conf": "acl20",
    "idd": 232
  },
  {
    "conf": "acl20",
    "idd": 492
  },
  {
    "conf": "acl20",
    "idd": 487
  },
  {
    "conf": "acl20",
    "idd": 201
  },
  {
    "conf": "acl20",
    "idd": 619
  },
  {
    "conf": "acl20",
    "idd": 354
  },
  {
    "conf": "acl20",
    "idd": 399
  },
  {
    "conf": "acl20",
    "idd": 769
  },
  {
    "conf": "acl20",
    "idd": 303
  },
  {
    "conf": "acl20",
    "idd": 320
  },
  {
    "conf": "acl20",
    "idd": 547
  },
  {
    "conf": "acl20",
    "idd": 41
  },
  {
    "conf": "acl20",
    "idd": 275
  },
  {
    "conf": "acl20",
    "idd": 117
  },
  {
    "conf": "acl20",
    "idd": 420
  },
  {
    "conf": "acl20",
    "idd": 710
  },
  {
    "conf": "acl20",
    "idd": 765
  },
  {
    "conf": "acl20",
    "idd": 315
  },
  {
    "conf": "acl20",
    "idd": 142
  },
  {
    "conf": "acl20",
    "idd": 497
  },
  {
    "conf": "acl20",
    "idd": 643
  },
  {
    "conf": "acl20",
    "idd": 389
  },
  {
    "conf": "acl20",
    "idd": 349
  },
  {
    "conf": "acl20",
    "idd": 595
  },
  {
    "conf": "acl20",
    "idd": 247
  },
  {
    "conf": "acl20",
    "idd": 221
  },
  {
    "conf": "acl20",
    "idd": 450
  },
  {
    "conf": "acl20",
    "idd": 331
  },
  {
    "conf": "acl20",
    "idd": 771
  },
  {
    "conf": "acl20",
    "idd": 717
  },
  {
    "conf": "acl20",
    "idd": 479
  },
  {
    "conf": "acl20",
    "idd": 419
  },
  {
    "conf": "acl20",
    "idd": 67
  },
  {
    "conf": "acl20",
    "idd": 380
  },
  {
    "conf": "acl20",
    "idd": 238
  },
  {
    "conf": "acl20",
    "idd": 24
  },
  {
    "conf": "acl20",
    "idd": 318
  },
  {
    "conf": "acl20",
    "idd": 18
  },
  {
    "conf": "acl20",
    "idd": 371
  },
  {
    "conf": "acl20",
    "idd": 131
  },
  {
    "conf": "acl20",
    "idd": 333
  },
  {
    "conf": "acl20",
    "idd": 716
  },
  {
    "conf": "acl20",
    "idd": 267
  },
  {
    "conf": "acl20",
    "idd": 305
  },
  {
    "conf": "acl20",
    "idd": 37
  },
  {
    "conf": "acl20",
    "idd": 429
  },
  {
    "conf": "acl20",
    "idd": 496
  },
  {
    "conf": "acl20",
    "idd": 430
  },
  {
    "conf": "acl20",
    "idd": 269
  },
  {
    "conf": "acl20",
    "idd": 342
  },
  {
    "conf": "acl20",
    "idd": 302
  },
  {
    "conf": "acl20",
    "idd": 768
  },
  {
    "conf": "acl20",
    "idd": 433
  },
  {
    "conf": "acl20",
    "idd": 248
  },
  {
    "conf": "acl20",
    "idd": 73
  },
  {
    "conf": "acl20",
    "idd": 535
  },
  {
    "conf": "acl20",
    "idd": 92
  },
  {
    "conf": "acl20",
    "idd": 321
  },
  {
    "conf": "acl20",
    "idd": 495
  },
  {
    "conf": "acl20",
    "idd": 678
  },
  {
    "conf": "acl20",
    "idd": 691
  },
  {
    "conf": "acl20",
    "idd": 167
  },
  {
    "conf": "acl20",
    "idd": 327
  },
  {
    "conf": "acl20",
    "idd": 219
  },
  {
    "conf": "acl20",
    "idd": 655
  },
  {
    "conf": "acl20",
    "idd": 714
  },
  {
    "conf": "acl20",
    "idd": 589
  },
  {
    "conf": "acl20",
    "idd": 473
  },
  {
    "conf": "acl20",
    "idd": 294
  },
  {
    "conf": "acl20",
    "idd": 372
  },
  {
    "conf": "acl20",
    "idd": 356
  },
  {
    "conf": "acl20",
    "idd": 260
  },
  {
    "conf": "acl20",
    "idd": 569
  },
  {
    "conf": "acl20",
    "idd": 101
  },
  {
    "conf": "acl20",
    "idd": 520
  },
  {
    "conf": "acl20",
    "idd": 421
  },
  {
    "conf": "acl20",
    "idd": 511
  },
  {
    "conf": "acl20",
    "idd": 284
  },
  {
    "conf": "acl20",
    "idd": 316
  },
  {
    "conf": "acl20",
    "idd": 659
  },
  {
    "conf": "acl20",
    "idd": 739
  },
  {
    "conf": "acl20",
    "idd": 440
  },
  {
    "conf": "acl20",
    "idd": 720
  },
  {
    "conf": "acl20",
    "idd": 21
  },
  {
    "conf": "acl20",
    "idd": 541
  },
  {
    "conf": "acl20",
    "idd": 343
  },
  {
    "conf": "acl20",
    "idd": 146
  },
  {
    "conf": "acl20",
    "idd": 13
  },
  {
    "conf": "acl20",
    "idd": 611
  },
  {
    "conf": "acl20",
    "idd": 338
  },
  {
    "conf": "acl20",
    "idd": 281
  },
  {
    "conf": "acl20",
    "idd": 459
  },
  {
    "conf": "acl20",
    "idd": 75
  },
  {
    "conf": "acl20",
    "idd": 524
  },
  {
    "conf": "acl20",
    "idd": 465
  },
  {
    "conf": "acl20",
    "idd": 625
  },
  {
    "conf": "acl20",
    "idd": 173
  },
  {
    "conf": "acl20",
    "idd": 546
  },
  {
    "conf": "acl20",
    "idd": 676
  },
  {
    "conf": "acl20",
    "idd": 123
  },
  {
    "conf": "acl20",
    "idd": 135
  },
  {
    "conf": "acl20",
    "idd": 553
  },
  {
    "conf": "acl20",
    "idd": 309
  },
  {
    "conf": "acl20",
    "idd": 412
  },
  {
    "conf": "acl20",
    "idd": 115
  },
  {
    "conf": "acl20",
    "idd": 162
  },
  {
    "conf": "acl20",
    "idd": 773
  },
  {
    "conf": "acl20",
    "idd": 493
  },
  {
    "conf": "acl20",
    "idd": 274
  },
  {
    "conf": "acl20",
    "idd": 188
  },
  {
    "conf": "acl20",
    "idd": 736
  },
  {
    "conf": "acl20",
    "idd": 26
  },
  {
    "conf": "acl20",
    "idd": 198
  },
  {
    "conf": "acl20",
    "idd": 176
  },
  {
    "conf": "acl20",
    "idd": 85
  },
  {
    "conf": "acl20",
    "idd": 573
  },
  {
    "conf": "acl20",
    "idd": 759
  },
  {
    "conf": "acl20",
    "idd": 122
  },
  {
    "conf": "acl20",
    "idd": 441
  },
  {
    "conf": "acl20",
    "idd": 603
  },
  {
    "conf": "acl20",
    "idd": 561
  },
  {
    "conf": "acl20",
    "idd": 359
  },
  {
    "conf": "acl20",
    "idd": 757
  },
  {
    "conf": "acl20",
    "idd": 692
  },
  {
    "conf": "acl20",
    "idd": 46
  },
  {
    "conf": "acl20",
    "idd": 568
  },
  {
    "conf": "acl20",
    "idd": 109
  },
  {
    "conf": "acl20",
    "idd": 251
  },
  {
    "conf": "acl20",
    "idd": 775
  },
  {
    "conf": "acl20",
    "idd": 560
  },
  {
    "conf": "acl20",
    "idd": 713
  },
  {
    "conf": "acl20",
    "idd": 234
  },
  {
    "conf": "acl20",
    "idd": 764
  },
  {
    "conf": "acl20",
    "idd": 207
  },
  {
    "conf": "acl20",
    "idd": 605
  },
  {
    "conf": "acl20",
    "idd": 776
  },
  {
    "conf": "acl20",
    "idd": 679
  },
  {
    "conf": "acl20",
    "idd": 657
  },
  {
    "conf": "acl20",
    "idd": 129
  },
  {
    "conf": "acl20",
    "idd": 633
  },
  {
    "conf": "acl20",
    "idd": 460
  },
  {
    "conf": "acl20",
    "idd": 725
  },
  {
    "conf": "acl20",
    "idd": 301
  },
  {
    "conf": "acl20",
    "idd": 402
  },
  {
    "conf": "acl20",
    "idd": 732
  },
  {
    "conf": "acl20",
    "idd": 662
  },
  {
    "conf": "acl20",
    "idd": 387
  },
  {
    "conf": "acl20",
    "idd": 729
  },
  {
    "conf": "acl20",
    "idd": 447
  },
  {
    "conf": "acl20",
    "idd": 182
  },
  {
    "conf": "acl20",
    "idd": 640
  },
  {
    "conf": "acl20",
    "idd": 388
  },
  {
    "conf": "acl20",
    "idd": 437
  },
  {
    "conf": "acl20",
    "idd": 160
  },
  {
    "conf": "acl20",
    "idd": 288
  },
  {
    "conf": "acl20",
    "idd": 34
  },
  {
    "conf": "acl20",
    "idd": 464
  },
  {
    "conf": "acl20",
    "idd": 435
  },
  {
    "conf": "acl20",
    "idd": 280
  },
  {
    "conf": "acl20",
    "idd": 667
  },
  {
    "conf": "acl20",
    "idd": 361
  },
  {
    "conf": "acl20",
    "idd": 200
  },
  {
    "conf": "acl20",
    "idd": 683
  },
  {
    "conf": "acl20",
    "idd": 494
  },
  {
    "conf": "acl20",
    "idd": 286
  },
  {
    "conf": "acl20",
    "idd": 244
  },
  {
    "conf": "acl20",
    "idd": 228
  },
  {
    "conf": "acl20",
    "idd": 365
  },
  {
    "conf": "acl20",
    "idd": 531
  },
  {
    "conf": "acl20",
    "idd": 350
  },
  {
    "conf": "acl20",
    "idd": 454
  },
  {
    "conf": "acl20",
    "idd": 723
  },
  {
    "conf": "acl20",
    "idd": 25
  },
  {
    "conf": "acl20",
    "idd": 651
  },
  {
    "conf": "acl20",
    "idd": 97
  },
  {
    "conf": "acl20",
    "idd": 506
  },
  {
    "conf": "acl20",
    "idd": 556
  },
  {
    "conf": "acl20",
    "idd": 630
  },
  {
    "conf": "acl20",
    "idd": 367
  },
  {
    "conf": "acl20",
    "idd": 9
  },
  {
    "conf": "acl20",
    "idd": 604
  },
  {
    "conf": "acl20",
    "idd": 187
  },
  {
    "conf": "acl20",
    "idd": 646
  },
  {
    "conf": "acl20",
    "idd": 6
  },
  {
    "conf": "acl20",
    "idd": 297
  },
  {
    "conf": "acl20",
    "idd": 262
  },
  {
    "conf": "acl20",
    "idd": 12
  },
  {
    "conf": "acl20",
    "idd": 139
  },
  {
    "conf": "acl20",
    "idd": 737
  },
  {
    "conf": "acl20",
    "idd": 310
  },
  {
    "conf": "acl20",
    "idd": 240
  },
  {
    "conf": "acl20",
    "idd": 760
  },
  {
    "conf": "acl20",
    "idd": 578
  },
  {
    "conf": "acl20",
    "idd": 570
  },
  {
    "conf": "acl20",
    "idd": 177
  },
  {
    "conf": "acl20",
    "idd": 166
  },
  {
    "conf": "acl20",
    "idd": 507
  },
  {
    "conf": "acl20",
    "idd": 516
  },
  {
    "conf": "acl20",
    "idd": 762
  },
  {
    "conf": "acl20",
    "idd": 159
  },
  {
    "conf": "acl20",
    "idd": 358
  },
  {
    "conf": "acl20",
    "idd": 615
  },
  {
    "conf": "acl20",
    "idd": 351
  },
  {
    "conf": "acl20",
    "idd": 680
  },
  {
    "conf": "acl20",
    "idd": 64
  },
  {
    "conf": "acl20",
    "idd": 27
  },
  {
    "conf": "acl20",
    "idd": 103
  },
  {
    "conf": "acl20",
    "idd": 156
  },
  {
    "conf": "acl20",
    "idd": 533
  },
  {
    "conf": "acl20",
    "idd": 613
  },
  {
    "conf": "acl20",
    "idd": 233
  },
  {
    "conf": "acl20",
    "idd": 672
  },
  {
    "conf": "acl20",
    "idd": 650
  },
  {
    "conf": "acl20",
    "idd": 295
  },
  {
    "conf": "acl20",
    "idd": 503
  },
  {
    "conf": "acl20",
    "idd": 63
  },
  {
    "conf": "acl20",
    "idd": 449
  },
  {
    "conf": "acl20",
    "idd": 185
  },
  {
    "conf": "acl20",
    "idd": 525
  },
  {
    "conf": "acl20",
    "idd": 145
  },
  {
    "conf": "acl20",
    "idd": 753
  },
  {
    "conf": "acl20",
    "idd": 376
  },
  {
    "conf": "acl20",
    "idd": 468
  },
  {
    "conf": "acl20",
    "idd": 552
  },
  {
    "conf": "acl20",
    "idd": 347
  },
  {
    "conf": "acl20",
    "idd": 621
  },
  {
    "conf": "acl20",
    "idd": 148
  },
  {
    "conf": "acl20",
    "idd": 66
  },
  {
    "conf": "acl20",
    "idd": 253
  },
  {
    "conf": "acl20",
    "idd": 424
  },
  {
    "conf": "acl20",
    "idd": 689
  },
  {
    "conf": "acl20",
    "idd": 323
  },
  {
    "conf": "acl20",
    "idd": 22
  },
  {
    "conf": "acl20",
    "idd": 225
  },
  {
    "conf": "acl20",
    "idd": 446
  },
  {
    "conf": "acl20",
    "idd": 601
  },
  {
    "conf": "acl20",
    "idd": 522
  },
  {
    "conf": "acl20",
    "idd": 442
  },
  {
    "conf": "acl20",
    "idd": 670
  },
  {
    "conf": "acl20",
    "idd": 19
  },
  {
    "conf": "acl20",
    "idd": 530
  },
  {
    "conf": "acl20",
    "idd": 452
  },
  {
    "conf": "acl20",
    "idd": 289
  },
  {
    "conf": "acl20",
    "idd": 363
  },
  {
    "conf": "acl20",
    "idd": 336
  },
  {
    "conf": "acl20",
    "idd": 565
  },
  {
    "conf": "acl20",
    "idd": 396
  },
  {
    "conf": "acl20",
    "idd": 137
  },
  {
    "conf": "acl20",
    "idd": 208
  },
  {
    "conf": "acl20",
    "idd": 405
  },
  {
    "conf": "acl20",
    "idd": 407
  },
  {
    "conf": "acl20",
    "idd": 575
  },
  {
    "conf": "acl20",
    "idd": 745
  },
  {
    "conf": "acl20",
    "idd": 744
  },
  {
    "conf": "acl20",
    "idd": 212
  },
  {
    "conf": "acl20",
    "idd": 136
  },
  {
    "conf": "acl20",
    "idd": 475
  },
  {
    "conf": "acl20",
    "idd": 44
  },
  {
    "conf": "acl20",
    "idd": 544
  },
  {
    "conf": "acl20",
    "idd": 127
  },
  {
    "conf": "acl20",
    "idd": 98
  },
  {
    "conf": "acl20",
    "idd": 414
  },
  {
    "conf": "acl20",
    "idd": 254
  },
  {
    "conf": "acl20",
    "idd": 512
  },
  {
    "conf": "acl20",
    "idd": 456
  },
  {
    "conf": "acl20",
    "idd": 403
  },
  {
    "conf": "acl20",
    "idd": 620
  },
  {
    "conf": "acl20",
    "idd": 750
  },
  {
    "conf": "acl20",
    "idd": 51
  },
  {
    "conf": "acl20",
    "idd": 699
  },
  {
    "conf": "acl20",
    "idd": 191
  },
  {
    "conf": "acl20",
    "idd": 697
  },
  {
    "conf": "acl20",
    "idd": 482
  },
  {
    "conf": "acl20",
    "idd": 408
  },
  {
    "conf": "acl20",
    "idd": 337
  },
  {
    "conf": "acl20",
    "idd": 33
  },
  {
    "conf": "acl20",
    "idd": 58
  },
  {
    "conf": "acl20",
    "idd": 425
  },
  {
    "conf": "acl20",
    "idd": 709
  },
  {
    "conf": "acl20",
    "idd": 557
  },
  {
    "conf": "acl20",
    "idd": 518
  },
  {
    "conf": "acl20",
    "idd": 112
  },
  {
    "conf": "acl20",
    "idd": 224
  },
  {
    "conf": "acl20",
    "idd": 658
  },
  {
    "conf": "acl20",
    "idd": 498
  },
  {
    "conf": "acl20",
    "idd": 352
  },
  {
    "conf": "acl20",
    "idd": 213
  },
  {
    "conf": "acl20",
    "idd": 574
  },
  {
    "conf": "acl20",
    "idd": 53
  },
  {
    "conf": "acl20",
    "idd": 28
  },
  {
    "conf": "acl20",
    "idd": 614
  },
  {
    "conf": "acl20",
    "idd": 548
  },
  {
    "conf": "acl20",
    "idd": 455
  },
  {
    "conf": "acl20",
    "idd": 322
  },
  {
    "conf": "acl20",
    "idd": 263
  },
  {
    "conf": "acl20",
    "idd": 567
  },
  {
    "conf": "acl20",
    "idd": 731
  },
  {
    "conf": "acl20",
    "idd": 183
  },
  {
    "conf": "acl20",
    "idd": 161
  },
  {
    "conf": "acl20",
    "idd": 169
  },
  {
    "conf": "acl20",
    "idd": 285
  },
  {
    "conf": "acl20",
    "idd": 687
  },
  {
    "conf": "acl20",
    "idd": 50
  },
  {
    "conf": "acl20",
    "idd": 550
  },
  {
    "conf": "acl20",
    "idd": 293
  },
  {
    "conf": "acl20",
    "idd": 246
  },
  {
    "conf": "acl20",
    "idd": 30
  },
  {
    "conf": "acl20",
    "idd": 385
  },
  {
    "conf": "acl20",
    "idd": 283
  },
  {
    "conf": "acl20",
    "idd": 627
  },
  {
    "conf": "acl20",
    "idd": 685
  },
  {
    "conf": "acl20",
    "idd": 140
  },
  {
    "conf": "acl20",
    "idd": 96
  },
  {
    "conf": "acl20",
    "idd": 357
  },
  {
    "conf": "acl20",
    "idd": 582
  },
  {
    "conf": "acl20",
    "idd": 364
  },
  {
    "conf": "acl20",
    "idd": 118
  },
  {
    "conf": "acl20",
    "idd": 174
  },
  {
    "conf": "acl20",
    "idd": 195
  },
  {
    "conf": "acl20",
    "idd": 434
  },
  {
    "conf": "acl20",
    "idd": 431
  },
  {
    "conf": "acl20",
    "idd": 585
  },
  {
    "conf": "acl20",
    "idd": 17
  },
  {
    "conf": "acl20",
    "idd": 693
  },
  {
    "conf": "acl20",
    "idd": 616
  },
  {
    "conf": "acl20",
    "idd": 681
  },
  {
    "conf": "acl20",
    "idd": 523
  },
  {
    "conf": "acl20",
    "idd": 144
  },
  {
    "conf": "acl20",
    "idd": 264
  },
  {
    "conf": "acl20",
    "idd": 378
  },
  {
    "conf": "acl20",
    "idd": 695
  },
  {
    "conf": "acl20",
    "idd": 155
  },
  {
    "conf": "acl20",
    "idd": 31
  },
  {
    "conf": "acl20",
    "idd": 635
  },
  {
    "conf": "acl20",
    "idd": 755
  },
  {
    "conf": "acl20",
    "idd": 733
  },
  {
    "conf": "acl20",
    "idd": 534
  },
  {
    "conf": "acl20",
    "idd": 411
  },
  {
    "conf": "acl20",
    "idd": 259
  },
  {
    "conf": "acl20",
    "idd": 706
  },
  {
    "conf": "acl20",
    "idd": 763
  },
  {
    "conf": "acl20",
    "idd": 23
  },
  {
    "conf": "acl20",
    "idd": 152
  },
  {
    "conf": "acl20",
    "idd": 272
  },
  {
    "conf": "acl20",
    "idd": 99
  },
  {
    "conf": "acl20",
    "idd": 143
  },
  {
    "conf": "acl20",
    "idd": 84
  },
  {
    "conf": "acl20",
    "idd": 499
  },
  {
    "conf": "acl20",
    "idd": 502
  },
  {
    "conf": "acl20",
    "idd": 5
  },
  {
    "conf": "acl20",
    "idd": 153
  },
  {
    "conf": "acl20",
    "idd": 754
  },
  {
    "conf": "acl20",
    "idd": 88
  },
  {
    "conf": "acl20",
    "idd": 410
  },
  {
    "conf": "acl20",
    "idd": 4
  },
  {
    "conf": "acl20",
    "idd": 641
  },
  {
    "conf": "acl20",
    "idd": 163
  },
  {
    "conf": "acl20",
    "idd": 214
  },
  {
    "conf": "acl20",
    "idd": 397
  },
  {
    "conf": "acl20",
    "idd": 777
  },
  {
    "conf": "acl20",
    "idd": 617
  },
  {
    "conf": "acl20",
    "idd": 652
  },
  {
    "conf": "acl20",
    "idd": 1
  },
  {
    "conf": "acl20",
    "idd": 273
  },
  {
    "conf": "acl20",
    "idd": 758
  },
  {
    "conf": "acl20",
    "idd": 32
  },
  {
    "conf": "acl20",
    "idd": 719
  },
  {
    "conf": "acl20",
    "idd": 370
  },
  {
    "conf": "acl20",
    "idd": 696
  },
  {
    "conf": "acl20",
    "idd": 598
  },
  {
    "conf": "acl20",
    "idd": 381
  },
  {
    "conf": "acl20",
    "idd": 39
  },
  {
    "conf": "acl20",
    "idd": 291
  },
  {
    "conf": "acl20",
    "idd": 134
  },
  {
    "conf": "acl20",
    "idd": 612
  },
  {
    "conf": "acl20",
    "idd": 766
  },
  {
    "conf": "acl20",
    "idd": 590
  },
  {
    "conf": "acl20",
    "idd": 341
  },
  {
    "conf": "acl20",
    "idd": 298
  },
  {
    "conf": "acl20",
    "idd": 151
  },
  {
    "conf": "acl20",
    "idd": 368
  },
  {
    "conf": "acl20",
    "idd": 374
  },
  {
    "conf": "acl20",
    "idd": 540
  },
  {
    "conf": "acl20",
    "idd": 74
  },
  {
    "conf": "acl20",
    "idd": 423
  },
  {
    "conf": "acl20",
    "idd": 313
  },
  {
    "conf": "acl20",
    "idd": 94
  },
  {
    "conf": "acl20",
    "idd": 7
  },
  {
    "conf": "acl20",
    "idd": 344
  },
  {
    "conf": "acl20",
    "idd": 554
  },
  {
    "conf": "acl20",
    "idd": 698
  },
  {
    "conf": "acl20",
    "idd": 727
  },
  {
    "conf": "acl20",
    "idd": 660
  },
  {
    "conf": "acl20",
    "idd": 448
  },
  {
    "conf": "acl20",
    "idd": 150
  },
  {
    "conf": "acl20",
    "idd": 666
  },
  {
    "conf": "acl20",
    "idd": 438
  },
  {
    "conf": "acl20",
    "idd": 631
  },
  {
    "conf": "acl20",
    "idd": 738
  },
  {
    "conf": "acl20",
    "idd": 624
  },
  {
    "conf": "acl20",
    "idd": 149
  },
  {
    "conf": "acl20",
    "idd": 38
  },
  {
    "conf": "acl20",
    "idd": 391
  },
  {
    "conf": "acl20",
    "idd": 49
  },
  {
    "conf": "acl20",
    "idd": 311
  },
  {
    "conf": "acl20",
    "idd": 583
  },
  {
    "conf": "acl20",
    "idd": 8
  },
  {
    "conf": "acl20",
    "idd": 55
  },
  {
    "conf": "acl20",
    "idd": 226
  },
  {
    "conf": "acl20",
    "idd": 245
  },
  {
    "conf": "acl20",
    "idd": 82
  },
  {
    "conf": "acl20",
    "idd": 77
  },
  {
    "conf": "acl20",
    "idd": 545
  },
  {
    "conf": "acl20",
    "idd": 355
  },
  {
    "conf": "acl20",
    "idd": 592
  },
  {
    "conf": "acl20",
    "idd": 113
  },
  {
    "conf": "acl20",
    "idd": 120
  },
  {
    "conf": "acl20",
    "idd": 255
  },
  {
    "conf": "acl20",
    "idd": 654
  },
  {
    "conf": "acl20",
    "idd": 718
  },
  {
    "conf": "acl20",
    "idd": 279
  },
  {
    "conf": "acl20",
    "idd": 694
  },
  {
    "conf": "acl20",
    "idd": 774
  },
  {
    "conf": "acl20",
    "idd": 751
  },
  {
    "conf": "acl20",
    "idd": 721
  },
  {
    "conf": "acl20",
    "idd": 509
  },
  {
    "conf": "acl20",
    "idd": 665
  },
  {
    "conf": "acl20",
    "idd": 638
  },
  {
    "conf": "acl20",
    "idd": 239
  },
  {
    "conf": "acl20",
    "idd": 78
  },
  {
    "conf": "acl20",
    "idd": 597
  },
  {
    "conf": "acl20",
    "idd": 36
  },
  {
    "conf": "acl20",
    "idd": 675
  },
  {
    "conf": "acl20",
    "idd": 481
  },
  {
    "conf": "acl20",
    "idd": 444
  },
  {
    "conf": "acl20",
    "idd": 730
  },
  {
    "conf": "acl20",
    "idd": 324
  },
  {
    "conf": "acl20",
    "idd": 571
  },
  {
    "conf": "acl20",
    "idd": 562
  },
  {
    "conf": "acl20",
    "idd": 623
  },
  {
    "conf": "acl20",
    "idd": 276
  },
  {
    "conf": "acl20",
    "idd": 261
  },
  {
    "conf": "acl20",
    "idd": 508
  },
  {
    "conf": "acl20",
    "idd": 708
  },
  {
    "conf": "acl20",
    "idd": 164
  },
  {
    "conf": "acl20",
    "idd": 715
  },
  {
    "conf": "acl20",
    "idd": 463
  },
  {
    "conf": "acl20",
    "idd": 505
  },
  {
    "conf": "acl20",
    "idd": 726
  },
  {
    "conf": "acl20",
    "idd": 653
  },
  {
    "conf": "acl20",
    "idd": 436
  },
  {
    "conf": "acl20",
    "idd": 61
  },
  {
    "conf": "acl20",
    "idd": 490
  },
  {
    "conf": "acl20",
    "idd": 168
  },
  {
    "conf": "acl20",
    "idd": 772
  },
  {
    "conf": "acl20",
    "idd": 395
  },
  {
    "conf": "acl20",
    "idd": 608
  },
  {
    "conf": "acl20",
    "idd": 218
  },
  {
    "conf": "acl20",
    "idd": 304
  },
  {
    "conf": "acl20",
    "idd": 104
  },
  {
    "conf": "acl20",
    "idd": 637
  },
  {
    "conf": "acl20",
    "idd": 277
  },
  {
    "conf": "acl20",
    "idd": 229
  },
  {
    "conf": "acl20",
    "idd": 306
  },
  {
    "conf": "acl20",
    "idd": 400
  },
  {
    "conf": "acl20",
    "idd": 102
  },
  {
    "conf": "acl20",
    "idd": 250
  },
  {
    "conf": "acl20",
    "idd": 20
  },
  {
    "conf": "acl20",
    "idd": 579
  },
  {
    "conf": "acl20",
    "idd": 265
  },
  {
    "conf": "acl20",
    "idd": 10
  }
]
